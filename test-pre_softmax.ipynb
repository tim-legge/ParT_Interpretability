{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb1f24c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pre-Softmax-Supplier ParT for analyzing attention and interaction\n",
    "\n",
    "Defines Batch, MultiHeadAttention, and multi_head_attention_forward such that they will return\n",
    "pre-softmaxed attention and interaction matrices for analysis purposes alongside typical outputs.\n",
    "\n",
    "This is achieved through forward hooks defined in Pre_Softmax_Hook such that the implementation of\n",
    "ParT is not significantly altered.*\n",
    "\n",
    "Passing the kwarg 'return_pre_softmax = True' (disabled by default) into a model configuration under\n",
    "the ParticleTransformerWrapper class (just ParticleTransformer probably also works, but the latter \n",
    "has not been tested empirically) will enable this feature and allow usage of Pre_Softmax_Hook. While \n",
    "hooks are registered, Pre_Softmax_Hook will stack pre-softmaxed attention and interaction matrices \n",
    "for each input jet. This is a passive process and the only further interaction suggested is to call\n",
    "pre_softmax_attentions or _interactions in order to view and analyze once you have run your model \n",
    "to satisfaction.\n",
    "\n",
    "Initialize and register hooks via:\n",
    "\n",
    "<Captain_Hook> = Pre_Softmax_Hook(model=<your_ParT_here>) # layer_name is also mentioned as a param\n",
    "                                                            below but quickly became irrelevant\n",
    "                                \n",
    "                ----    Run your model  ----\n",
    "\n",
    "Check results via:\n",
    "\n",
    "<Captain_Hook>.pre_softmax_attentions\n",
    "and\n",
    "<Captain_Hook>.pre_softmax_interactions\n",
    "\n",
    "(both are of shape (total_num_of_layers, num_heads, seq_length, seq_length)\n",
    "where total_num_of_layers is the number of particle attention blocks that were passed through in\n",
    "total by all jets in the sample)\n",
    "\n",
    "                ----  Run your analysis  ----\n",
    "\n",
    "Clear your hooks and give the captain some rest via:**\n",
    "\n",
    "<Captain_Hook>.clear_hooks()\n",
    "\n",
    "--------------------------------------------------------------------------------------------------\n",
    "\n",
    "OTHER NOTES AND FOOTNOTES:\n",
    "\n",
    "Keeping return_pre_softmax = False will leave the implementation functionally identical to original\n",
    "Pytorch and ParT code.***\n",
    "\n",
    "If you want to know whether any of this is optimized without needing to read the code, it's not.\n",
    "Ditto for rigorous testing.\n",
    "\n",
    "*Depends on your definition of 'significantly'. ParT computes in the exact way as before\n",
    "and inner workings are 99% the same, but future edits made to the model will have to deal with\n",
    "different output structures for multi_head_attention_forward, MultiheadAttention,\n",
    "Block, and ParticleTransformer.\n",
    "\n",
    "**You don't have to anthropomorphize your class instances in order for this to work. I just thought\n",
    "it was funny.\n",
    "\n",
    "***Currently untested. Also not really sure why you would do this if the hook doesn't appear to \n",
    "disrupt any other process, but I won't judge.\n",
    "'''\n",
    "\n",
    "\n",
    "from typing import List, Optional\n",
    "import timeit\n",
    "import awkward as ak\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter \n",
    "from torch.nn.init import xavier_uniform_, xavier_normal_, constant_\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from typing import Optional\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple\n",
    "_is_fastpath_enabled: bool = True\n",
    "from torch.overrides import (\n",
    "    handle_torch_function,\n",
    "    has_torch_function,\n",
    "    has_torch_function_unary,\n",
    "    has_torch_function_variadic,\n",
    ")\n",
    "linear = torch._C._nn.linear\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "import copy\n",
    "from torch._C import _add_docstr, _infer_size\n",
    "\n",
    "from functools import partial\n",
    "from weaver.utils.logger import _logger\n",
    "import os\n",
    "import uproot\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch._torch_docs import reproducibility_notes, sparse_support_notes, tf32_notes\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    __constants__ = ['batch_first']\n",
    "    bias_k: Optional[torch.Tensor]\n",
    "    bias_v: Optional[torch.Tensor]\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False,\n",
    "                 kdim=None, vdim=None, batch_first=False, device=None, dtype=None, return_pre_softmax=False) -> None:\n",
    "        if embed_dim <= 0 or num_heads <= 0:\n",
    "            raise ValueError(\n",
    "                f\"embed_dim and num_heads must be greater than 0,\"\n",
    "                f\" got embed_dim={embed_dim} and num_heads={num_heads} instead\"\n",
    "            )\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.kdim = kdim if kdim is not None else embed_dim\n",
    "        self.vdim = vdim if vdim is not None else embed_dim\n",
    "        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.batch_first = batch_first\n",
    "        self.return_pre_softmax = return_pre_softmax\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        if not self._qkv_same_embed_dim:\n",
    "            self.q_proj_weight = Parameter(torch.empty((embed_dim, embed_dim), **factory_kwargs))\n",
    "            self.k_proj_weight = Parameter(torch.empty((embed_dim, self.kdim), **factory_kwargs))\n",
    "            self.v_proj_weight = Parameter(torch.empty((embed_dim, self.vdim), **factory_kwargs))\n",
    "            self.register_parameter('in_proj_weight', None)\n",
    "        else:\n",
    "            self.in_proj_weight = Parameter(torch.empty((3 * embed_dim, embed_dim), **factory_kwargs))\n",
    "            self.register_parameter('q_proj_weight', None)\n",
    "            self.register_parameter('k_proj_weight', None)\n",
    "            self.register_parameter('v_proj_weight', None)\n",
    "\n",
    "        if bias:\n",
    "            self.in_proj_bias = Parameter(torch.empty(3 * embed_dim, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('in_proj_bias', None)\n",
    "        self.out_proj = torch.nn.Linear(embed_dim, embed_dim, bias=bias, **factory_kwargs)\n",
    "\n",
    "        if add_bias_kv:\n",
    "            self.bias_k = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n",
    "            self.bias_v = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n",
    "        else:\n",
    "            self.bias_k = self.bias_v = None\n",
    "\n",
    "        self.add_zero_attn = add_zero_attn\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        if self._qkv_same_embed_dim:\n",
    "            xavier_uniform_(self.in_proj_weight)\n",
    "        else:\n",
    "            xavier_uniform_(self.q_proj_weight)\n",
    "            xavier_uniform_(self.k_proj_weight)\n",
    "            xavier_uniform_(self.v_proj_weight)\n",
    "\n",
    "        if self.in_proj_bias is not None:\n",
    "            constant_(self.in_proj_bias, 0.)\n",
    "            constant_(self.out_proj.bias, 0.)\n",
    "        if self.bias_k is not None:\n",
    "            xavier_normal_(self.bias_k)\n",
    "        if self.bias_v is not None:\n",
    "            xavier_normal_(self.bias_v)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        # Support loading old MultiheadAttention checkpoints generated by v1.1.0\n",
    "        if '_qkv_same_embed_dim' not in state:\n",
    "            state['_qkv_same_embed_dim'] = True\n",
    "\n",
    "        super().__setstate__(state)\n",
    "\n",
    "    def forward(\n",
    "                self,\n",
    "                query: Tensor,\n",
    "                key: Tensor,\n",
    "                value: Tensor,\n",
    "                key_padding_mask: Optional[Tensor] = None,\n",
    "                need_weights: bool = True,\n",
    "                attn_mask: Optional[Tensor] = None,\n",
    "                average_attn_weights: bool = True,\n",
    "                is_causal : bool = False,\n",
    "                **kwargs) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "\n",
    "            why_not_fast_path = ''\n",
    "            if ((attn_mask is not None and torch.is_floating_point(attn_mask))\n",
    "            or (key_padding_mask is not None) and torch.is_floating_point(key_padding_mask)):\n",
    "                why_not_fast_path = \"floating-point masks are not supported for fast path.\"\n",
    "\n",
    "            is_batched = query.dim() == 3\n",
    "\n",
    "            key_padding_mask = _canonical_mask(\n",
    "                mask=key_padding_mask,\n",
    "                mask_name=\"key_padding_mask\",\n",
    "                other_type=_none_or_dtype(attn_mask),\n",
    "                other_name=\"attn_mask\",\n",
    "                target_type=query.dtype\n",
    "            )\n",
    "\n",
    "            attn_mask = _canonical_mask(\n",
    "                mask=attn_mask,\n",
    "                mask_name=\"attn_mask\",\n",
    "                other_type=None,\n",
    "                other_name=\"\",\n",
    "                target_type=query.dtype,\n",
    "                check_other=False,\n",
    "            )\n",
    "\n",
    "            is_fastpath_enabled = get_fastpath_enabled()\n",
    "\n",
    "            if not is_fastpath_enabled:\n",
    "                why_not_fast_path = \"torch.backends.mha.get_fastpath_enabled() was not True\"\n",
    "            elif not is_batched:\n",
    "                why_not_fast_path = f\"input not batched; expected query.dim() of 3 but got {query.dim()}\"\n",
    "            elif query is not key or key is not value:\n",
    "                # When lifting this restriction, don't forget to either\n",
    "                # enforce that the dtypes all match or test cases where\n",
    "                # they don't!\n",
    "                why_not_fast_path = \"non-self attention was used (query, key, and value are not the same Tensor)\"\n",
    "            elif self.in_proj_bias is not None and query.dtype != self.in_proj_bias.dtype:\n",
    "                why_not_fast_path = f\"dtypes of query ({query.dtype}) and self.in_proj_bias ({self.in_proj_bias.dtype}) don't match\"\n",
    "            elif self.in_proj_weight is None:\n",
    "                why_not_fast_path = \"in_proj_weight was None\"\n",
    "            elif query.dtype != self.in_proj_weight.dtype:\n",
    "                # this case will fail anyway, but at least they'll get a useful error message.\n",
    "                why_not_fast_path = f\"dtypes of query ({query.dtype}) and self.in_proj_weight ({self.in_proj_weight.dtype}) don't match\"\n",
    "            elif self.training:\n",
    "                why_not_fast_path = \"training is enabled\"\n",
    "            elif (self.num_heads % 2) != 0:\n",
    "                why_not_fast_path = \"self.num_heads is not even\"\n",
    "            elif not self.batch_first:\n",
    "                why_not_fast_path = \"batch_first was not True\"\n",
    "            elif self.bias_k is not None:\n",
    "                why_not_fast_path = \"self.bias_k was not None\"\n",
    "            elif self.bias_v is not None:\n",
    "                why_not_fast_path = \"self.bias_v was not None\"\n",
    "            elif self.add_zero_attn:\n",
    "                why_not_fast_path = \"add_zero_attn was enabled\"\n",
    "            elif not self._qkv_same_embed_dim:\n",
    "                why_not_fast_path = \"_qkv_same_embed_dim was not True\"\n",
    "            elif query.is_nested and (key_padding_mask is not None or attn_mask is not None):\n",
    "                why_not_fast_path = \"supplying both src_key_padding_mask and src_mask at the same time \\\n",
    "                                    is not supported with NestedTensor input\"\n",
    "            elif torch.is_autocast_enabled():\n",
    "                why_not_fast_path = \"autocast is enabled\"\n",
    "\n",
    "            if not why_not_fast_path:\n",
    "                tensor_args = (\n",
    "                    query,\n",
    "                    key,\n",
    "                    value,\n",
    "                    self.in_proj_weight,\n",
    "                    self.in_proj_bias,\n",
    "                    self.out_proj.weight,\n",
    "                    self.out_proj.bias,\n",
    "                )\n",
    "                # We have to use list comprehensions below because TorchScript does not support\n",
    "                # generator expressions.\n",
    "                if torch.overrides.has_torch_function(tensor_args):\n",
    "                    why_not_fast_path = \"some Tensor argument has_torch_function\"\n",
    "                elif _is_make_fx_tracing():\n",
    "                    why_not_fast_path = \"we are running make_fx tracing\"\n",
    "                elif not all(_check_arg_device(x) for x in tensor_args):\n",
    "                    why_not_fast_path = (\"some Tensor argument's device is neither one of \"\n",
    "                                        f\"cpu, cuda or {torch.utils.backend_registration._privateuse1_backend_name}\")\n",
    "                elif torch.is_grad_enabled() and any(_arg_requires_grad(x) for x in tensor_args):\n",
    "                    why_not_fast_path = (\"grad is enabled and at least one of query or the \"\n",
    "                                        \"input/output projection weights or biases requires_grad\")\n",
    "                if not why_not_fast_path:\n",
    "                    merged_mask, mask_type = self.merge_masks(attn_mask, key_padding_mask, query)\n",
    "\n",
    "                    if self.in_proj_bias is not None and self.in_proj_weight is not None:\n",
    "                        return torch._native_multi_head_attention(\n",
    "                            query,\n",
    "                            key,\n",
    "                            value,\n",
    "                            self.embed_dim,\n",
    "                            self.num_heads,\n",
    "                            self.in_proj_weight,\n",
    "                            self.in_proj_bias,\n",
    "                            self.out_proj.weight,\n",
    "                            self.out_proj.bias,\n",
    "                            merged_mask,\n",
    "                            need_weights,\n",
    "                            average_attn_weights,\n",
    "                            mask_type)\n",
    "\n",
    "            any_nested = query.is_nested or key.is_nested or value.is_nested\n",
    "            assert not any_nested, (\"MultiheadAttention does not support NestedTensor outside of its fast path. \" +\n",
    "                                    f\"The fast path was not hit because {why_not_fast_path}\")\n",
    "\n",
    "            if self.batch_first and is_batched:\n",
    "                # make sure that the transpose op does not affect the \"is\" property\n",
    "                if key is value:\n",
    "                    if query is key:\n",
    "                        query = key = value = query.transpose(1, 0)\n",
    "                    else:\n",
    "                        query, key = (x.transpose(1, 0) for x in (query, key))\n",
    "                        value = key\n",
    "                else:\n",
    "                    query, key, value = (x.transpose(1, 0) for x in (query, key, value))\n",
    "\n",
    "            if not self._qkv_same_embed_dim:\n",
    "                if self.return_pre_softmax:\n",
    "                    attn_output, attn_output_weights, pre_softmax_attention, pre_softmax_interaction = multi_head_attention_forward(\n",
    "                        query, key, value, self.embed_dim, self.num_heads,\n",
    "                        self.in_proj_weight, self.in_proj_bias,\n",
    "                        self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "                        self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "                        training=self.training,\n",
    "                        key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
    "                        attn_mask=attn_mask,\n",
    "                        use_separate_proj_weight=True,\n",
    "                        q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\n",
    "                        v_proj_weight=self.v_proj_weight,\n",
    "                        average_attn_weights=average_attn_weights,\n",
    "                        return_pre_softmax=self.return_pre_softmax\n",
    "                )\n",
    "                else:\n",
    "                    attn_output, attn_output_weights = multi_head_attention_forward(\n",
    "                        query, key, value, self.embed_dim, self.num_heads,\n",
    "                        self.in_proj_weight, self.in_proj_bias,\n",
    "                        self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "                        self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "                        training=self.training,\n",
    "                        key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
    "                        attn_mask=attn_mask,\n",
    "                        use_separate_proj_weight=True,\n",
    "                        q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\n",
    "                        v_proj_weight=self.v_proj_weight,\n",
    "                        average_attn_weights=average_attn_weights,\n",
    "                        return_pre_softmax=self.return_pre_softmax\n",
    "                )\n",
    "            else:\n",
    "                if self.return_pre_softmax:\n",
    "                    attn_output, attn_output_weights, pre_softmax_attention, pre_softmax_interaction = multi_head_attention_forward(\n",
    "                        query, key, value, self.embed_dim, self.num_heads,\n",
    "                        self.in_proj_weight, self.in_proj_bias,\n",
    "                        self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "                        self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "                        training=self.training,\n",
    "                        key_padding_mask=key_padding_mask,\n",
    "                        need_weights=need_weights,\n",
    "                        attn_mask=attn_mask,\n",
    "                        average_attn_weights=average_attn_weights,\n",
    "                        return_pre_softmax=self.return_pre_softmax)\n",
    "                else:\n",
    "                    attn_output, attn_output_weights = multi_head_attention_forward(\n",
    "                        query, key, value, self.embed_dim, self.num_heads,\n",
    "                        self.in_proj_weight, self.in_proj_bias,\n",
    "                        self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "                        self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "                        training=self.training,\n",
    "                        key_padding_mask=key_padding_mask,\n",
    "                        need_weights=need_weights,\n",
    "                        attn_mask=attn_mask,\n",
    "                        average_attn_weights=average_attn_weights,\n",
    "                        return_pre_softmax=self.return_pre_softmax)\n",
    "\n",
    "            if self.batch_first and is_batched:\n",
    "                if self.return_pre_softmax:\n",
    "                    pre_softmax_attention.detach()\n",
    "                    pre_softmax_interaction.detach()\n",
    "                    return {'output': attn_output.transpose(1, 0), 'attn_output_weights': attn_output_weights, 'pre_softmax_attention': pre_softmax_attention, 'pre_softmax_interaction': pre_softmax_interaction}\n",
    "                else:\n",
    "                    return attn_output.transpose(1, 0), attn_output_weights\n",
    "            else:\n",
    "                if self.return_pre_softmax:\n",
    "                    pre_softmax_attention.detach()\n",
    "                    pre_softmax_interaction.detach()\n",
    "                    return {'output': attn_output, 'attn_output_weights': attn_output_weights, 'pre_softmax_attention': pre_softmax_attention, 'pre_softmax_interaction': pre_softmax_interaction}\n",
    "                else:\n",
    "                    return attn_output, attn_output_weights\n",
    "\n",
    "def _canonical_mask(\n",
    "    mask: Optional[Tensor],\n",
    "    mask_name: str,\n",
    "    other_type: Optional[torch.dtype],\n",
    "    other_name: str,\n",
    "    target_type: torch.dtype,\n",
    "    check_other: bool = True,\n",
    ") -> Optional[Tensor]:\n",
    "    if mask is not None:\n",
    "        _mask_dtype = mask.dtype\n",
    "        _mask_is_float = torch.is_floating_point(mask)\n",
    "        if _mask_dtype != torch.bool and not _mask_is_float:\n",
    "            raise AssertionError(\n",
    "                f\"only bool and floating types of {mask_name} are supported\"\n",
    "            )\n",
    "        if check_other and other_type is not None:\n",
    "            if _mask_dtype != other_type:\n",
    "                warnings.warn(\n",
    "                    f\"Support for mismatched {mask_name} and {other_name} \"\n",
    "                    \"is deprecated. Use same type for both instead.\"\n",
    "                )\n",
    "        if not _mask_is_float:\n",
    "            mask = torch.zeros_like(mask, dtype=target_type).masked_fill_(\n",
    "                mask, float(\"-inf\")\n",
    "            )\n",
    "    return mask\n",
    "def _none_or_dtype(input: Optional[Tensor]) -> Optional[torch.dtype]:\n",
    "    if input is None:\n",
    "        return None\n",
    "    elif isinstance(input, torch.Tensor):\n",
    "        return input.dtype\n",
    "    raise RuntimeError(\"input to _none_or_dtype() must be None or torch.Tensor\")\n",
    "def get_fastpath_enabled() -> bool:\n",
    "    \"\"\"Returns whether fast path for TransformerEncoder and MultiHeadAttention\n",
    "    is enabled, or ``True`` if jit is scripting.\n",
    "\n",
    "    ..note:\n",
    "        The fastpath might not be run even if ``get_fastpath_enabled`` returns\n",
    "        ``True`` unless all conditions on inputs are met.\n",
    "    \"\"\"\n",
    "    if not torch.jit.is_scripting():\n",
    "        return _is_fastpath_enabled\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "def set_fastpath_enabled(value: bool) -> None:\n",
    "    \"\"\"Sets whether fast path is enabled\"\"\"\n",
    "    global _is_fastpath_enabled\n",
    "    _is_fastpath_enabled = value\n",
    "\n",
    "def multi_head_attention_forward(\n",
    "    query: Tensor,\n",
    "    key: Tensor,\n",
    "    value: Tensor,\n",
    "    embed_dim_to_check: int,\n",
    "    num_heads: int,\n",
    "    in_proj_weight: Optional[Tensor],\n",
    "    in_proj_bias: Optional[Tensor],\n",
    "    bias_k: Optional[Tensor],\n",
    "    bias_v: Optional[Tensor],\n",
    "    add_zero_attn: bool,\n",
    "    dropout_p: float,\n",
    "    out_proj_weight: Tensor,\n",
    "    out_proj_bias: Optional[Tensor],\n",
    "    training: bool = True,\n",
    "    key_padding_mask: Optional[Tensor] = None,\n",
    "    need_weights: bool = True,\n",
    "    attn_mask: Optional[Tensor] = None,\n",
    "    use_separate_proj_weight: bool = False,\n",
    "    q_proj_weight: Optional[Tensor] = None,\n",
    "    k_proj_weight: Optional[Tensor] = None,\n",
    "    v_proj_weight: Optional[Tensor] = None,\n",
    "    static_k: Optional[Tensor] = None,\n",
    "    static_v: Optional[Tensor] = None,\n",
    "    average_attn_weights: bool = True,\n",
    "    is_causal: bool = False,\n",
    "    return_pre_softmax: bool = False,\n",
    ") -> Tuple[Tensor, Optional[Tensor]]:\n",
    "    tens_ops = (\n",
    "        query,\n",
    "        key,\n",
    "        value,\n",
    "        in_proj_weight,\n",
    "        in_proj_bias,\n",
    "        bias_k,\n",
    "        bias_v,\n",
    "        out_proj_weight,\n",
    "        out_proj_bias,\n",
    "    )\n",
    "    if has_torch_function(tens_ops):\n",
    "        return handle_torch_function(\n",
    "            multi_head_attention_forward,\n",
    "            tens_ops,\n",
    "            query,\n",
    "            key,\n",
    "            value,\n",
    "            embed_dim_to_check,\n",
    "            num_heads,\n",
    "            in_proj_weight,\n",
    "            in_proj_bias,\n",
    "            bias_k,\n",
    "            bias_v,\n",
    "            add_zero_attn,\n",
    "            dropout_p,\n",
    "            out_proj_weight,\n",
    "            out_proj_bias,\n",
    "            training=training,\n",
    "            key_padding_mask=key_padding_mask,\n",
    "            need_weights=need_weights,\n",
    "            attn_mask=attn_mask,\n",
    "            is_causal=is_causal,\n",
    "            use_separate_proj_weight=use_separate_proj_weight,\n",
    "            q_proj_weight=q_proj_weight,\n",
    "            k_proj_weight=k_proj_weight,\n",
    "            v_proj_weight=v_proj_weight,\n",
    "            static_k=static_k,\n",
    "            static_v=static_v,\n",
    "            average_attn_weights=average_attn_weights,\n",
    "            return_pre_softmax=return_pre_softmax,\n",
    "        )\n",
    "\n",
    "    is_batched = _mha_shape_check(\n",
    "        query, key, value, key_padding_mask, attn_mask, num_heads\n",
    "    )\n",
    "\n",
    "    # For unbatched input, we unsqueeze at the expected batch-dim to pretend that the input\n",
    "    # is batched, run the computation and before returning squeeze the\n",
    "    # batch dimension so that the output doesn't carry this temporary batch dimension.\n",
    "    if not is_batched:\n",
    "        # unsqueeze if the input is unbatched\n",
    "        query = query.unsqueeze(1)\n",
    "        key = key.unsqueeze(1)\n",
    "        value = value.unsqueeze(1)\n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = key_padding_mask.unsqueeze(0)\n",
    "\n",
    "    # set up shape vars\n",
    "    tgt_len, bsz, embed_dim = query.shape\n",
    "    src_len, _, _ = key.shape\n",
    "\n",
    "    key_padding_mask = _canonical_mask(\n",
    "        mask=key_padding_mask,\n",
    "        mask_name=\"key_padding_mask\",\n",
    "        other_type=_none_or_dtype(attn_mask),\n",
    "        other_name=\"attn_mask\",\n",
    "        target_type=query.dtype,\n",
    "    )\n",
    "\n",
    "    if is_causal and attn_mask is None:\n",
    "        raise RuntimeError(\n",
    "            \"Need attn_mask if specifying the is_causal hint. \"\n",
    "            \"You may use the Transformer module method \"\n",
    "            \"`generate_square_subsequent_mask` to create this mask.\"\n",
    "        )\n",
    "\n",
    "    if is_causal and key_padding_mask is None and not need_weights:\n",
    "        # when we have a kpm or need weights, we need attn_mask\n",
    "        # Otherwise, we use the is_causal hint go as is_causal\n",
    "        # indicator to SDPA.\n",
    "        attn_mask = None\n",
    "    else:\n",
    "        attn_mask = _canonical_mask(\n",
    "            mask=attn_mask,\n",
    "            mask_name=\"attn_mask\",\n",
    "            other_type=None,\n",
    "            other_name=\"\",\n",
    "            target_type=query.dtype,\n",
    "            check_other=False,\n",
    "        )\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            # We have the attn_mask, and use that to merge kpm into it.\n",
    "            # Turn off use of is_causal hint, as the merged mask is no\n",
    "            # longer causal.\n",
    "            is_causal = False\n",
    "\n",
    "    assert (\n",
    "        embed_dim == embed_dim_to_check\n",
    "    ), f\"was expecting embedding dimension of {embed_dim_to_check}, but got {embed_dim}\"\n",
    "    if isinstance(embed_dim, torch.Tensor):\n",
    "        # embed_dim can be a tensor when JIT tracing\n",
    "        head_dim = embed_dim.div(num_heads, rounding_mode=\"trunc\")\n",
    "    else:\n",
    "        head_dim = embed_dim // num_heads\n",
    "    assert (\n",
    "        head_dim * num_heads == embed_dim\n",
    "    ), f\"embed_dim {embed_dim} not divisible by num_heads {num_heads}\"\n",
    "    if use_separate_proj_weight:\n",
    "        # allow MHA to have different embedding dimensions when separate projection weights are used\n",
    "        assert (\n",
    "            key.shape[:2] == value.shape[:2]\n",
    "        ), f\"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}\"\n",
    "    else:\n",
    "        assert (\n",
    "            key.shape == value.shape\n",
    "        ), f\"key shape {key.shape} does not match value shape {value.shape}\"\n",
    "\n",
    "    #\n",
    "    # compute in-projection\n",
    "    #\n",
    "    if not use_separate_proj_weight:\n",
    "        assert (\n",
    "            in_proj_weight is not None\n",
    "        ), \"use_separate_proj_weight is False but in_proj_weight is None\"\n",
    "        q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n",
    "    else:\n",
    "        assert (\n",
    "            q_proj_weight is not None\n",
    "        ), \"use_separate_proj_weight is True but q_proj_weight is None\"\n",
    "        assert (\n",
    "            k_proj_weight is not None\n",
    "        ), \"use_separate_proj_weight is True but k_proj_weight is None\"\n",
    "        assert (\n",
    "            v_proj_weight is not None\n",
    "        ), \"use_separate_proj_weight is True but v_proj_weight is None\"\n",
    "        if in_proj_bias is None:\n",
    "            b_q = b_k = b_v = None\n",
    "        else:\n",
    "            b_q, b_k, b_v = in_proj_bias.chunk(3)\n",
    "        q, k, v = _in_projection(\n",
    "            query,\n",
    "            key,\n",
    "            value,\n",
    "            q_proj_weight,\n",
    "            k_proj_weight,\n",
    "            v_proj_weight,\n",
    "            b_q,\n",
    "            b_k,\n",
    "            b_v,\n",
    "        )\n",
    "\n",
    "    # prep attention mask\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        # ensure attn_mask's dim is 3\n",
    "        if attn_mask.dim() == 2:\n",
    "            correct_2d_size = (tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_2d_size:\n",
    "                raise RuntimeError(\n",
    "                    f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\"\n",
    "                )\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        elif attn_mask.dim() == 3:\n",
    "            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_3d_size:\n",
    "                raise RuntimeError(\n",
    "                    f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\"\n",
    "                )\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                f\"attn_mask's dimension {attn_mask.dim()} is not supported\"\n",
    "            )\n",
    "\n",
    "    # add bias along batch dimension (currently second)\n",
    "    if bias_k is not None and bias_v is not None:\n",
    "        assert static_k is None, \"bias cannot be added to static key.\"\n",
    "        assert static_v is None, \"bias cannot be added to static value.\"\n",
    "        k = torch.cat([k, bias_k.repeat(1, bsz, 1)])\n",
    "        v = torch.cat([v, bias_v.repeat(1, bsz, 1)])\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = pad(attn_mask, (0, 1))\n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = pad(key_padding_mask, (0, 1))\n",
    "    else:\n",
    "        assert bias_k is None\n",
    "        assert bias_v is None\n",
    "\n",
    "    #\n",
    "    # reshape q, k, v for multihead attention and make them batch first\n",
    "    #\n",
    "    q = q.view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    if static_k is None:\n",
    "        k = k.view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    else:\n",
    "        # TODO finish disentangling control flow so we don't do in-projections when statics are passed\n",
    "        assert (\n",
    "            static_k.size(0) == bsz * num_heads\n",
    "        ), f\"expecting static_k.size(0) of {bsz * num_heads}, but got {static_k.size(0)}\"\n",
    "        assert (\n",
    "            static_k.size(2) == head_dim\n",
    "        ), f\"expecting static_k.size(2) of {head_dim}, but got {static_k.size(2)}\"\n",
    "        k = static_k\n",
    "    if static_v is None:\n",
    "        v = v.view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    else:\n",
    "        # TODO finish disentangling control flow so we don't do in-projections when statics are passed\n",
    "        assert (\n",
    "            static_v.size(0) == bsz * num_heads\n",
    "        ), f\"expecting static_v.size(0) of {bsz * num_heads}, but got {static_v.size(0)}\"\n",
    "        assert (\n",
    "            static_v.size(2) == head_dim\n",
    "        ), f\"expecting static_v.size(2) of {head_dim}, but got {static_v.size(2)}\"\n",
    "        v = static_v\n",
    "\n",
    "    # add zero attention along batch dimension (now first)\n",
    "    if add_zero_attn:\n",
    "        zero_attn_shape = (bsz * num_heads, 1, head_dim)\n",
    "        k = torch.cat(\n",
    "            [k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1\n",
    "        )\n",
    "        v = torch.cat(\n",
    "            [v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1\n",
    "        )\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = pad(attn_mask, (0, 1))\n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = pad(key_padding_mask, (0, 1))\n",
    "\n",
    "    # update source sequence length after adjustments\n",
    "    src_len = k.size(1)\n",
    "\n",
    "    # merge key padding and attention masks\n",
    "    if key_padding_mask is not None:\n",
    "        assert key_padding_mask.shape == (\n",
    "            bsz,\n",
    "            src_len,\n",
    "        ), f\"expecting key_padding_mask shape of {(bsz, src_len)}, but got {key_padding_mask.shape}\"\n",
    "        key_padding_mask = (\n",
    "            key_padding_mask.view(bsz, 1, 1, src_len)\n",
    "            .expand(-1, num_heads, -1, -1)\n",
    "            .reshape(bsz * num_heads, 1, src_len)\n",
    "        )\n",
    "        if attn_mask is None:\n",
    "            attn_mask = key_padding_mask\n",
    "        else:\n",
    "            attn_mask = attn_mask + key_padding_mask\n",
    "\n",
    "    # adjust dropout probability\n",
    "    if not training:\n",
    "        dropout_p = 0.0\n",
    "\n",
    "    #\n",
    "    # (deep breath) calculate attention and out projection\n",
    "    #\n",
    "\n",
    "    if need_weights:\n",
    "        B, Nt, E = q.shape\n",
    "        q_scaled = q * math.sqrt(1.0 / float(E))\n",
    "\n",
    "        assert not (\n",
    "            is_causal and attn_mask is None\n",
    "        ), \"FIXME: is_causal not implemented for need_weights\"\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            pre_softmax_interaction = attn_mask.expand(-1,128,-1)\n",
    "            pre_softmax_attention = torch.bmm(q_scaled, k.transpose(-2, -1))\n",
    "            pre_softmax_attention.detach()\n",
    "            pre_softmax_interaction.detach()\n",
    "            attn_output_weights = torch.baddbmm(\n",
    "                input=attn_mask, batch1=q_scaled, batch2=k.transpose(-2, -1)\n",
    "            )\n",
    "        else:\n",
    "            attn_output_weights = pre_softmax_attention = torch.bmm(q_scaled, k.transpose(-2, -1))\n",
    "        \n",
    "        attn_output_weights = softmax(attn_output_weights, dim=-1)\n",
    "        if dropout_p > 0.0:\n",
    "            attn_output_weights = dropout(attn_output_weights, p=dropout_p)\n",
    "\n",
    "        attn_output = torch.bmm(attn_output_weights, v)\n",
    "\n",
    "        attn_output = (\n",
    "            attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)\n",
    "        )\n",
    "        attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
    "        attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))\n",
    "\n",
    "        # optionally average attention weights over heads\n",
    "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
    "        if average_attn_weights:\n",
    "            attn_output_weights = attn_output_weights.mean(dim=1)\n",
    "\n",
    "        if not is_batched:\n",
    "            # squeeze the output if input was unbatched\n",
    "            attn_output = attn_output.squeeze(1)\n",
    "            attn_output_weights = attn_output_weights.squeeze(0)\n",
    "        if return_pre_softmax:\n",
    "            return attn_output, attn_output_weights, pre_softmax_attention, pre_softmax_interaction\n",
    "        else:\n",
    "            return attn_output, attn_output_weights\n",
    "    else:\n",
    "        # attn_mask can be either (L,S) or (N*num_heads, L, S)\n",
    "        # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)\n",
    "        # in order to match the input for SDPA of (N, num_heads, L, S)\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n",
    "                attn_mask = attn_mask.unsqueeze(0)\n",
    "            else:\n",
    "                attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n",
    "\n",
    "        q = q.view(bsz, num_heads, tgt_len, head_dim)\n",
    "        k = k.view(bsz, num_heads, src_len, head_dim)\n",
    "        v = v.view(bsz, num_heads, src_len, head_dim)\n",
    "\n",
    "        attn_output = scaled_dot_product_attention(\n",
    "            q, k, v, attn_mask, dropout_p, is_causal\n",
    "        )\n",
    "        attn_output = (\n",
    "            attn_output.permute(2, 0, 1, 3).contiguous().view(bsz * tgt_len, embed_dim)\n",
    "        )\n",
    "\n",
    "        attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
    "        attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))\n",
    "        if not is_batched:\n",
    "            # squeeze the output if input was unbatched\n",
    "            attn_output = attn_output.squeeze(1)\n",
    "        return attn_output, None\n",
    "def _mha_shape_check(\n",
    "    query: Tensor,\n",
    "    key: Tensor,\n",
    "    value: Tensor,\n",
    "    key_padding_mask: Optional[Tensor],\n",
    "    attn_mask: Optional[Tensor],\n",
    "    num_heads: int,\n",
    "):\n",
    "    # Verifies the expected shape for `query, `key`, `value`, `key_padding_mask` and `attn_mask`\n",
    "    # and returns if the input is batched or not.\n",
    "    # Raises an error if `query` is not 2-D (unbatched) or 3-D (batched) tensor.\n",
    "\n",
    "    # Shape check.\n",
    "    if query.dim() == 3:\n",
    "        # Batched Inputs\n",
    "        is_batched = True\n",
    "        assert key.dim() == 3 and value.dim() == 3, (\n",
    "            \"For batched (3-D) `query`, expected `key` and `value` to be 3-D\"\n",
    "            f\" but found {key.dim()}-D and {value.dim()}-D tensors respectively\"\n",
    "        )\n",
    "        if key_padding_mask is not None:\n",
    "            assert key_padding_mask.dim() == 2, (\n",
    "                \"For batched (3-D) `query`, expected `key_padding_mask` to be `None` or 2-D\"\n",
    "                f\" but found {key_padding_mask.dim()}-D tensor instead\"\n",
    "            )\n",
    "        if attn_mask is not None:\n",
    "            assert attn_mask.dim() in (2, 3), (\n",
    "                \"For batched (3-D) `query`, expected `attn_mask` to be `None`, 2-D or 3-D\"\n",
    "                f\" but found {attn_mask.dim()}-D tensor instead\"\n",
    "            )\n",
    "    elif query.dim() == 2:\n",
    "        # Unbatched Inputs\n",
    "        is_batched = False\n",
    "        assert key.dim() == 2 and value.dim() == 2, (\n",
    "            \"For unbatched (2-D) `query`, expected `key` and `value` to be 2-D\"\n",
    "            f\" but found {key.dim()}-D and {value.dim()}-D tensors respectively\"\n",
    "        )\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            assert key_padding_mask.dim() == 1, (\n",
    "                \"For unbatched (2-D) `query`, expected `key_padding_mask` to be `None` or 1-D\"\n",
    "                f\" but found {key_padding_mask.dim()}-D tensor instead\"\n",
    "            )\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            assert attn_mask.dim() in (2, 3), (\n",
    "                \"For unbatched (2-D) `query`, expected `attn_mask` to be `None`, 2-D or 3-D\"\n",
    "                f\" but found {attn_mask.dim()}-D tensor instead\"\n",
    "            )\n",
    "            if attn_mask.dim() == 3:\n",
    "                expected_shape = (num_heads, query.shape[0], key.shape[0])\n",
    "                assert (\n",
    "                    attn_mask.shape == expected_shape\n",
    "                ), f\"Expected `attn_mask` shape to be {expected_shape} but got {attn_mask.shape}\"\n",
    "    else:\n",
    "        raise AssertionError(\n",
    "            f\"query should be unbatched 2D or batched 3D tensor but received {query.dim()}-D query tensor\"\n",
    "        )\n",
    "\n",
    "    return is_batched\n",
    "def _in_projection_packed(\n",
    "    q: Tensor,\n",
    "    k: Tensor,\n",
    "    v: Tensor,\n",
    "    w: Tensor,\n",
    "    b: Optional[Tensor] = None,\n",
    ") -> List[Tensor]:\n",
    "    r\"\"\"Perform the in-projection step of the attention operation, using packed weights.\n",
    "\n",
    "    Output is a triple containing projection tensors for query, key and value.\n",
    "\n",
    "    Args:\n",
    "        q, k, v: query, key and value tensors to be projected. For self-attention,\n",
    "            these are typically the same tensor; for encoder-decoder attention,\n",
    "            k and v are typically the same tensor. (We take advantage of these\n",
    "            identities for performance if they are present.) Regardless, q, k and v\n",
    "            must share a common embedding dimension; otherwise their shapes may vary.\n",
    "        w: projection weights for q, k and v, packed into a single tensor. Weights\n",
    "            are packed along dimension 0, in q, k, v order.\n",
    "        b: optional projection biases for q, k and v, packed into a single tensor\n",
    "            in q, k, v order.\n",
    "\n",
    "    Shape:\n",
    "        Inputs:\n",
    "        - q: :math:`(..., E)` where E is the embedding dimension\n",
    "        - k: :math:`(..., E)` where E is the embedding dimension\n",
    "        - v: :math:`(..., E)` where E is the embedding dimension\n",
    "        - w: :math:`(E * 3, E)` where E is the embedding dimension\n",
    "        - b: :math:`E * 3` where E is the embedding dimension\n",
    "\n",
    "        Output:\n",
    "        - in output list :math:`[q', k', v']`, each output tensor will have the\n",
    "            same shape as the corresponding input tensor.\n",
    "    \"\"\"\n",
    "    E = q.size(-1)\n",
    "    if k is v:\n",
    "        if q is k:\n",
    "            # self-attention\n",
    "            proj = linear(q, w, b)\n",
    "            # reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\n",
    "            proj = (\n",
    "                proj.unflatten(-1, (3, E))\n",
    "                .unsqueeze(0)\n",
    "                .transpose(0, -2)\n",
    "                .squeeze(-2)\n",
    "                .contiguous()\n",
    "            )\n",
    "            return proj[0], proj[1], proj[2]\n",
    "        else:\n",
    "            # encoder-decoder attention\n",
    "            w_q, w_kv = w.split([E, E * 2])\n",
    "            if b is None:\n",
    "                b_q = b_kv = None\n",
    "            else:\n",
    "                b_q, b_kv = b.split([E, E * 2])\n",
    "            q_proj = linear(q, w_q, b_q)\n",
    "            kv_proj = linear(k, w_kv, b_kv)\n",
    "            # reshape to 2, E and not E, 2 is deliberate for better memory coalescing and keeping same order as chunk()\n",
    "            kv_proj = (\n",
    "                kv_proj.unflatten(-1, (2, E))\n",
    "                .unsqueeze(0)\n",
    "                .transpose(0, -2)\n",
    "                .squeeze(-2)\n",
    "                .contiguous()\n",
    "            )\n",
    "            return (q_proj, kv_proj[0], kv_proj[1])\n",
    "    else:\n",
    "        w_q, w_k, w_v = w.chunk(3)\n",
    "        if b is None:\n",
    "            b_q = b_k = b_v = None\n",
    "        else:\n",
    "            b_q, b_k, b_v = b.chunk(3)\n",
    "        return linear(q, w_q, b_q), linear(k, w_k, b_k), linear(v, w_v, b_v)\n",
    "def softmax(\n",
    "    input: Tensor,\n",
    "    dim: Optional[int] = None,\n",
    "    _stacklevel: int = 3,\n",
    "    dtype: Optional[torch.dtype] = None,\n",
    ") -> Tensor:\n",
    "    r\"\"\"Apply a softmax function.\n",
    "\n",
    "    Softmax is defined as:\n",
    "\n",
    "    :math:`\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}`\n",
    "\n",
    "    It is applied to all slices along dim, and will re-scale them so that the elements\n",
    "    lie in the range `[0, 1]` and sum to 1.\n",
    "\n",
    "    See :class:`~torch.nn.Softmax` for more details.\n",
    "\n",
    "    Args:\n",
    "        input (Tensor): input\n",
    "        dim (int): A dimension along which softmax will be computed.\n",
    "        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
    "          If specified, the input tensor is casted to :attr:`dtype` before the operation\n",
    "          is performed. This is useful for preventing data type overflows. Default: None.\n",
    "\n",
    "    .. note::\n",
    "        This function doesn't work directly with NLLLoss,\n",
    "        which expects the Log to be computed between the Softmax and itself.\n",
    "        Use log_softmax instead (it's faster and has better numerical properties).\n",
    "\n",
    "    \"\"\"\n",
    "    if has_torch_function_unary(input):\n",
    "        return handle_torch_function(\n",
    "            softmax, (input,), input, dim=dim, _stacklevel=_stacklevel, dtype=dtype\n",
    "        )\n",
    "    if dim is None:\n",
    "        dim = _get_softmax_dim(\"softmax\", input.dim(), _stacklevel)\n",
    "    if dtype is None:\n",
    "        ret = input.softmax(dim)\n",
    "    else:\n",
    "        ret = input.softmax(dim, dtype=dtype)\n",
    "    return ret\n",
    "\n",
    "\n",
    "\n",
    "def build_features_and_labels(tree, transform_features=True):\n",
    "    # load arrays from the tree\n",
    "    a = tree.arrays(filter_name=['part_*', 'jet_pt', 'jet_energy', 'label_*'])\n",
    "\n",
    "    # compute new features\n",
    "    a['part_mask'] = ak.ones_like(a['part_energy'])\n",
    "    a['part_pt'] = np.hypot(a['part_px'], a['part_py'])\n",
    "    a['part_pt_log'] = np.log(a['part_pt'])\n",
    "    a['part_e_log'] = np.log(a['part_energy'])\n",
    "    a['part_logptrel'] = np.log(a['part_pt']/a['jet_pt'])\n",
    "    a['part_logerel'] = np.log(a['part_energy']/a['jet_energy'])\n",
    "    a['part_deltaR'] = np.hypot(a['part_deta'], a['part_dphi'])\n",
    "    a['part_d0'] = np.tanh(a['part_d0val'])\n",
    "    a['part_dz'] = np.tanh(a['part_dzval'])\n",
    "\n",
    "    # apply standardization\n",
    "    if transform_features:\n",
    "        a['part_pt_log'] = (a['part_pt_log'] - 1.7) * 0.7\n",
    "        a['part_e_log'] = (a['part_e_log'] - 2.0) * 0.7\n",
    "        a['part_logptrel'] = (a['part_logptrel'] - (-4.7)) * 0.7\n",
    "        a['part_logerel'] = (a['part_logerel'] - (-4.7)) * 0.7\n",
    "        a['part_deltaR'] = (a['part_deltaR'] - 0.2) * 4.0\n",
    "        a['part_d0err'] = _clip(a['part_d0err'], 0, 1)\n",
    "        a['part_dzerr'] = _clip(a['part_dzerr'], 0, 1)\n",
    "\n",
    "    feature_list = {\n",
    "        'pf_points': ['part_deta', 'part_dphi'], # not used in ParT\n",
    "        'pf_features': [\n",
    "            'part_pt_log',\n",
    "            'part_e_log',\n",
    "            'part_logptrel',\n",
    "            'part_logerel',\n",
    "            'part_deltaR',\n",
    "            'part_charge',\n",
    "            'part_isChargedHadron',\n",
    "            'part_isNeutralHadron',\n",
    "            'part_isPhoton',\n",
    "            'part_isElectron',\n",
    "            'part_isMuon',\n",
    "            'part_d0',\n",
    "            'part_d0err',\n",
    "            'part_dz',\n",
    "            'part_dzerr',\n",
    "            'part_deta',\n",
    "            'part_dphi',\n",
    "        ],\n",
    "        'pf_vectors': [\n",
    "            'part_px',\n",
    "            'part_py',\n",
    "            'part_pz',\n",
    "            'part_energy',\n",
    "        ],\n",
    "        'pf_mask': ['part_mask']\n",
    "    }\n",
    "\n",
    "    out = {}\n",
    "    for k, names in feature_list.items():\n",
    "        out[k] = np.stack([_pad(a[n], maxlen=128).to_numpy() for n in names], axis=1)\n",
    "\n",
    "    label_list = ['label_QCD', 'label_Hbb', 'label_Hcc', 'label_Hgg', 'label_H4q', 'label_Hqql', 'label_Zqq', 'label_Wqq', 'label_Tbqq', 'label_Tbl']\n",
    "    out['label'] = np.stack([a[n].to_numpy().astype('int') for n in label_list], axis=1)\n",
    "\n",
    "    return out\n",
    "\n",
    "def _clip(a, a_min, a_max):\n",
    "    try:\n",
    "        return np.clip(a, a_min, a_max)\n",
    "    except ValueError:\n",
    "        return ak.unflatten(np.clip(ak.flatten(a), a_min, a_max), ak.num(a))\n",
    "\n",
    "def _pad(a, maxlen, value=0, dtype='float32'):\n",
    "    if isinstance(a, np.ndarray) and a.ndim >= 2 and a.shape[1] == maxlen:\n",
    "        return a\n",
    "    elif isinstance(a, ak.Array):\n",
    "        if a.ndim == 1:\n",
    "            a = ak.unflatten(a, 1)\n",
    "        a = ak.fill_none(ak.pad_none(a, maxlen, clip=True), value)\n",
    "        return ak.values_astype(a, dtype)\n",
    "    else:\n",
    "        x = (np.ones((len(a), maxlen)) * value).astype(dtype)\n",
    "        for idx, s in enumerate(a):\n",
    "            if not len(s):\n",
    "                continue\n",
    "            trunc = s[:maxlen].astype(dtype)\n",
    "            x[idx, :len(trunc)] = trunc\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "''' Particle Transformer (ParT)\n",
    "\n",
    "Paper: \"Particle Transformer for Jet Tagging\" - https://arxiv.org/abs/2202.03772\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def delta_phi(a, b):\n",
    "    return (a - b + math.pi) % (2 * math.pi) - math.pi\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def delta_r2(eta1, phi1, eta2, phi2):\n",
    "    return (eta1 - eta2)**2 + delta_phi(phi1, phi2)**2\n",
    "\n",
    "\n",
    "def to_pt2(x, eps=1e-8):\n",
    "    pt2 = x[:, :2].square().sum(dim=1, keepdim=True)\n",
    "    if eps is not None:\n",
    "        pt2 = pt2.clamp(min=eps)\n",
    "    return pt2\n",
    "\n",
    "\n",
    "def to_m2(x, eps=1e-8):\n",
    "    m2 = x[:, 3:4].square() - x[:, :3].square().sum(dim=1, keepdim=True)\n",
    "    if eps is not None:\n",
    "        m2 = m2.clamp(min=eps)\n",
    "    return m2\n",
    "\n",
    "\n",
    "def atan2(y, x):\n",
    "    sx = torch.sign(x)\n",
    "    sy = torch.sign(y)\n",
    "    pi_part = (sy + sx * (sy ** 2 - 1)) * (sx - 1) * (-math.pi / 2)\n",
    "    atan_part = torch.arctan(y / (x + (1 - sx ** 2))) * sx ** 2\n",
    "    return atan_part + pi_part\n",
    "\n",
    "\n",
    "def to_ptrapphim(x, return_mass=True, eps=1e-8, for_onnx=False):\n",
    "    # x: (N, 4, ...), dim1 : (px, py, pz, E)\n",
    "    px, py, pz, energy = x.split((1, 1, 1, 1), dim=1)\n",
    "    pt = torch.sqrt(to_pt2(x, eps=eps))\n",
    "    # rapidity = 0.5 * torch.log((energy + pz) / (energy - pz))\n",
    "    rapidity = 0.5 * torch.log(1 + (2 * pz) / (energy - pz).clamp(min=1e-20))\n",
    "    phi = (atan2 if for_onnx else torch.atan2)(py, px)\n",
    "    if not return_mass:\n",
    "        return torch.cat((pt, rapidity, phi), dim=1)\n",
    "    else:\n",
    "        m = torch.sqrt(to_m2(x, eps=eps))\n",
    "        return torch.cat((pt, rapidity, phi, m), dim=1)\n",
    "\n",
    "\n",
    "def boost(x, boostp4, eps=1e-8):\n",
    "    # boost x to the rest frame of boostp4\n",
    "    # x: (N, 4, ...), dim1 : (px, py, pz, E)\n",
    "    p3 = -boostp4[:, :3] / boostp4[:, 3:].clamp(min=eps)\n",
    "    b2 = p3.square().sum(dim=1, keepdim=True)\n",
    "    gamma = (1 - b2).clamp(min=eps)**(-0.5)\n",
    "    gamma2 = (gamma - 1) / b2\n",
    "    gamma2.masked_fill_(b2 == 0, 0)\n",
    "    bp = (x[:, :3] * p3).sum(dim=1, keepdim=True)\n",
    "    v = x[:, :3] + gamma2 * bp * p3 + x[:, 3:] * gamma * p3\n",
    "    return v\n",
    "\n",
    "\n",
    "def p3_norm(p, eps=1e-8):\n",
    "    return p[:, :3] / p[:, :3].norm(dim=1, keepdim=True).clamp(min=eps)\n",
    "\n",
    "\n",
    "def pairwise_lv_fts(xi, xj, num_outputs=4, eps=1e-8, for_onnx=False):\n",
    "    pti, rapi, phii = to_ptrapphim(xi, False, eps=None, for_onnx=for_onnx).split((1, 1, 1), dim=1)\n",
    "    ptj, rapj, phij = to_ptrapphim(xj, False, eps=None, for_onnx=for_onnx).split((1, 1, 1), dim=1)\n",
    "\n",
    "    delta = delta_r2(rapi, phii, rapj, phij).sqrt()\n",
    "    lndelta = torch.log(delta.clamp(min=eps))\n",
    "    if num_outputs == 1:\n",
    "        return lndelta\n",
    "\n",
    "    if num_outputs > 1:\n",
    "        ptmin = ((pti <= ptj) * pti + (pti > ptj) * ptj) if for_onnx else torch.minimum(pti, ptj)\n",
    "        lnkt = torch.log((ptmin * delta).clamp(min=eps))\n",
    "        lnz = torch.log((ptmin / (pti + ptj).clamp(min=eps)).clamp(min=eps))\n",
    "        outputs = [lnkt, lnz, lndelta]\n",
    "\n",
    "    if num_outputs > 3:\n",
    "        xij = xi + xj\n",
    "        lnm2 = torch.log(to_m2(xij, eps=eps))\n",
    "        outputs.append(lnm2)\n",
    "\n",
    "    if num_outputs > 4:\n",
    "        lnds2 = torch.log(torch.clamp(-to_m2(xi - xj, eps=None), min=eps))\n",
    "        outputs.append(lnds2)\n",
    "\n",
    "    # the following features are not symmetric for (i, j)\n",
    "    if num_outputs > 5:\n",
    "        xj_boost = boost(xj, xij)\n",
    "        costheta = (p3_norm(xj_boost, eps=eps) * p3_norm(xij, eps=eps)).sum(dim=1, keepdim=True)\n",
    "        outputs.append(costheta)\n",
    "\n",
    "    if num_outputs > 6:\n",
    "        deltarap = rapi - rapj\n",
    "        deltaphi = delta_phi(phii, phij)\n",
    "        outputs += [deltarap, deltaphi]\n",
    "\n",
    "    assert (len(outputs) == num_outputs)\n",
    "    return torch.cat(outputs, dim=1)\n",
    "\n",
    "\n",
    "def build_sparse_tensor(uu, idx, seq_len):\n",
    "    # inputs: uu (N, C, num_pairs), idx (N, 2, num_pairs)\n",
    "    # return: (N, C, seq_len, seq_len)\n",
    "    batch_size, num_fts, num_pairs = uu.size()\n",
    "    idx = torch.min(idx, torch.ones_like(idx) * seq_len)\n",
    "    i = torch.cat((\n",
    "        torch.arange(0, batch_size, device=uu.device).repeat_interleave(num_fts * num_pairs).unsqueeze(0),\n",
    "        torch.arange(0, num_fts, device=uu.device).repeat_interleave(num_pairs).repeat(batch_size).unsqueeze(0),\n",
    "        idx[:, :1, :].expand_as(uu).flatten().unsqueeze(0),\n",
    "        idx[:, 1:, :].expand_as(uu).flatten().unsqueeze(0),\n",
    "    ), dim=0)\n",
    "    return torch.sparse_coo_tensor(\n",
    "        i, uu.flatten(),\n",
    "        size=(batch_size, num_fts, seq_len + 1, seq_len + 1),\n",
    "        device=uu.device).to_dense()[:, :, :seq_len, :seq_len]\n",
    "\n",
    "\n",
    "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
    "    # From https://github.com/rwightman/pytorch-image-models/blob/18ec173f95aa220af753358bf860b16b6691edb2/timm/layers/weight_init.py#L8\n",
    "    r\"\"\"Fills the input Tensor with values drawn from a truncated\n",
    "    normal distribution. The values are effectively drawn from the\n",
    "    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n",
    "    with values outside :math:`[a, b]` redrawn until they are within\n",
    "    the bounds. The method used for generating the random values works\n",
    "    best when :math:`a \\leq \\text{mean} \\leq b`.\n",
    "    Args:\n",
    "        tensor: an n-dimensional `torch.Tensor`\n",
    "        mean: the mean of the normal distribution\n",
    "        std: the standard deviation of the normal distribution\n",
    "        a: the minimum cutoff value\n",
    "        b: the maximum cutoff value\n",
    "    Examples:\n",
    "        >>> w = torch.empty(3, 5)\n",
    "        >>> nn.init.trunc_normal_(w)\n",
    "    \"\"\"\n",
    "    def norm_cdf(x):\n",
    "        # Computes standard normal cumulative distribution function\n",
    "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
    "\n",
    "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
    "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
    "                      \"The distribution of values may be incorrect.\",\n",
    "                      stacklevel=2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Values are generated by using a truncated uniform distribution and\n",
    "        # then using the inverse CDF for the normal distribution.\n",
    "        # Get upper and lower cdf values\n",
    "        l = norm_cdf((a - mean) / std)\n",
    "        u = norm_cdf((b - mean) / std)\n",
    "\n",
    "        # Uniformly fill tensor with values from [l, u], then translate to\n",
    "        # [2l-1, 2u-1].\n",
    "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
    "\n",
    "        # Use inverse cdf transform for normal distribution to get truncated\n",
    "        # standard normal\n",
    "        tensor.erfinv_()\n",
    "\n",
    "        # Transform to proper mean, std\n",
    "        tensor.mul_(std * math.sqrt(2.))\n",
    "        tensor.add_(mean)\n",
    "\n",
    "        # Clamp to ensure it's in the proper range\n",
    "        tensor.clamp_(min=a, max=b)\n",
    "        return tensor\n",
    "\n",
    "\n",
    "class SequenceTrimmer(nn.Module):\n",
    "\n",
    "    def __init__(self, enabled=False, target=(0.9, 1.02), **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.enabled = enabled\n",
    "        self.target = target\n",
    "        self._counter = 0\n",
    "\n",
    "    def forward(self, x, v=None, mask=None, uu=None, **kwargs):\n",
    "        # x: (N, C, P)\n",
    "        # v: (N, 4, P) [px,py,pz,energy]\n",
    "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
    "        # uu: (N, C', P, P)\n",
    "        if mask is None:\n",
    "            mask = torch.ones_like(x[:, :1])\n",
    "        mask = mask.bool()\n",
    "\n",
    "        if self.enabled:\n",
    "            if self._counter < 5:\n",
    "                self._counter += 1\n",
    "            else:\n",
    "                if self.training:\n",
    "                    q = min(1, random.uniform(*self.target))\n",
    "                    maxlen = torch.quantile(mask.type_as(x).sum(dim=-1), q).long()\n",
    "                    rand = torch.rand_like(mask.type_as(x))\n",
    "                    rand.masked_fill_(~mask, -1)\n",
    "                    perm = rand.argsort(dim=-1, descending=True)  # (N, 1, P)\n",
    "                    mask = torch.gather(mask, -1, perm)\n",
    "                    x = torch.gather(x, -1, perm.expand_as(x))\n",
    "                    if v is not None:\n",
    "                        v = torch.gather(v, -1, perm.expand_as(v))\n",
    "                    if uu is not None:\n",
    "                        uu = torch.gather(uu, -2, perm.unsqueeze(-1).expand_as(uu))\n",
    "                        uu = torch.gather(uu, -1, perm.unsqueeze(-2).expand_as(uu))\n",
    "                else:\n",
    "                    maxlen = mask.sum(dim=-1).max()\n",
    "                maxlen = max(maxlen, 1)\n",
    "                if maxlen < mask.size(-1):\n",
    "                    mask = mask[:, :, :maxlen]\n",
    "                    x = x[:, :, :maxlen]\n",
    "                    if v is not None:\n",
    "                        v = v[:, :, :maxlen]\n",
    "                    if uu is not None:\n",
    "                        uu = uu[:, :, :maxlen, :maxlen]\n",
    "\n",
    "        return x, v, mask, uu\n",
    "\n",
    "\n",
    "class Embed(nn.Module):\n",
    "    def __init__(self, input_dim, dims, normalize_input=True, activation='gelu'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_bn = nn.BatchNorm1d(input_dim) if normalize_input else None\n",
    "        module_list = []\n",
    "        for dim in dims:\n",
    "            module_list.extend([\n",
    "                nn.LayerNorm(input_dim),\n",
    "                nn.Linear(input_dim, dim),\n",
    "                nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
    "            ])\n",
    "            input_dim = dim\n",
    "        self.embed = nn.Sequential(*module_list)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        if self.input_bn is not None:\n",
    "            # x: (batch, embed_dim, seq_len)\n",
    "            x = self.input_bn(x)\n",
    "            x = x.permute(2, 0, 1).contiguous()\n",
    "        # x: (seq_len, batch, embed_dim)\n",
    "        return self.embed(x)\n",
    "\n",
    "\n",
    "class PairEmbed(nn.Module):\n",
    "    def __init__(\n",
    "            self, pairwise_lv_dim, pairwise_input_dim, dims,\n",
    "            remove_self_pair=False, use_pre_activation_pair=True, mode='sum',\n",
    "            normalize_input=True, activation='gelu', eps=1e-8,\n",
    "            for_onnx=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pairwise_lv_dim = pairwise_lv_dim\n",
    "        self.pairwise_input_dim = pairwise_input_dim\n",
    "        self.is_symmetric = (pairwise_lv_dim <= 5) and (pairwise_input_dim == 0)\n",
    "        self.remove_self_pair = remove_self_pair\n",
    "        self.mode = mode\n",
    "        self.for_onnx = for_onnx\n",
    "        self.pairwise_lv_fts = partial(pairwise_lv_fts, num_outputs=pairwise_lv_dim, eps=eps, for_onnx=for_onnx)\n",
    "        self.out_dim = dims[-1]\n",
    "\n",
    "        if self.mode == 'concat':\n",
    "            input_dim = pairwise_lv_dim + pairwise_input_dim\n",
    "            module_list = [nn.BatchNorm1d(input_dim)] if normalize_input else []\n",
    "            for dim in dims:\n",
    "                module_list.extend([\n",
    "                    nn.Conv1d(input_dim, dim, 1),\n",
    "                    nn.BatchNorm1d(dim),\n",
    "                    nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
    "                ])\n",
    "                input_dim = dim\n",
    "            if use_pre_activation_pair:\n",
    "                module_list = module_list[:-1]\n",
    "            self.embed = nn.Sequential(*module_list)\n",
    "        elif self.mode == 'sum':\n",
    "            if pairwise_lv_dim > 0:\n",
    "                input_dim = pairwise_lv_dim\n",
    "                module_list = [nn.BatchNorm1d(input_dim)] if normalize_input else []\n",
    "                for dim in dims:\n",
    "                    module_list.extend([\n",
    "                        nn.Conv1d(input_dim, dim, 1),\n",
    "                        nn.BatchNorm1d(dim),\n",
    "                        nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
    "                    ])\n",
    "                    input_dim = dim\n",
    "                if use_pre_activation_pair:\n",
    "                    module_list = module_list[:-1]\n",
    "                self.embed = nn.Sequential(*module_list)\n",
    "\n",
    "            if pairwise_input_dim > 0:\n",
    "                input_dim = pairwise_input_dim\n",
    "                module_list = [nn.BatchNorm1d(input_dim)] if normalize_input else []\n",
    "                for dim in dims:\n",
    "                    module_list.extend([\n",
    "                        nn.Conv1d(input_dim, dim, 1),\n",
    "                        nn.BatchNorm1d(dim),\n",
    "                        nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
    "                    ])\n",
    "                    input_dim = dim\n",
    "                if use_pre_activation_pair:\n",
    "                    module_list = module_list[:-1]\n",
    "                self.fts_embed = nn.Sequential(*module_list)\n",
    "        else:\n",
    "            raise RuntimeError('`mode` can only be `sum` or `concat`')\n",
    "\n",
    "    def forward(self, x, uu=None, **kwargs):\n",
    "        # x: (batch, v_dim, seq_len)\n",
    "        # uu: (batch, v_dim, seq_len, seq_len)\n",
    "        assert (x is not None or uu is not None)\n",
    "        with torch.no_grad():\n",
    "            if x is not None:\n",
    "                batch_size, _, seq_len = x.size()\n",
    "            else:\n",
    "                batch_size, _, seq_len, _ = uu.size()\n",
    "            if self.is_symmetric and not self.for_onnx:\n",
    "                i, j = torch.tril_indices(seq_len, seq_len, offset=-1 if self.remove_self_pair else 0,\n",
    "                                          device=(x if x is not None else uu).device)\n",
    "                if x is not None:\n",
    "                    x = x.unsqueeze(-1).repeat(1, 1, 1, seq_len)\n",
    "                    xi = x[:, :, i, j]  # (batch, dim, seq_len*(seq_len+1)/2)\n",
    "                    xj = x[:, :, j, i]\n",
    "                    x = self.pairwise_lv_fts(xi, xj)\n",
    "                if uu is not None:\n",
    "                    # (batch, dim, seq_len*(seq_len+1)/2)\n",
    "                    uu = uu[:, :, i, j]\n",
    "            else:\n",
    "                if x is not None:\n",
    "                    x = self.pairwise_lv_fts(x.unsqueeze(-1), x.unsqueeze(-2))\n",
    "                    if self.remove_self_pair:\n",
    "                        i = torch.arange(0, seq_len, device=x.device)\n",
    "                        x[:, :, i, i] = 0\n",
    "                    x = x.view(-1, self.pairwise_lv_dim, seq_len * seq_len)\n",
    "                if uu is not None:\n",
    "                    uu = uu.view(-1, self.pairwise_input_dim, seq_len * seq_len)\n",
    "            if self.mode == 'concat':\n",
    "                if x is None:\n",
    "                    pair_fts = uu\n",
    "                elif uu is None:\n",
    "                    pair_fts = x\n",
    "                else:\n",
    "                    pair_fts = torch.cat((x, uu), dim=1)\n",
    "\n",
    "        if self.mode == 'concat':\n",
    "            elements = self.embed(pair_fts)  # (batch, embed_dim, num_elements)\n",
    "        elif self.mode == 'sum':\n",
    "            if x is None:\n",
    "                elements = self.fts_embed(uu)\n",
    "            elif uu is None:\n",
    "                elements = self.embed(x)\n",
    "            else:\n",
    "                elements = self.embed(x) + self.fts_embed(uu)\n",
    "\n",
    "        if self.is_symmetric and not self.for_onnx:\n",
    "            y = torch.zeros(batch_size, self.out_dim, seq_len, seq_len, dtype=elements.dtype, device=elements.device)\n",
    "            y[:, :, i, j] = elements\n",
    "            y[:, :, j, i] = elements\n",
    "        else:\n",
    "            y = elements.view(-1, self.out_dim, seq_len, seq_len)\n",
    "        return y\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, embed_dim=128, num_heads=8, ffn_ratio=4,\n",
    "                 dropout=0.1, attn_dropout=0.1, activation_dropout=0.1,\n",
    "                 add_bias_kv=False, activation='gelu',\n",
    "                 scale_fc=True, scale_attn=True, scale_heads=True, scale_resids=True, return_pre_softmax=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.ffn_dim = embed_dim * ffn_ratio\n",
    "        self.return_pre_softmax = return_pre_softmax\n",
    "\n",
    "        self.pre_attn_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.attn = MultiheadAttention(\n",
    "                embed_dim, \n",
    "                num_heads, \n",
    "                return_pre_softmax=self.return_pre_softmax,\n",
    "                add_bias_kv=add_bias_kv\n",
    "                )\n",
    "\n",
    "        self.post_attn_norm = nn.LayerNorm(embed_dim) if scale_attn else None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.pre_fc_norm = nn.LayerNorm(embed_dim)\n",
    "        self.fc1 = nn.Linear(embed_dim, self.ffn_dim)\n",
    "        self.act = nn.GELU() if activation == 'gelu' else nn.ReLU()\n",
    "        self.act_dropout = nn.Dropout(activation_dropout)\n",
    "        self.post_fc_norm = nn.LayerNorm(self.ffn_dim) if scale_fc else None\n",
    "        self.fc2 = nn.Linear(self.ffn_dim, embed_dim)\n",
    "\n",
    "        self.c_attn = nn.Parameter(torch.ones(num_heads), requires_grad=True) if scale_heads else None\n",
    "        self.w_resid = nn.Parameter(torch.ones(embed_dim), requires_grad=True) if scale_resids else None\n",
    "\n",
    "    def forward(self, x, x_cls=None, padding_mask=None, attn_mask=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
    "            x_cls (Tensor, optional): class token input to the layer of shape `(1, batch, embed_dim)`\n",
    "            padding_mask (ByteTensor, optional): binary\n",
    "                ByteTensor of shape `(batch, seq_len)` where padding\n",
    "                elements are indicated by ``1``.\n",
    "\n",
    "        Returns:\n",
    "            encoded output of shape `(seq_len, batch, embed_dim)`\n",
    "        \"\"\"\n",
    "\n",
    "        if x_cls is not None:\n",
    "            with torch.no_grad():\n",
    "                # prepend one element for x_cls: -> (batch, 1+seq_len)\n",
    "                padding_mask = torch.cat((torch.zeros_like(padding_mask[:, :1]), padding_mask), dim=1)\n",
    "            # class attention: https://arxiv.org/pdf/2103.17239.pdf\n",
    "            residual = x_cls\n",
    "            u = torch.cat((x_cls, x), dim=0)  # (seq_len+1, batch, embed_dim)\n",
    "            u = self.pre_attn_norm(u)\n",
    "\n",
    "            if self.return_pre_softmax:\n",
    "                dict_output = self.attn(x_cls, u, u, key_padding_mask=padding_mask, \n",
    "                                                                                     return_pre_softmax=self.return_pre_softmax\n",
    "                                                                            )  # (1, batch, embed_dim)\n",
    "                x = dict_output['output']  # (1, batch, embed_dim)\n",
    "                pre_softmax_attention = dict_output['pre_softmax_attention']\n",
    "                pre_softmax_interaction = dict_output['pre_softmax_interaction']\n",
    "\n",
    "                pre_softmax_attention.detach()\n",
    "                pre_softmax_interaction.detach()\n",
    "\n",
    "            else:\n",
    "                x, _ = self.attn(x_cls, u, u, key_padding_mask=padding_mask, \n",
    "                                    return_pre_softmax=self.return_pre_softmax\n",
    "                                    )  # (1, batch, embed_dim)\n",
    "\n",
    "        else:            \n",
    "            residual = x\n",
    "            x = self.pre_attn_norm(x)\n",
    "            \n",
    "            if self.return_pre_softmax:\n",
    "                dict_output = self.attn(x, x, x, key_padding_mask=padding_mask, attn_mask=attn_mask,\n",
    "                                         return_pre_softmax=self.return_pre_softmax\n",
    "                                         )\n",
    "                x = dict_output['output']\n",
    "                pre_softmax_attention = dict_output['pre_softmax_attention']\n",
    "                pre_softmax_interaction = dict_output['pre_softmax_interaction']\n",
    "\n",
    "                pre_softmax_attention.detach()\n",
    "                pre_softmax_interaction.detach()\n",
    "\n",
    "            else:\n",
    "                x, _ = self.attn(x, x, x, key_padding_mask=padding_mask,\n",
    "                                attn_mask=attn_mask, return_pre_softmax=self.return_pre_softmax\n",
    "                                )  # (seq_len, batch, embed_dim)\n",
    "\n",
    "        if self.c_attn is not None:\n",
    "            tgt_len = x.size(0)\n",
    "            x = x.view(tgt_len, -1, self.num_heads, self.head_dim)\n",
    "            x = torch.einsum('tbhd,h->tbdh', x, self.c_attn)\n",
    "            x = x.reshape(tgt_len, -1, self.embed_dim)\n",
    "        if self.post_attn_norm is not None:\n",
    "            x = self.post_attn_norm(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x += residual.reshape(x.shape)\n",
    "\n",
    "        residual = x\n",
    "        x = self.pre_fc_norm(x)\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.act_dropout(x)\n",
    "        if self.post_fc_norm is not None:\n",
    "            x = self.post_fc_norm(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        if self.w_resid is not None:\n",
    "            residual = torch.mul(self.w_resid, residual)\n",
    "        x += residual\n",
    "\n",
    "        if self.return_pre_softmax:\n",
    "            return {'output': x, 'pre_softmax_attention': pre_softmax_attention, 'pre_softmax_interaction': pre_softmax_interaction}\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class ParticleTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 num_classes=None,\n",
    "                 # network configurations\n",
    "                 pair_input_dim=4,\n",
    "                 pair_extra_dim=0,\n",
    "                 remove_self_pair=False,\n",
    "                 use_pre_activation_pair=True,\n",
    "                 embed_dims=[128, 512, 128],\n",
    "                 pair_embed_dims=[64, 64, 64],\n",
    "                 num_heads=8,\n",
    "                 num_layers=8,\n",
    "                 num_cls_layers=2,\n",
    "                 block_params=None,\n",
    "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
    "                 fc_params=[],\n",
    "                 activation='gelu',\n",
    "                 # misc\n",
    "                 trim=True,\n",
    "                 for_inference=False,\n",
    "                 use_amp=False,\n",
    "                 return_pre_softmax=False,\n",
    "                 **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
    "        self.for_inference = for_inference\n",
    "        self.use_amp = use_amp\n",
    "        self.return_pre_softmax = return_pre_softmax\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # init the collected pre_softmax matrices for later torch.cat()\n",
    "\n",
    "        #if self.return_pre_softmax:\n",
    "        #    self.pre_softmax_attention = torch.empty(0, dtype=torch.float32)\n",
    "        #    self.pre_softmax_interaction = torch.empty(0, dtype=torch.float32)\n",
    "        #    self.cls_pre_softmax_attention = torch.empty(0, dtype=torch.float32)\n",
    "        #    self.cls_pre_softmax_interaction = torch.empty(0, dtype=torch.float32)\n",
    "\n",
    "        embed_dim = embed_dims[-1] if len(embed_dims) > 0 else input_dim\n",
    "        self.default_cfg = default_cfg = dict(embed_dim=embed_dim, num_heads=num_heads, ffn_ratio=4,\n",
    "                           dropout=0.1, attn_dropout=0.1, activation_dropout=0.1,\n",
    "                           add_bias_kv=False, activation=activation,\n",
    "                           scale_fc=True, scale_attn=True, scale_heads=True, scale_resids=True, return_pre_softmax=self.return_pre_softmax)\n",
    "                           \n",
    "\n",
    "        cfg_block = copy.deepcopy(default_cfg)\n",
    "        if block_params is not None:\n",
    "            cfg_block.update(block_params)\n",
    "        _logger.info('cfg_block: %s' % str(cfg_block))\n",
    "\n",
    "        cfg_cls_block = copy.deepcopy(default_cfg)\n",
    "        if cls_block_params is not None:\n",
    "            cfg_cls_block.update(cls_block_params)\n",
    "        _logger.info('cfg_cls_block: %s' % str(cfg_cls_block))\n",
    "\n",
    "        self.pair_extra_dim = pair_extra_dim\n",
    "        self.embed = Embed(input_dim, embed_dims, activation=activation) if len(embed_dims) > 0 else nn.Identity()\n",
    "        self.pair_embed = PairEmbed(\n",
    "            pair_input_dim, pair_extra_dim, pair_embed_dims + [cfg_block['num_heads']],\n",
    "            remove_self_pair=remove_self_pair, use_pre_activation_pair=use_pre_activation_pair,\n",
    "            for_onnx=for_inference) if pair_embed_dims is not None and pair_input_dim + pair_extra_dim > 0 else None\n",
    "        # we want to add descriptive layer names, so use ModuleDict instead of List\n",
    "        #self.blocks = nn.ModuleDict({f'Block_{i}': Block(**cfg_block) for i in range(num_layers)})\n",
    "        #self.cls_blocks = nn.ModuleDict({f'cls_Block_{i}': Block(**cfg_cls_block) for i in range(num_cls_layers)})\n",
    "\n",
    "        self.blocks = nn.ModuleList([Block(**cfg_block) for _ in range(num_layers)])\n",
    "        self.cls_blocks = nn.ModuleList([Block(**cfg_cls_block) for _ in range(num_cls_layers)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        if fc_params is not None:\n",
    "            fcs = []\n",
    "            in_dim = embed_dim\n",
    "            for out_dim, drop_rate in fc_params:\n",
    "                fcs.append(nn.Sequential(nn.Linear(in_dim, out_dim), nn.ReLU(), nn.Dropout(drop_rate)))\n",
    "                in_dim = out_dim\n",
    "            fcs.append(nn.Linear(in_dim, num_classes))\n",
    "            self.fc = nn.Sequential(*fcs)\n",
    "        else:\n",
    "            self.fc = None\n",
    "\n",
    "        # init\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim), requires_grad=True)\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'cls_token', }\n",
    "\n",
    "    def forward(self, x, v=None, mask=None, uu=None, uu_idx=None, **kwargs):\n",
    "        # x: (N, C, P)\n",
    "        # v: (N, 4, P) [px,py,pz,energy]\n",
    "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
    "        # for pytorch: uu (N, C', num_pairs), uu_idx (N, 2, num_pairs)\n",
    "        # for onnx: uu (N, C', P, P), uu_idx=None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if not self.for_inference:\n",
    "                if uu_idx is not None:\n",
    "                    uu = build_sparse_tensor(uu, uu_idx, x.size(-1))\n",
    "            x, v, mask, uu = self.trimmer(x, v, mask, uu)\n",
    "            padding_mask = ~mask.squeeze(1)  # (N, P)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
    "            # input embedding\n",
    "            x = self.embed(x).masked_fill(~mask.permute(2, 0, 1), 0)  # (P, N, C)\n",
    "            attn_mask = None\n",
    "            if (v is not None or uu is not None) and self.pair_embed is not None:\n",
    "                attn_mask = self.pair_embed(v, uu).view(-1, v.size(-1), v.size(-1))  # (N*num_heads, P, P)\n",
    "\n",
    "            # transform\n",
    "            for block in self.blocks:\n",
    "                if self.return_pre_softmax:\n",
    "                    #x, pre_softmax_attention_vals, pre_softmax_interaction_vals = block(x, x_cls=None, padding_mask=padding_mask, \n",
    "                    #                                                          attn_mask=attn_mask, return_pre_softmax=self.return_pre_softmax)['output']\n",
    "                    #pre_softmax_attention_vals = pre_softmax_attention_vals.unsqueeze(0)  # (N, num_heads, P, P)\n",
    "                    #pre_softmax_interaction_vals = pre_softmax_interaction_vals.unsqueeze(0)  # (N, num_heads, P, P)\n",
    "\n",
    "                    #self.pre_softmax_attention = torch.cat((self.pre_softmax_attention, pre_softmax_attention_vals), dim=0)\n",
    "                    #self.pre_softmax_interaction = torch.cat((self.pre_softmax_interaction, pre_softmax_interaction_vals), dim=0)\n",
    "                    x = block(x, x_cls=None, padding_mask=padding_mask, attn_mask=attn_mask, return_pre_softmax=self.return_pre_softmax)['output']\n",
    "                else:\n",
    "                    x = block(x, x_cls=None, padding_mask=padding_mask, attn_mask=attn_mask, return_pre_softmax=self.return_pre_softmax)\n",
    "\n",
    "            # extract class token\n",
    "            cls_tokens = self.cls_token.expand(1, x.size(1), -1)  # (1, N, C)\n",
    "            for block in self.cls_blocks:\n",
    "                if self.return_pre_softmax:\n",
    "                    cls_tokens = block(x, x_cls=cls_tokens, padding_mask=padding_mask, return_pre_softmax=self.return_pre_softmax)['output']\n",
    "                else:\n",
    "                    cls_tokens = block(x, x_cls=cls_tokens, padding_mask=padding_mask)\n",
    "\n",
    "            x_cls = self.norm(cls_tokens).squeeze(0)\n",
    "\n",
    "            # fc\n",
    "            if self.fc is None:\n",
    "                return x_cls\n",
    "            output = self.fc(x_cls)\n",
    "            if self.for_inference:\n",
    "                output = torch.softmax(output, dim=1)\n",
    "            # print('output:\\n', output)\n",
    "            if self.return_pre_softmax:\n",
    "                return output#, self.pre_softmax_attention, self.pre_softmax_interaction\n",
    "            else:\n",
    "                return output\n",
    "\n",
    "\n",
    "class ParticleTransformerTagger(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 pf_input_dim,\n",
    "                 sv_input_dim,\n",
    "                 num_classes=None,\n",
    "                 # network configurations\n",
    "                 pair_input_dim=4,\n",
    "                 pair_extra_dim=0,\n",
    "                 remove_self_pair=False,\n",
    "                 use_pre_activation_pair=True,\n",
    "                 embed_dims=[128, 512, 128],\n",
    "                 pair_embed_dims=[64, 64, 64],\n",
    "                 num_heads=8,\n",
    "                 num_layers=8,\n",
    "                 num_cls_layers=2,\n",
    "                 block_params=None,\n",
    "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
    "                 fc_params=[],\n",
    "                 activation='gelu',\n",
    "                 # misc\n",
    "                 trim=True,\n",
    "                 for_inference=False,\n",
    "                 use_amp=False,\n",
    "                 **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.use_amp = use_amp\n",
    "\n",
    "        self.pf_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
    "        self.sv_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
    "\n",
    "        self.pf_embed = Embed(pf_input_dim, embed_dims, activation=activation)\n",
    "        self.sv_embed = Embed(sv_input_dim, embed_dims, activation=activation)\n",
    "\n",
    "        self.part = ParticleTransformer(input_dim=embed_dims[-1],\n",
    "                                        num_classes=num_classes,\n",
    "                                        # network configurations\n",
    "                                        pair_input_dim=pair_input_dim,\n",
    "                                        pair_extra_dim=pair_extra_dim,\n",
    "                                        remove_self_pair=remove_self_pair,\n",
    "                                        use_pre_activation_pair=use_pre_activation_pair,\n",
    "                                        embed_dims=[],\n",
    "                                        pair_embed_dims=pair_embed_dims,\n",
    "                                        num_heads=num_heads,\n",
    "                                        num_layers=num_layers,\n",
    "                                        num_cls_layers=num_cls_layers,\n",
    "                                        block_params=block_params,\n",
    "                                        cls_block_params=cls_block_params,\n",
    "                                        fc_params=fc_params,\n",
    "                                        activation=activation,\n",
    "                                        # misc\n",
    "                                        trim=False,\n",
    "                                        for_inference=for_inference,\n",
    "                                        use_amp=use_amp)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'part.cls_token', }\n",
    "\n",
    "    def forward(self, pf_x, pf_v=None, pf_mask=None, sv_x=None, sv_v=None, sv_mask=None, **kwargs):\n",
    "        # x: (N, C, P)\n",
    "        # v: (N, 4, P) [px,py,pz,energy]\n",
    "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pf_x, pf_v, pf_mask, _ = self.pf_trimmer(pf_x, pf_v, pf_mask)\n",
    "            sv_x, sv_v, sv_mask, _ = self.sv_trimmer(sv_x, sv_v, sv_mask)\n",
    "            v = torch.cat([pf_v, sv_v], dim=2)\n",
    "            mask = torch.cat([pf_mask, sv_mask], dim=2)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
    "            pf_x = self.pf_embed(pf_x)  # after embed: (seq_len, batch, embed_dim)\n",
    "            sv_x = self.sv_embed(sv_x)\n",
    "            x = torch.cat([pf_x, sv_x], dim=0)\n",
    "\n",
    "            return self.part(x, v, mask)\n",
    "\n",
    "\n",
    "class ParticleTransformerTaggerWithExtraPairFeatures(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 pf_input_dim,\n",
    "                 sv_input_dim,\n",
    "                 num_classes=None,\n",
    "                 # network configurations\n",
    "                 pair_input_dim=4,\n",
    "                 pair_extra_dim=0,\n",
    "                 remove_self_pair=False,\n",
    "                 use_pre_activation_pair=True,\n",
    "                 embed_dims=[128, 512, 128],\n",
    "                 pair_embed_dims=[64, 64, 64],\n",
    "                 num_heads=8,\n",
    "                 num_layers=8,\n",
    "                 num_cls_layers=2,\n",
    "                 block_params=None,\n",
    "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
    "                 fc_params=[],\n",
    "                 activation='gelu',\n",
    "                 # misc\n",
    "                 trim=True,\n",
    "                 for_inference=False,\n",
    "                 use_amp=False,\n",
    "                 **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.use_amp = use_amp\n",
    "        self.for_inference = for_inference\n",
    "\n",
    "        self.pf_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
    "        self.sv_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
    "\n",
    "        self.pf_embed = Embed(pf_input_dim, embed_dims, activation=activation)\n",
    "        self.sv_embed = Embed(sv_input_dim, embed_dims, activation=activation)\n",
    "\n",
    "        self.part = ParticleTransformer(input_dim=embed_dims[-1],\n",
    "                                        num_classes=num_classes,\n",
    "                                        # network configurations\n",
    "                                        pair_input_dim=pair_input_dim,\n",
    "                                        pair_extra_dim=pair_extra_dim,\n",
    "                                        remove_self_pair=remove_self_pair,\n",
    "                                        use_pre_activation_pair=use_pre_activation_pair,\n",
    "                                        embed_dims=[],\n",
    "                                        pair_embed_dims=pair_embed_dims,\n",
    "                                        num_heads=num_heads,\n",
    "                                        num_layers=num_layers,\n",
    "                                        num_cls_layers=num_cls_layers,\n",
    "                                        block_params=block_params,\n",
    "                                        cls_block_params=cls_block_params,\n",
    "                                        fc_params=fc_params,\n",
    "                                        activation=activation,\n",
    "                                        # misc\n",
    "                                        trim=False,\n",
    "                                        for_inference=for_inference,\n",
    "                                        use_amp=use_amp)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'part.cls_token', }\n",
    "\n",
    "    def forward(self, pf_x, pf_v=None, pf_mask=None, sv_x=None, sv_v=None, sv_mask=None, pf_uu=None, pf_uu_idx=None, **kwargs):\n",
    "        # x: (N, C, P)\n",
    "        # v: (N, 4, P) [px,py,pz,energy]\n",
    "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if not self.for_inference:\n",
    "                if pf_uu_idx is not None:\n",
    "                    pf_uu = build_sparse_tensor(pf_uu, pf_uu_idx, pf_x.size(-1))\n",
    "\n",
    "            pf_x, pf_v, pf_mask, pf_uu = self.pf_trimmer(pf_x, pf_v, pf_mask, pf_uu)\n",
    "            sv_x, sv_v, sv_mask, _ = self.sv_trimmer(sv_x, sv_v, sv_mask)\n",
    "            v = torch.cat([pf_v, sv_v], dim=2)\n",
    "            mask = torch.cat([pf_mask, sv_mask], dim=2)\n",
    "            uu = torch.zeros(v.size(0), pf_uu.size(1), v.size(2), v.size(2), dtype=v.dtype, device=v.device)\n",
    "            uu[:, :, :pf_x.size(2), :pf_x.size(2)] = pf_uu\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
    "            pf_x = self.pf_embed(pf_x)  # after embed: (seq_len, batch, embed_dim)\n",
    "            sv_x = self.sv_embed(sv_x)\n",
    "            x = torch.cat([pf_x, sv_x], dim=0)\n",
    "\n",
    "            return self.part(x, v, mask, uu)\n",
    "\n",
    "class ParticleTransformerWrapper(torch.nn.Module):\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.mod = ParticleTransformer(**kwargs)\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'mod.cls_token', }\n",
    "\n",
    "    def forward(self, points, features, lorentz_vectors, mask):\n",
    "        #if 'return_pre_softmax' in self.kwargs.keys() and self.kwargs['return_pre_softmax'] == True :\n",
    "        #    output, pre_softmax_attention, pre_softmax_interaction = self.mod(features, v=lorentz_vectors, mask=mask)\n",
    "        #    return output, pre_softmax_attention, pre_softmax_interaction\n",
    "        #else:\n",
    "        output = self.mod(features, v=lorentz_vectors, mask=mask)\n",
    "        return output\n",
    "\n",
    "# Define Forward Hook to grab pre-softmax w/o altering outputs of forward() for ParticleTransformer\n",
    "\n",
    "class Pre_Softmax_Hook:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.kwargs = self.model.kwargs\n",
    "\n",
    "        if isinstance(self.model, ParticleTransformerWrapper):\n",
    "            self.model = self.model.mod\n",
    "\n",
    "        for module in self.model.blocks:\n",
    "            #if layer_name in name:\n",
    "                # register forward hook functions\n",
    "            handle_attn = module.register_forward_hook(lambda *args, **kwargs: Pre_Softmax_Hook.get_pre_softmax_attention(self, *args, **kwargs))\n",
    "            handle_inter = module.register_forward_hook(lambda *args, **kwargs: Pre_Softmax_Hook.get_pre_softmax_interaction(self, *args, **kwargs))\n",
    "\n",
    "            print(f\"Registered hook onto module\")\n",
    "        #else:\n",
    "        #    print(f'A Layer does not match')\n",
    "\n",
    "        # for now, we will not register hooks on class blocks\n",
    "        # would not be too terribly difficult to implement, but it is not really the focus of current experiment\n",
    "\n",
    "        #for module in self.model.cls_blocks:\n",
    "            #if layer_name in name:\n",
    "                # register forward hook function - no interaction in class blocks\n",
    "            #cls_handle_attn = module.register_forward_hook(lambda *args, **kwargs: Pre_Softmax_Hook.get_pre_softmax_attention(self, *args, **kwargs))\n",
    "            \n",
    "            #print(f\"Registered hook onto module\")\n",
    "        #else:\n",
    "        #    print(f'Layer {module} does not match {layer_name}')\n",
    "\n",
    "        self.handle_attn = handle_attn\n",
    "        self.handle_inter = handle_inter\n",
    "        #self.cls_handle_attn = cls_handle_attn\n",
    "\n",
    "        self.embed_dim = self.model.default_cfg['embed_dim']\n",
    "        self.num_heads = self.model.default_cfg['num_heads']\n",
    "        self.seq_len = self.embed_dim\n",
    "\n",
    "        self.pre_softmax_attentions = torch.empty((0, self.num_heads, self.seq_len, self.seq_len), dtype=torch.float32)\n",
    "        self.pre_softmax_interactions = torch.empty((0, self.num_heads, self.seq_len, self.seq_len), dtype=torch.float32)\n",
    "\n",
    "    # hooks will grab from the outputs of Block\n",
    "    # meaning we don't need to modify forward methods for anything else\n",
    "    # 3 hours later -- lol this was wrong\n",
    "\n",
    "    def get_pre_softmax_attention(self, module, input, output):\n",
    "        print('Getting pre_softmax attention...')\n",
    "\n",
    "        # handle batching - we will divide 1st dimension by the number such that it will comport with num_heads\n",
    "\n",
    "        output_hooked = output['pre_softmax_attention'].unsqueeze(dim=0)\n",
    "        output_split = torch.split(output_hooked, self.num_heads, dim=1)\n",
    "\n",
    "        print('Split the output into heads.\\nNew Tensor Shapes:')\n",
    "        for tensor in output_split:\n",
    "            print(f'{tensor.shape}')\n",
    "\n",
    "        for tensor in output_split:\n",
    "            self.pre_softmax_attentions = torch.cat((self.pre_softmax_attentions, tensor), dim=0)\n",
    "\n",
    "        # test to see if it will cat, if function is called\n",
    "\n",
    "        #self.pre_softmax_attentions = torch.cat((self.pre_softmax_attentions, torch.randn((1,self.model.num_layers, self.num_heads, self.seq_len, self.seq_len), dtype=torch.float32)), dim=0)\n",
    "\n",
    "    def get_pre_softmax_interaction(self, module, input, output):\n",
    "        print('Getting pre-softmaxed interaction...')\n",
    "\n",
    "        output_hooked = output['pre_softmax_interaction'].unsqueeze(dim=0)\n",
    "        output_split = torch.split(output_hooked, self.num_heads, dim=1)\n",
    "\n",
    "        print(f'Got shape:{output_hooked.shape}')\n",
    "\n",
    "        print('Split the output into heads.\\nNew Tensor Shapes:')\n",
    "        for tensor in output_split:\n",
    "            print(f'{tensor.shape}')\n",
    "\n",
    "        for tensor in output_split:\n",
    "            self.pre_softmax_interactions = torch.cat((self.pre_softmax_interactions, tensor), dim=0)\n",
    "\n",
    "    #def __call__(self, input_tensor: Tensor):\n",
    "    #    output = self.model(input_tensor, **self.kwargs)\n",
    "    #    return output\n",
    "\n",
    "    def cut_padding(self, tensor, mask):\n",
    "        '''\n",
    "        Presents collected tensor from Pre_Softmax_Hook as list of particles with each item a 4d ndarray like (layers, heads, jet_length, jet_length).\n",
    "        Padding removed.\n",
    "\n",
    "        Args:\n",
    "        - tensor: The input tensor to process.\n",
    "        - config: Model configuration dict. If 'num_layers' is not present, defaults to 8 as in original ParT.\n",
    "\n",
    "        Outputs:\n",
    "        tensor_as_np: list of jagged arrays with shape (layers, heads, jet_length, jet_length)\n",
    "        '''\n",
    "        config = self.kwargs\n",
    "\n",
    "        if 'num_layers' in config:\n",
    "            num_layers = config['num_layers']\n",
    "        else:\n",
    "            num_layers = 8\n",
    "\n",
    "        num_particles = tensor.shape[0] // num_layers\n",
    "\n",
    "        tensor_as_np = tensor.numpy() # should be (total_num_layers, num_heads, jet_length, jet_length)\n",
    "        tensor_as_np = np.split(tensor_as_np, indices_or_sections=num_particles, axis=0)\n",
    "\n",
    "        for particle_idx, particle in enumerate(tensor_as_np):\n",
    "            padding_limit = np.sum(mask[particle_idx])\n",
    "            print(f'Padding Limit: {padding_limit}')\n",
    "            padding_limit = np.sum(mask[particle_idx]).astype(int)\n",
    "            tensor_as_np[particle_idx] = tensor_as_np[particle_idx][:,:,:padding_limit, :padding_limit]\n",
    "\n",
    "        return tensor_as_np\n",
    "\n",
    "    def clear(self):\n",
    "        self.pre_softmax_attentions = torch.empty((0, self.num_heads, self.seq_len, self.seq_len), dtype=torch.float32)\n",
    "        self.pre_softmax_interactions = torch.empty((0, self.num_heads, self.seq_len, self.seq_len), dtype=torch.float32)\n",
    "        self.handle_attn.remove()\n",
    "        self.handle_inter.remove()\n",
    "        #self.cls_handle_attn.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05022d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "{'embed_dim': 128, 'num_heads': 8, 'ffn_ratio': 4, 'dropout': 0.1, 'attn_dropout': 0.1, 'activation_dropout': 0.1, 'add_bias_kv': False, 'activation': 'gelu', 'scale_fc': True, 'scale_attn': True, 'scale_heads': True, 'scale_resids': True, 'return_pre_softmax': True}\n",
      "Layer Names in test_ParT:\n",
      "Block(\n",
      "  (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (act): GELU(approximate='none')\n",
      "  (act_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n",
      "Block(\n",
      "  (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (act): GELU(approximate='none')\n",
      "  (act_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n",
      "Block(\n",
      "  (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (act): GELU(approximate='none')\n",
      "  (act_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n",
      "Block(\n",
      "  (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (act): GELU(approximate='none')\n",
      "  (act_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n",
      "Block(\n",
      "  (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (act): GELU(approximate='none')\n",
      "  (act_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n",
      "Block(\n",
      "  (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (act): GELU(approximate='none')\n",
      "  (act_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n",
      "Block(\n",
      "  (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (act): GELU(approximate='none')\n",
      "  (act_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n",
      "Block(\n",
      "  (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (act): GELU(approximate='none')\n",
      "  (act_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n",
      "Block(\n",
      "  (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (act): GELU(approximate='none')\n",
      "  (act_dropout): Dropout(p=0, inplace=False)\n",
      "  (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n",
      "Block(\n",
      "  (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (act): GELU(approximate='none')\n",
      "  (act_dropout): Dropout(p=0, inplace=False)\n",
      "  (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# testing with TL config and data\n",
    "\n",
    "def get_model(model_type='qg',**kwargs):\n",
    "\n",
    "    if model_type == 'qg':\n",
    "        # QuarkGluon model configuration (13 kinpid features)\n",
    "        cfg = dict(\n",
    "            input_dim=13,  # pt_log, e_log, logptrel, logerel, deltaR, charge, isChargedHadron, isNeutralHadron, isPhoton, isElectron, isMuon, deta, dphi\n",
    "            num_classes=2,  # Quark vs Gluon\n",
    "            pair_input_dim=4,\n",
    "            use_pre_activation_pair=False,\n",
    "            embed_dims=[128, 512, 128],\n",
    "            pair_embed_dims=[64, 64, 64],\n",
    "            num_heads=8,\n",
    "            num_layers=8,\n",
    "            num_cls_layers=2,\n",
    "            block_params=None,\n",
    "            cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
    "            fc_params=[],\n",
    "            activation='gelu',\n",
    "            trim=True,\n",
    "            for_inference=False,\n",
    "        )\n",
    "    elif model_type == 'tl':\n",
    "        # TopLandscape model configuration (7 kinematic features)\n",
    "        cfg = dict(\n",
    "            input_dim=7,  # part_pt_log, part_e_log, part_logptrel, part_logerel, part_deltaR, part_deta, part_dphi\n",
    "            num_classes=2,  # Top vs QCD\n",
    "            pair_input_dim=4,\n",
    "            use_pre_activation_pair=False,\n",
    "            embed_dims=[128, 512, 128],\n",
    "            pair_embed_dims=[64, 64, 64],\n",
    "            num_heads=8,\n",
    "            num_layers=8,\n",
    "            num_cls_layers=2,\n",
    "            block_params=None,\n",
    "            cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
    "            fc_params=[],\n",
    "            activation='gelu',\n",
    "            trim=True,\n",
    "            for_inference=False,\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Defaulting to Jet_Class-trained model configuration\")\n",
    "        cfg = dict(\n",
    "            input_dim=17,\n",
    "            num_classes=10,\n",
    "            # network configurations\n",
    "            pair_input_dim=4,\n",
    "            use_pre_activation_pair=False,\n",
    "            embed_dims=[128, 512, 128],\n",
    "            pair_embed_dims=[64, 64, 64],\n",
    "            num_heads=8,\n",
    "            num_layers=8,\n",
    "            num_cls_layers=2,\n",
    "            block_params=None,\n",
    "            cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
    "            fc_params=[],\n",
    "            activation='gelu',\n",
    "            # misc\n",
    "            trim=True,\n",
    "            for_inference=False,\n",
    "        )\n",
    "    \n",
    "    cfg.update(**kwargs)\n",
    "    model = ParticleTransformerWrapper(**cfg)\n",
    "\n",
    "    model_info = {\n",
    "\n",
    "    }\n",
    "\n",
    "    return model, model_info\n",
    "\n",
    "test_ParT, model_info = get_model(model_type='qg', return_pre_softmax=True)\n",
    "\n",
    "print(test_ParT.mod.return_pre_softmax)\n",
    "print(test_ParT.mod.default_cfg)\n",
    "\n",
    "print(f'Layer Names in test_ParT:')\n",
    "for name in test_ParT.mod.blocks:\n",
    "    print(name)\n",
    "for name in test_ParT.mod.cls_blocks:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "949683ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that the model by itself can product outputs\n",
    "# bring in actual data to verify this\n",
    "# code from QG_and_TL_AttentionGraphs.ipynb\n",
    "\n",
    "def build_features_and_labels_qg(tree, transform_features=True):\n",
    "    \"\"\"Build features for QuarkGluon dataset based on qg_kinpid.yaml\"\"\"\n",
    "    # load arrays from the tree\n",
    "    a = tree.arrays(filter_name=['part_*', 'jet_pt', 'jet_energy', 'label'])\n",
    "\n",
    "    # compute new features\n",
    "    a['part_mask'] = ak.ones_like(a['part_energy'])\n",
    "    a['part_pt'] = np.hypot(a['part_px'], a['part_py'])\n",
    "    a['part_pt_log'] = np.log(a['part_pt'])\n",
    "    a['part_e_log'] = np.log(a['part_energy'])\n",
    "    a['part_logptrel'] = np.log(a['part_pt']/a['jet_pt'])\n",
    "    a['part_logerel'] = np.log(a['part_energy']/a['jet_energy'])\n",
    "    a['part_deltaR'] = np.hypot(a['part_deta'], a['part_dphi'])\n",
    "\n",
    "    # apply standardization based on qg_kinpid.yaml\n",
    "    if transform_features:\n",
    "        a['part_pt_log'] = (a['part_pt_log'] - 1.7) * 0.7\n",
    "        a['part_e_log'] = (a['part_e_log'] - 2.0) * 0.7\n",
    "        a['part_logptrel'] = (a['part_logptrel'] - (-4.7)) * 0.7\n",
    "        a['part_logerel'] = (a['part_logerel'] - (-4.7)) * 0.7\n",
    "        a['part_deltaR'] = (a['part_deltaR'] - 0.2) * 4.0\n",
    "\n",
    "    # Feature list for QuarkGluon (kinematic + particle ID)\n",
    "    feature_list = {\n",
    "        'pf_points': ['part_deta', 'part_dphi'],\n",
    "        'pf_features': [\n",
    "            'part_pt_log',\n",
    "            'part_e_log', \n",
    "            'part_logptrel',\n",
    "            'part_logerel',\n",
    "            'part_deltaR',\n",
    "            'part_charge',\n",
    "            'part_isChargedHadron',\n",
    "            'part_isNeutralHadron',\n",
    "            'part_isPhoton',\n",
    "            'part_isElectron',\n",
    "            'part_isMuon',\n",
    "            'part_deta',\n",
    "            'part_dphi',\n",
    "        ],\n",
    "        'pf_vectors': [\n",
    "            'part_px',\n",
    "            'part_py',\n",
    "            'part_pz',\n",
    "            'part_energy',\n",
    "        ],\n",
    "        'pf_mask': ['part_mask']\n",
    "    }\n",
    "\n",
    "    def _pad(a, maxlen=128, value=0, dtype='float32'):\n",
    "        if isinstance(a, np.ndarray) and a.ndim >= 2 and a.shape[1] == maxlen:\n",
    "            return a\n",
    "        elif isinstance(a, ak.Array):\n",
    "            if a.ndim == 1:\n",
    "                a = ak.unflatten(a, 1)\n",
    "            a = ak.fill_none(ak.pad_none(a, maxlen, clip=True), value)\n",
    "            return ak.values_astype(a, dtype)\n",
    "        else:\n",
    "            x = (np.ones((len(a), maxlen)) * value).astype(dtype)\n",
    "            for idx, s in enumerate(a):\n",
    "                if not len(s):\n",
    "                    continue\n",
    "                trunc = s[:maxlen].astype(dtype)\n",
    "                x[idx, :len(trunc)] = trunc\n",
    "            return x\n",
    "\n",
    "    out = {}\n",
    "    for k, names in feature_list.items():\n",
    "        out[k] = np.stack([_pad(a[n], maxlen=128).to_numpy() for n in names], axis=1)\n",
    "\n",
    "    # Labels for QuarkGluon (binary classification)\n",
    "    out['label'] = a['label'].to_numpy().astype('int')\n",
    "    \n",
    "    return out\n",
    "\n",
    "def build_features_and_labels_tl(tree, transform_features=True):\n",
    "    \"\"\"Build features for TopLandscape dataset based on top_kin.yaml\"\"\"\n",
    "    # load arrays from the tree\n",
    "    a = tree.arrays(filter_name=['part_*', 'jet_pt', 'jet_energy', 'label'])\n",
    "\n",
    "    # compute new features (same as QG)\n",
    "    a['part_mask'] = ak.ones_like(a['part_energy'])\n",
    "    a['part_pt'] = np.hypot(a['part_px'], a['part_py'])\n",
    "    a['part_pt_log'] = np.log(a['part_pt'])\n",
    "    a['part_e_log'] = np.log(a['part_energy'])\n",
    "    a['part_logptrel'] = np.log(a['part_pt']/a['jet_pt'])\n",
    "    a['part_logerel'] = np.log(a['part_energy']/a['jet_energy'])\n",
    "    a['part_deltaR'] = np.hypot(a['part_deta'], a['part_dphi'])\n",
    "\n",
    "    # apply standardization based on top_kin.yaml (same as QG)\n",
    "    if transform_features:\n",
    "        a['part_pt_log'] = (a['part_pt_log'] - 1.7) * 0.7\n",
    "        a['part_e_log'] = (a['part_e_log'] - 2.0) * 0.7\n",
    "        a['part_logptrel'] = (a['part_logptrel'] - (-4.7)) * 0.7\n",
    "        a['part_logerel'] = (a['part_logerel'] - (-4.7)) * 0.7\n",
    "        a['part_deltaR'] = (a['part_deltaR'] - 0.2) * 4.0\n",
    "\n",
    "    # Feature list for TopLandscape (same kinematic features as QG)\n",
    "    feature_list = {\n",
    "        'pf_points': ['part_deta', 'part_dphi'],\n",
    "        'pf_features': [\n",
    "            'part_pt_log',\n",
    "            'part_e_log',\n",
    "            'part_logptrel', \n",
    "            'part_logerel',\n",
    "            'part_deltaR',\n",
    "            'part_deta',\n",
    "            'part_dphi',\n",
    "        ],\n",
    "        'pf_vectors': [\n",
    "            'part_px',\n",
    "            'part_py',\n",
    "            'part_pz',\n",
    "            'part_energy',\n",
    "        ],\n",
    "        'pf_mask': ['part_mask']\n",
    "    }\n",
    "\n",
    "    def _pad(a, maxlen=128, value=0, dtype='float32'):\n",
    "        if isinstance(a, np.ndarray) and a.ndim >= 2 and a.shape[1] == maxlen:\n",
    "            return a\n",
    "        elif isinstance(a, ak.Array):\n",
    "            if a.ndim == 1:\n",
    "                a = ak.unflatten(a, 1)\n",
    "            a = ak.fill_none(ak.pad_none(a, maxlen, clip=True), value)\n",
    "            return ak.values_astype(a, dtype)\n",
    "        else:\n",
    "            x = (np.ones((len(a), maxlen)) * value).astype(dtype)\n",
    "            for idx, s in enumerate(a):\n",
    "                if not len(s):\n",
    "                    continue\n",
    "                trunc = s[:maxlen].astype(dtype)\n",
    "                x[idx, :len(trunc)] = trunc\n",
    "            return x\n",
    "\n",
    "    out = {}\n",
    "    for k, names in feature_list.items():\n",
    "        out[k] = np.stack([_pad(a[n], maxlen=128).to_numpy() for n in names], axis=1)\n",
    "\n",
    "    # Labels for TopLandscape (binary classification) \n",
    "    out['label'] = a['label'].to_numpy().astype('int')\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a172333c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading QuarkGluon data from ../qg_test_file_0.root\n",
      "This part is working - QG\n",
      "Truncating from 99999 jets to 6 jets\n",
      "Loading TopLandscape data from ../tl_test_file_0.root\n",
      "This part is working - TL\n"
     ]
    }
   ],
   "source": [
    "# loading TL data\n",
    "\n",
    "def load_data(dataset_type='qg', batch_size=300):\n",
    "    \"\"\"\n",
    "    Load data from \n",
    "    \n",
    "    Args:\n",
    "        dataset_type: 'qg', QuarkGluon, or 'tl', TopLandscape\n",
    "        batch_size: Number of jets to load\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        if dataset_type == 'qg':\n",
    "            # Try to load QuarkGluon data \n",
    "            data_path = \"../qg_test_file_0.root\"\n",
    "            if os.path.exists(data_path):\n",
    "                print(f\"Loading QuarkGluon data from {data_path}\")\n",
    "                with uproot.open(data_path)['tree'] as tree:\n",
    "                    print('This part is working - QG')\n",
    "                    data = build_features_and_labels_qg(tree)\n",
    "                    # Truncate to batch_size\n",
    "                    if data['pf_points'].shape[0] > batch_size:\n",
    "                        print(f\"Truncating from {data['pf_points'].shape[0]} jets to {batch_size} jets\")\n",
    "                        data = {\n",
    "                            'pf_points': data['pf_points'][:batch_size],\n",
    "                            'pf_features': data['pf_features'][:batch_size], \n",
    "                            'pf_vectors': data['pf_vectors'][:batch_size],\n",
    "                            'pf_mask': data['pf_mask'][:batch_size],\n",
    "                            'labels': data['label'][:batch_size]\n",
    "                        }\n",
    "                    return data\n",
    "\n",
    "        elif dataset_type == 'tl':\n",
    "            # Try to load TopLandscape data\n",
    "            data_path = \"../tl_test_file_0.root\"\n",
    "            if os.path.exists(data_path):\n",
    "                print(f\"Loading TopLandscape data from {data_path}\")\n",
    "                with uproot.open(data_path)['tree'] as tree:\n",
    "                    print('This part is working - TL')\n",
    "                    data = build_features_and_labels_tl(tree)\n",
    "                    # Truncate to batch_size\n",
    "                    #if data['pf_points'].shape[0] > batch_size:\n",
    "                    #    print(f\"Truncating from {data['pf_points'].shape[0]} jets to {batch_size} jets\")\n",
    "                    data = {\n",
    "                            'pf_points': data['pf_points'][:batch_size],\n",
    "                            'pf_features': data['pf_features'][:batch_size],\n",
    "                            'pf_vectors': data['pf_vectors'][:batch_size],\n",
    "                            'pf_mask': data['pf_mask'][:batch_size],\n",
    "                            'labels': data['label'][:batch_size]\n",
    "                        }\n",
    "                    return data\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Could not load actual data: {e}\")\n",
    "        raise e\n",
    "\n",
    "qg_data = load_data('qg', batch_size=6)\n",
    "tl_data = load_data('tl', batch_size=6)\n",
    "\n",
    "print(f\"TL sample data shapes:\")\n",
    "for k, v in tl_data.items():\n",
    "    print(f\"  {k}: {v.shape}\")\n",
    "\n",
    "print(f\"\\nFeature dimensions:\")\n",
    "print(f\"  QuarkGluon (kinpid): {qg_data['pf_features'].shape[1]} features\")\n",
    "print(f\"  TopLandscape (kin): {tl_data['pf_features'].shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd32187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting pre_softmax attention...\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre-softmaxed interaction...\n",
      "Got shape:torch.Size([1, 48, 128, 128])\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre_softmax attention...\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre-softmaxed interaction...\n",
      "Got shape:torch.Size([1, 48, 128, 128])\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre_softmax attention...\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre-softmaxed interaction...\n",
      "Got shape:torch.Size([1, 48, 128, 128])\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre_softmax attention...\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre-softmaxed interaction...\n",
      "Got shape:torch.Size([1, 48, 128, 128])\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre_softmax attention...\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre-softmaxed interaction...\n",
      "Got shape:torch.Size([1, 48, 128, 128])\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre_softmax attention...\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre-softmaxed interaction...\n",
      "Got shape:torch.Size([1, 48, 128, 128])\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre_softmax attention...\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre-softmaxed interaction...\n",
      "Got shape:torch.Size([1, 48, 128, 128])\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre_softmax attention...\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre-softmaxed interaction...\n",
      "Got shape:torch.Size([1, 48, 128, 128])\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim_legge/miniconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel_launcher.py:396: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n"
     ]
    }
   ],
   "source": [
    "# forward pass of model by itself to check that it can run\n",
    "\n",
    "qgtrained_modelpath = '/home/tim_legge/save_qg_model/on-qg-run2_best_epoch_state.pt'\n",
    "tltrained_modelpath = '/home/tim_legge/save_tl_model/on-tl-run3_best_epoch_state.pt'\n",
    "\n",
    "qg_model = get_model('qg')\n",
    "qg_state_dict = torch.load(qgtrained_modelpath, map_location=torch.device('cpu'))\n",
    "qg_model[0].load_state_dict(qg_state_dict)\n",
    "qg_pf_features = qg_data['pf_features'][:]\n",
    "qg_pf_vectors = qg_data['pf_vectors'][:]\n",
    "qg_pf_mask = qg_data['pf_mask'][:]\n",
    "qg_pf_points = qg_data['pf_points'][:]\n",
    "qg_labels = qg_data['labels'][:]\n",
    "qg_model[0].eval()\n",
    "with torch.no_grad():\n",
    "    qg_y_pred= qg_model[0](torch.from_numpy(qg_pf_points),torch.from_numpy(qg_pf_features),torch.from_numpy(qg_pf_vectors),torch.from_numpy(qg_pf_mask))\n",
    "\n",
    "tl_model = get_model('tl')\n",
    "tl_state_dict = torch.load(tltrained_modelpath, map_location=torch.device('cpu'))\n",
    "tl_model[0].load_state_dict(tl_state_dict)\n",
    "tl_pf_features = tl_data['pf_features'][:]\n",
    "tl_pf_vectors = tl_data['pf_vectors'][:]\n",
    "tl_pf_mask = tl_data['pf_mask'][:]\n",
    "tl_pf_points = tl_data['pf_points'][:]\n",
    "tl_labels = tl_data['labels'][:]\n",
    "tl_model[0].eval()\n",
    "with torch.no_grad():\n",
    "    tl_y_pred= tl_model[0](torch.from_numpy(tl_pf_points),torch.from_numpy(tl_pf_features),torch.from_numpy(tl_pf_vectors),torch.from_numpy(tl_pf_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c6ed8c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered hook onto module\n",
      "Registered hook onto module\n",
      "Registered hook onto module\n",
      "Registered hook onto module\n",
      "Registered hook onto module\n",
      "Registered hook onto module\n",
      "Registered hook onto module\n",
      "Registered hook onto module\n"
     ]
    }
   ],
   "source": [
    "# run clear in order to sample results again\n",
    "\n",
    "test_hooks = Pre_Softmax_Hook(model=test_ParT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc5b0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting pre_softmax attention...\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre-softmaxed interaction...\n",
      "Got shape:torch.Size([1, 48, 128, 128])\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre_softmax attention...\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre-softmaxed interaction...\n",
      "Got shape:torch.Size([1, 48, 128, 128])\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre_softmax attention...\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre-softmaxed interaction...\n",
      "Got shape:torch.Size([1, 48, 128, 128])\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre_softmax attention...\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre-softmaxed interaction...\n",
      "Got shape:torch.Size([1, 48, 128, 128])\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre_softmax attention...\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre-softmaxed interaction...\n",
      "Got shape:torch.Size([1, 48, 128, 128])\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre_softmax attention...\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre-softmaxed interaction...\n",
      "Got shape:torch.Size([1, 48, 128, 128])\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre_softmax attention...\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre-softmaxed interaction...\n",
      "Got shape:torch.Size([1, 48, 128, 128])\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre_softmax attention...\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre-softmaxed interaction...\n",
      "Got shape:torch.Size([1, 48, 128, 128])\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# forward pass with hooks enabled to collect pre-softmax attention and interaction matrices\n",
    "    \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    tl_y_pred = model(torch.from_numpy(tl_pf_points), torch.from_numpy(tl_pf_features), torch.from_numpy(tl_pf_vectors), torch.from_numpy(tl_pf_mask))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dc996953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48, 8, 128, 128])\n",
      "torch.Size([48, 8, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# check properties of test_hooks attention and interaction matrices\n",
    "\n",
    "print(test_hooks.pre_softmax_attentions.shape)  # should be (num_layers, num_heads, seq_len, seq_len)\n",
    "print(test_hooks.pre_softmax_interactions.shape)  # should be (num_layers, num_heads, seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "df6c4b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0685,  0.6336,  0.3280,  0.1784,  0.9440],\n",
      "        [ 0.2345,  0.3212,  0.1522,  0.0210,  0.6071],\n",
      "        [ 0.2825,  0.3943,  0.2112,  0.0688,  0.7048],\n",
      "        [ 0.3323,  0.4133,  0.2351,  0.0879,  0.7377],\n",
      "        [ 0.1646,  0.2462,  0.0944, -0.0239,  0.5097]])\n",
      "tensor([[ 0.0340,  5.5754,  4.9319,  4.9567,  4.8064],\n",
      "        [ 5.5754,  0.0340, -0.1685, -0.0900, -0.1220],\n",
      "        [ 4.9319, -0.1685,  0.0340, -0.1303, -0.1150],\n",
      "        [ 4.9567, -0.0900, -0.1303, -0.0758,  0.1684],\n",
      "        [ 4.8064, -0.1220, -0.1150,  0.1684,  0.0340]])\n"
     ]
    }
   ],
   "source": [
    "# print some values from both for another check\n",
    "\n",
    "print(test_hooks.pre_softmax_attentions[0, 4, :5, :5])  # first layer, first head, first 5x5 block\n",
    "print(test_hooks.pre_softmax_interactions[0, 4, :5, :5])  # first layer, fifth head, first 5x5 block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b6cf63",
   "metadata": {},
   "source": [
    "Implementation used here could be extended to pretty much any other variable that passes through Block.\n",
    "I'm reminded of vector norm analysis here (https://arxiv.org/pdf/2004.10102) from Kobayashi et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33ca4a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 1, 128)\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(tl_pf_mask.shape)\n",
    "print(tl_pf_mask[0,:,:45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfbe37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding Limit: 17.0\n",
      "Padding Limit: 61.0\n",
      "Padding Limit: 45.0\n",
      "Padding Limit: 74.0\n",
      "Padding Limit: 30.0\n",
      "Padding Limit: 37.0\n",
      "Padding Limit: 17.0\n",
      "Padding Limit: 61.0\n",
      "Padding Limit: 45.0\n",
      "Padding Limit: 74.0\n",
      "Padding Limit: 30.0\n",
      "Padding Limit: 37.0\n"
     ]
    }
   ],
   "source": [
    "# Now for some processing: cut out the padding and convert torch tensors to list of jagged ndarrays\n",
    "\n",
    "pre_softmax_attention_as_np = test_hooks.cut_padding(test_hooks.pre_softmax_attentions, tl_pf_mask)\n",
    "pre_softmax_interaction_as_np = test_hooks.cut_padding(test_hooks.pre_softmax_interactions, tl_pf_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "23336a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "jet_num = 4\n",
    "head_num = 1\n",
    "layer_num = 7\n",
    "\n",
    "jet_length = len(pre_softmax_attention_as_np[jet_num][layer_num, head_num, 0, :])\n",
    "\n",
    "plt.imshow(pre_softmax_attention_as_np[jet_num][layer_num, head_num, :, :], origin='lower', cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title('Pre-Softmax Attention Map')\n",
    "\n",
    "plt.xticks(np.arange(0, jet_length, step=5))  # Set x-axis ticks as integers with step size of 5\n",
    "plt.yticks(np.arange(0, jet_length, step=5))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(pre_softmax_interaction_as_np[jet_num][layer_num, head_num, :, :], origin='lower', cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title('Pre-Softmax Interaction Map')\n",
    "\n",
    "plt.xticks(np.arange(0, jet_length, step=5))  # Set x-axis ticks as integers with step size of 5\n",
    "plt.yticks(np.arange(0, jet_length, step=5))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
