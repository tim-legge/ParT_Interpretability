{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1f24c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pre-Softmax-Supplier ParT for analyzing attention and interaction\n",
    "\n",
    "Defines Batch, MultiHeadAttention, and multi_head_attention_forward such that they will return\n",
    "pre-softmaxed attention and interaction matrices for analysis purposes alongside typical outputs.\n",
    "\n",
    "This is achieved through forward hooks defined in Pre_Softmax_Hook such that the implementation of\n",
    "ParT is not significantly altered.*\n",
    "\n",
    "Passing the kwarg 'return_pre_softmax = True' (disabled by default) into a model configuration under\n",
    "the ParticleTransformerWrapper class (just ParticleTransformer probably also works, but the latter \n",
    "has not been tested empirically) will enable this feature and allow usage of Pre_Softmax_Hook. While \n",
    "hooks are registered, Pre_Softmax_Hook will stack pre-softmaxed attention and interaction matrices \n",
    "for each input jet. This is a passive process and the only further interaction suggested is to call\n",
    "pre_softmax_attentions or _interactions in order to view and analyze once you have run your model \n",
    "to satisfaction.\n",
    "\n",
    "Initialize and register hooks via:\n",
    "\n",
    "<Captain_Hook> = Pre_Softmax_Hook(model=<your_ParT_here>) # layer_name is also mentioned as a param\n",
    "                                                            below but quickly became irrelevant\n",
    "                                \n",
    "                ----    Run your model  ----\n",
    "\n",
    "Check results via:\n",
    "\n",
    "<Captain_Hook>.pre_softmax_attentions\n",
    "and\n",
    "<Captain_Hook>.pre_softmax_interactions\n",
    "\n",
    "(both are of shape (total_num_of_layers, num_heads, seq_length, seq_length)\n",
    "where total_num_of_layers is the number of particle attention blocks that were passed through in\n",
    "total by all jets in the sample)\n",
    "\n",
    "                ----  Run your analysis  ----\n",
    "\n",
    "Clear your hooks and give the captain some rest via:**\n",
    "\n",
    "<Captain_Hook>.clear_hooks()\n",
    "\n",
    "--------------------------------------------------------------------------------------------------\n",
    "\n",
    "OTHER NOTES AND FOOTNOTES:\n",
    "\n",
    "Keeping return_pre_softmax = False will leave the implementation functionally identical to original\n",
    "Pytorch and ParT code.***\n",
    "\n",
    "If you want to know whether any of this is optimized without needing to read the code, it's not.\n",
    "Ditto for rigorous testing.\n",
    "\n",
    "*Depends on your definition of 'significantly'. ParT computes in the exact way as before\n",
    "and inner workings are 99% the same, but future edits made to the model will have to deal with\n",
    "different output structures for multi_head_attention_forward, MultiheadAttention,\n",
    "Block, and ParticleTransformer.\n",
    "\n",
    "**You don't have to anthropomorphize your class instances in order for this to work. I just thought\n",
    "it was funny.\n",
    "\n",
    "***Currently untested. Also not really sure why you would do this if the hook doesn't appear to \n",
    "disrupt any other process, but I won't judge.\n",
    "'''\n",
    "\n",
    "\n",
    "from typing import List, Optional\n",
    "import timeit\n",
    "import awkward as ak\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter \n",
    "from torch.nn.init import xavier_uniform_, xavier_normal_, constant_\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from typing import Optional\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple\n",
    "_is_fastpath_enabled: bool = True\n",
    "from torch.overrides import (\n",
    "    handle_torch_function,\n",
    "    has_torch_function,\n",
    "    has_torch_function_unary,\n",
    "    has_torch_function_variadic,\n",
    ")\n",
    "linear = torch._C._nn.linear\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "import copy\n",
    "from torch._C import _add_docstr, _infer_size\n",
    "\n",
    "from functools import partial\n",
    "from weaver.utils.logger import _logger\n",
    "import os\n",
    "import uproot\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch._torch_docs import reproducibility_notes, sparse_support_notes, tf32_notes\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    __constants__ = ['batch_first']\n",
    "    bias_k: Optional[torch.Tensor]\n",
    "    bias_v: Optional[torch.Tensor]\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False,\n",
    "                 kdim=None, vdim=None, batch_first=False, device=None, dtype=None, return_pre_softmax=False) -> None:\n",
    "        if embed_dim <= 0 or num_heads <= 0:\n",
    "            raise ValueError(\n",
    "                f\"embed_dim and num_heads must be greater than 0,\"\n",
    "                f\" got embed_dim={embed_dim} and num_heads={num_heads} instead\"\n",
    "            )\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.kdim = kdim if kdim is not None else embed_dim\n",
    "        self.vdim = vdim if vdim is not None else embed_dim\n",
    "        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.batch_first = batch_first\n",
    "        self.return_pre_softmax = return_pre_softmax\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        if not self._qkv_same_embed_dim:\n",
    "            self.q_proj_weight = Parameter(torch.empty((embed_dim, embed_dim), **factory_kwargs))\n",
    "            self.k_proj_weight = Parameter(torch.empty((embed_dim, self.kdim), **factory_kwargs))\n",
    "            self.v_proj_weight = Parameter(torch.empty((embed_dim, self.vdim), **factory_kwargs))\n",
    "            self.register_parameter('in_proj_weight', None)\n",
    "        else:\n",
    "            self.in_proj_weight = Parameter(torch.empty((3 * embed_dim, embed_dim), **factory_kwargs))\n",
    "            self.register_parameter('q_proj_weight', None)\n",
    "            self.register_parameter('k_proj_weight', None)\n",
    "            self.register_parameter('v_proj_weight', None)\n",
    "\n",
    "        if bias:\n",
    "            self.in_proj_bias = Parameter(torch.empty(3 * embed_dim, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('in_proj_bias', None)\n",
    "        self.out_proj = torch.nn.Linear(embed_dim, embed_dim, bias=bias, **factory_kwargs)\n",
    "\n",
    "        if add_bias_kv:\n",
    "            self.bias_k = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n",
    "            self.bias_v = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n",
    "        else:\n",
    "            self.bias_k = self.bias_v = None\n",
    "\n",
    "        self.add_zero_attn = add_zero_attn\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        if self._qkv_same_embed_dim:\n",
    "            xavier_uniform_(self.in_proj_weight)\n",
    "        else:\n",
    "            xavier_uniform_(self.q_proj_weight)\n",
    "            xavier_uniform_(self.k_proj_weight)\n",
    "            xavier_uniform_(self.v_proj_weight)\n",
    "\n",
    "        if self.in_proj_bias is not None:\n",
    "            constant_(self.in_proj_bias, 0.)\n",
    "            constant_(self.out_proj.bias, 0.)\n",
    "        if self.bias_k is not None:\n",
    "            xavier_normal_(self.bias_k)\n",
    "        if self.bias_v is not None:\n",
    "            xavier_normal_(self.bias_v)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        # Support loading old MultiheadAttention checkpoints generated by v1.1.0\n",
    "        if '_qkv_same_embed_dim' not in state:\n",
    "            state['_qkv_same_embed_dim'] = True\n",
    "\n",
    "        super().__setstate__(state)\n",
    "\n",
    "    def forward(\n",
    "                self,\n",
    "                query: Tensor,\n",
    "                key: Tensor,\n",
    "                value: Tensor,\n",
    "                key_padding_mask: Optional[Tensor] = None,\n",
    "                need_weights: bool = True,\n",
    "                attn_mask: Optional[Tensor] = None,\n",
    "                average_attn_weights: bool = True,\n",
    "                is_causal : bool = False,\n",
    "                **kwargs) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "\n",
    "            why_not_fast_path = ''\n",
    "            if ((attn_mask is not None and torch.is_floating_point(attn_mask))\n",
    "            or (key_padding_mask is not None) and torch.is_floating_point(key_padding_mask)):\n",
    "                why_not_fast_path = \"floating-point masks are not supported for fast path.\"\n",
    "\n",
    "            is_batched = query.dim() == 3\n",
    "\n",
    "            key_padding_mask = _canonical_mask(\n",
    "                mask=key_padding_mask,\n",
    "                mask_name=\"key_padding_mask\",\n",
    "                other_type=_none_or_dtype(attn_mask),\n",
    "                other_name=\"attn_mask\",\n",
    "                target_type=query.dtype\n",
    "            )\n",
    "\n",
    "            attn_mask = _canonical_mask(\n",
    "                mask=attn_mask,\n",
    "                mask_name=\"attn_mask\",\n",
    "                other_type=None,\n",
    "                other_name=\"\",\n",
    "                target_type=query.dtype,\n",
    "                check_other=False,\n",
    "            )\n",
    "\n",
    "            is_fastpath_enabled = get_fastpath_enabled()\n",
    "\n",
    "            if not is_fastpath_enabled:\n",
    "                why_not_fast_path = \"torch.backends.mha.get_fastpath_enabled() was not True\"\n",
    "            elif not is_batched:\n",
    "                why_not_fast_path = f\"input not batched; expected query.dim() of 3 but got {query.dim()}\"\n",
    "            elif query is not key or key is not value:\n",
    "                # When lifting this restriction, don't forget to either\n",
    "                # enforce that the dtypes all match or test cases where\n",
    "                # they don't!\n",
    "                why_not_fast_path = \"non-self attention was used (query, key, and value are not the same Tensor)\"\n",
    "            elif self.in_proj_bias is not None and query.dtype != self.in_proj_bias.dtype:\n",
    "                why_not_fast_path = f\"dtypes of query ({query.dtype}) and self.in_proj_bias ({self.in_proj_bias.dtype}) don't match\"\n",
    "            elif self.in_proj_weight is None:\n",
    "                why_not_fast_path = \"in_proj_weight was None\"\n",
    "            elif query.dtype != self.in_proj_weight.dtype:\n",
    "                # this case will fail anyway, but at least they'll get a useful error message.\n",
    "                why_not_fast_path = f\"dtypes of query ({query.dtype}) and self.in_proj_weight ({self.in_proj_weight.dtype}) don't match\"\n",
    "            elif self.training:\n",
    "                why_not_fast_path = \"training is enabled\"\n",
    "            elif (self.num_heads % 2) != 0:\n",
    "                why_not_fast_path = \"self.num_heads is not even\"\n",
    "            elif not self.batch_first:\n",
    "                why_not_fast_path = \"batch_first was not True\"\n",
    "            elif self.bias_k is not None:\n",
    "                why_not_fast_path = \"self.bias_k was not None\"\n",
    "            elif self.bias_v is not None:\n",
    "                why_not_fast_path = \"self.bias_v was not None\"\n",
    "            elif self.add_zero_attn:\n",
    "                why_not_fast_path = \"add_zero_attn was enabled\"\n",
    "            elif not self._qkv_same_embed_dim:\n",
    "                why_not_fast_path = \"_qkv_same_embed_dim was not True\"\n",
    "            elif query.is_nested and (key_padding_mask is not None or attn_mask is not None):\n",
    "                why_not_fast_path = \"supplying both src_key_padding_mask and src_mask at the same time \\\n",
    "                                    is not supported with NestedTensor input\"\n",
    "            elif torch.is_autocast_enabled():\n",
    "                why_not_fast_path = \"autocast is enabled\"\n",
    "\n",
    "            if not why_not_fast_path:\n",
    "                tensor_args = (\n",
    "                    query,\n",
    "                    key,\n",
    "                    value,\n",
    "                    self.in_proj_weight,\n",
    "                    self.in_proj_bias,\n",
    "                    self.out_proj.weight,\n",
    "                    self.out_proj.bias,\n",
    "                )\n",
    "                # We have to use list comprehensions below because TorchScript does not support\n",
    "                # generator expressions.\n",
    "                if torch.overrides.has_torch_function(tensor_args):\n",
    "                    why_not_fast_path = \"some Tensor argument has_torch_function\"\n",
    "                elif _is_make_fx_tracing():\n",
    "                    why_not_fast_path = \"we are running make_fx tracing\"\n",
    "                elif not all(_check_arg_device(x) for x in tensor_args):\n",
    "                    why_not_fast_path = (\"some Tensor argument's device is neither one of \"\n",
    "                                        f\"cpu, cuda or {torch.utils.backend_registration._privateuse1_backend_name}\")\n",
    "                elif torch.is_grad_enabled() and any(_arg_requires_grad(x) for x in tensor_args):\n",
    "                    why_not_fast_path = (\"grad is enabled and at least one of query or the \"\n",
    "                                        \"input/output projection weights or biases requires_grad\")\n",
    "                if not why_not_fast_path:\n",
    "                    merged_mask, mask_type = self.merge_masks(attn_mask, key_padding_mask, query)\n",
    "\n",
    "                    if self.in_proj_bias is not None and self.in_proj_weight is not None:\n",
    "                        return torch._native_multi_head_attention(\n",
    "                            query,\n",
    "                            key,\n",
    "                            value,\n",
    "                            self.embed_dim,\n",
    "                            self.num_heads,\n",
    "                            self.in_proj_weight,\n",
    "                            self.in_proj_bias,\n",
    "                            self.out_proj.weight,\n",
    "                            self.out_proj.bias,\n",
    "                            merged_mask,\n",
    "                            need_weights,\n",
    "                            average_attn_weights,\n",
    "                            mask_type)\n",
    "\n",
    "            any_nested = query.is_nested or key.is_nested or value.is_nested\n",
    "            assert not any_nested, (\"MultiheadAttention does not support NestedTensor outside of its fast path. \" +\n",
    "                                    f\"The fast path was not hit because {why_not_fast_path}\")\n",
    "\n",
    "            if self.batch_first and is_batched:\n",
    "                # make sure that the transpose op does not affect the \"is\" property\n",
    "                if key is value:\n",
    "                    if query is key:\n",
    "                        query = key = value = query.transpose(1, 0)\n",
    "                    else:\n",
    "                        query, key = (x.transpose(1, 0) for x in (query, key))\n",
    "                        value = key\n",
    "                else:\n",
    "                    query, key, value = (x.transpose(1, 0) for x in (query, key, value))\n",
    "\n",
    "            if not self._qkv_same_embed_dim:\n",
    "                if self.return_pre_softmax:\n",
    "                    attn_output, attn_output_weights, pre_softmax_attention, pre_softmax_interaction = multi_head_attention_forward(\n",
    "                        query, key, value, self.embed_dim, self.num_heads,\n",
    "                        self.in_proj_weight, self.in_proj_bias,\n",
    "                        self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "                        self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "                        training=self.training,\n",
    "                        key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
    "                        attn_mask=attn_mask,\n",
    "                        use_separate_proj_weight=True,\n",
    "                        q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\n",
    "                        v_proj_weight=self.v_proj_weight,\n",
    "                        average_attn_weights=average_attn_weights,\n",
    "                        return_pre_softmax=self.return_pre_softmax\n",
    "                )\n",
    "                else:\n",
    "                    attn_output, attn_output_weights = multi_head_attention_forward(\n",
    "                        query, key, value, self.embed_dim, self.num_heads,\n",
    "                        self.in_proj_weight, self.in_proj_bias,\n",
    "                        self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "                        self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "                        training=self.training,\n",
    "                        key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
    "                        attn_mask=attn_mask,\n",
    "                        use_separate_proj_weight=True,\n",
    "                        q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\n",
    "                        v_proj_weight=self.v_proj_weight,\n",
    "                        average_attn_weights=average_attn_weights,\n",
    "                        return_pre_softmax=self.return_pre_softmax\n",
    "                )\n",
    "            else:\n",
    "                if self.return_pre_softmax:\n",
    "                    attn_output, attn_output_weights, pre_softmax_attention, pre_softmax_interaction = multi_head_attention_forward(\n",
    "                        query, key, value, self.embed_dim, self.num_heads,\n",
    "                        self.in_proj_weight, self.in_proj_bias,\n",
    "                        self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "                        self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "                        training=self.training,\n",
    "                        key_padding_mask=key_padding_mask,\n",
    "                        need_weights=need_weights,\n",
    "                        attn_mask=attn_mask,\n",
    "                        average_attn_weights=average_attn_weights,\n",
    "                        return_pre_softmax=self.return_pre_softmax)\n",
    "                else:\n",
    "                    attn_output, attn_output_weights = multi_head_attention_forward(\n",
    "                        query, key, value, self.embed_dim, self.num_heads,\n",
    "                        self.in_proj_weight, self.in_proj_bias,\n",
    "                        self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "                        self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "                        training=self.training,\n",
    "                        key_padding_mask=key_padding_mask,\n",
    "                        need_weights=need_weights,\n",
    "                        attn_mask=attn_mask,\n",
    "                        average_attn_weights=average_attn_weights,\n",
    "                        return_pre_softmax=self.return_pre_softmax)\n",
    "\n",
    "            if self.batch_first and is_batched:\n",
    "                if self.return_pre_softmax:\n",
    "                    pre_softmax_attention.detach()\n",
    "                    pre_softmax_interaction.detach()\n",
    "                    return {'output': attn_output.transpose(1, 0), 'attn_output_weights': attn_output_weights, 'pre_softmax_attention': pre_softmax_attention, 'pre_softmax_interaction': pre_softmax_interaction}\n",
    "                else:\n",
    "                    return attn_output.transpose(1, 0), attn_output_weights\n",
    "            else:\n",
    "                if self.return_pre_softmax:\n",
    "                    pre_softmax_attention.detach()\n",
    "                    pre_softmax_interaction.detach()\n",
    "                    return {'output': attn_output, 'attn_output_weights': attn_output_weights, 'pre_softmax_attention': pre_softmax_attention, 'pre_softmax_interaction': pre_softmax_interaction}\n",
    "                else:\n",
    "                    return attn_output, attn_output_weights\n",
    "\n",
    "def _canonical_mask(\n",
    "    mask: Optional[Tensor],\n",
    "    mask_name: str,\n",
    "    other_type: Optional[torch.dtype],\n",
    "    other_name: str,\n",
    "    target_type: torch.dtype,\n",
    "    check_other: bool = True,\n",
    ") -> Optional[Tensor]:\n",
    "    if mask is not None:\n",
    "        _mask_dtype = mask.dtype\n",
    "        _mask_is_float = torch.is_floating_point(mask)\n",
    "        if _mask_dtype != torch.bool and not _mask_is_float:\n",
    "            raise AssertionError(\n",
    "                f\"only bool and floating types of {mask_name} are supported\"\n",
    "            )\n",
    "        if check_other and other_type is not None:\n",
    "            if _mask_dtype != other_type:\n",
    "                warnings.warn(\n",
    "                    f\"Support for mismatched {mask_name} and {other_name} \"\n",
    "                    \"is deprecated. Use same type for both instead.\"\n",
    "                )\n",
    "        if not _mask_is_float:\n",
    "            mask = torch.zeros_like(mask, dtype=target_type).masked_fill_(\n",
    "                mask, float(\"-inf\")\n",
    "            )\n",
    "    return mask\n",
    "def _none_or_dtype(input: Optional[Tensor]) -> Optional[torch.dtype]:\n",
    "    if input is None:\n",
    "        return None\n",
    "    elif isinstance(input, torch.Tensor):\n",
    "        return input.dtype\n",
    "    raise RuntimeError(\"input to _none_or_dtype() must be None or torch.Tensor\")\n",
    "def get_fastpath_enabled() -> bool:\n",
    "    \"\"\"Returns whether fast path for TransformerEncoder and MultiHeadAttention\n",
    "    is enabled, or ``True`` if jit is scripting.\n",
    "\n",
    "    ..note:\n",
    "        The fastpath might not be run even if ``get_fastpath_enabled`` returns\n",
    "        ``True`` unless all conditions on inputs are met.\n",
    "    \"\"\"\n",
    "    if not torch.jit.is_scripting():\n",
    "        return _is_fastpath_enabled\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "def set_fastpath_enabled(value: bool) -> None:\n",
    "    \"\"\"Sets whether fast path is enabled\"\"\"\n",
    "    global _is_fastpath_enabled\n",
    "    _is_fastpath_enabled = value\n",
    "\n",
    "def multi_head_attention_forward(\n",
    "    query: Tensor,\n",
    "    key: Tensor,\n",
    "    value: Tensor,\n",
    "    embed_dim_to_check: int,\n",
    "    num_heads: int,\n",
    "    in_proj_weight: Optional[Tensor],\n",
    "    in_proj_bias: Optional[Tensor],\n",
    "    bias_k: Optional[Tensor],\n",
    "    bias_v: Optional[Tensor],\n",
    "    add_zero_attn: bool,\n",
    "    dropout_p: float,\n",
    "    out_proj_weight: Tensor,\n",
    "    out_proj_bias: Optional[Tensor],\n",
    "    training: bool = True,\n",
    "    key_padding_mask: Optional[Tensor] = None,\n",
    "    need_weights: bool = True,\n",
    "    attn_mask: Optional[Tensor] = None,\n",
    "    use_separate_proj_weight: bool = False,\n",
    "    q_proj_weight: Optional[Tensor] = None,\n",
    "    k_proj_weight: Optional[Tensor] = None,\n",
    "    v_proj_weight: Optional[Tensor] = None,\n",
    "    static_k: Optional[Tensor] = None,\n",
    "    static_v: Optional[Tensor] = None,\n",
    "    average_attn_weights: bool = True,\n",
    "    is_causal: bool = False,\n",
    "    return_pre_softmax: bool = False,\n",
    ") -> Tuple[Tensor, Optional[Tensor]]:\n",
    "    tens_ops = (\n",
    "        query,\n",
    "        key,\n",
    "        value,\n",
    "        in_proj_weight,\n",
    "        in_proj_bias,\n",
    "        bias_k,\n",
    "        bias_v,\n",
    "        out_proj_weight,\n",
    "        out_proj_bias,\n",
    "    )\n",
    "    if has_torch_function(tens_ops):\n",
    "        return handle_torch_function(\n",
    "            multi_head_attention_forward,\n",
    "            tens_ops,\n",
    "            query,\n",
    "            key,\n",
    "            value,\n",
    "            embed_dim_to_check,\n",
    "            num_heads,\n",
    "            in_proj_weight,\n",
    "            in_proj_bias,\n",
    "            bias_k,\n",
    "            bias_v,\n",
    "            add_zero_attn,\n",
    "            dropout_p,\n",
    "            out_proj_weight,\n",
    "            out_proj_bias,\n",
    "            training=training,\n",
    "            key_padding_mask=key_padding_mask,\n",
    "            need_weights=need_weights,\n",
    "            attn_mask=attn_mask,\n",
    "            is_causal=is_causal,\n",
    "            use_separate_proj_weight=use_separate_proj_weight,\n",
    "            q_proj_weight=q_proj_weight,\n",
    "            k_proj_weight=k_proj_weight,\n",
    "            v_proj_weight=v_proj_weight,\n",
    "            static_k=static_k,\n",
    "            static_v=static_v,\n",
    "            average_attn_weights=average_attn_weights,\n",
    "            return_pre_softmax=return_pre_softmax,\n",
    "        )\n",
    "\n",
    "    is_batched = _mha_shape_check(\n",
    "        query, key, value, key_padding_mask, attn_mask, num_heads\n",
    "    )\n",
    "\n",
    "    # For unbatched input, we unsqueeze at the expected batch-dim to pretend that the input\n",
    "    # is batched, run the computation and before returning squeeze the\n",
    "    # batch dimension so that the output doesn't carry this temporary batch dimension.\n",
    "    if not is_batched:\n",
    "        # unsqueeze if the input is unbatched\n",
    "        query = query.unsqueeze(1)\n",
    "        key = key.unsqueeze(1)\n",
    "        value = value.unsqueeze(1)\n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = key_padding_mask.unsqueeze(0)\n",
    "\n",
    "    # set up shape vars\n",
    "    tgt_len, bsz, embed_dim = query.shape\n",
    "    src_len, _, _ = key.shape\n",
    "\n",
    "    key_padding_mask = _canonical_mask(\n",
    "        mask=key_padding_mask,\n",
    "        mask_name=\"key_padding_mask\",\n",
    "        other_type=_none_or_dtype(attn_mask),\n",
    "        other_name=\"attn_mask\",\n",
    "        target_type=query.dtype,\n",
    "    )\n",
    "\n",
    "    if is_causal and attn_mask is None:\n",
    "        raise RuntimeError(\n",
    "            \"Need attn_mask if specifying the is_causal hint. \"\n",
    "            \"You may use the Transformer module method \"\n",
    "            \"`generate_square_subsequent_mask` to create this mask.\"\n",
    "        )\n",
    "\n",
    "    if is_causal and key_padding_mask is None and not need_weights:\n",
    "        # when we have a kpm or need weights, we need attn_mask\n",
    "        # Otherwise, we use the is_causal hint go as is_causal\n",
    "        # indicator to SDPA.\n",
    "        attn_mask = None\n",
    "    else:\n",
    "        attn_mask = _canonical_mask(\n",
    "            mask=attn_mask,\n",
    "            mask_name=\"attn_mask\",\n",
    "            other_type=None,\n",
    "            other_name=\"\",\n",
    "            target_type=query.dtype,\n",
    "            check_other=False,\n",
    "        )\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            # We have the attn_mask, and use that to merge kpm into it.\n",
    "            # Turn off use of is_causal hint, as the merged mask is no\n",
    "            # longer causal.\n",
    "            is_causal = False\n",
    "\n",
    "    assert (\n",
    "        embed_dim == embed_dim_to_check\n",
    "    ), f\"was expecting embedding dimension of {embed_dim_to_check}, but got {embed_dim}\"\n",
    "    if isinstance(embed_dim, torch.Tensor):\n",
    "        # embed_dim can be a tensor when JIT tracing\n",
    "        head_dim = embed_dim.div(num_heads, rounding_mode=\"trunc\")\n",
    "    else:\n",
    "        head_dim = embed_dim // num_heads\n",
    "    assert (\n",
    "        head_dim * num_heads == embed_dim\n",
    "    ), f\"embed_dim {embed_dim} not divisible by num_heads {num_heads}\"\n",
    "    if use_separate_proj_weight:\n",
    "        # allow MHA to have different embedding dimensions when separate projection weights are used\n",
    "        assert (\n",
    "            key.shape[:2] == value.shape[:2]\n",
    "        ), f\"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}\"\n",
    "    else:\n",
    "        assert (\n",
    "            key.shape == value.shape\n",
    "        ), f\"key shape {key.shape} does not match value shape {value.shape}\"\n",
    "\n",
    "    #\n",
    "    # compute in-projection\n",
    "    #\n",
    "    if not use_separate_proj_weight:\n",
    "        assert (\n",
    "            in_proj_weight is not None\n",
    "        ), \"use_separate_proj_weight is False but in_proj_weight is None\"\n",
    "        q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n",
    "    else:\n",
    "        assert (\n",
    "            q_proj_weight is not None\n",
    "        ), \"use_separate_proj_weight is True but q_proj_weight is None\"\n",
    "        assert (\n",
    "            k_proj_weight is not None\n",
    "        ), \"use_separate_proj_weight is True but k_proj_weight is None\"\n",
    "        assert (\n",
    "            v_proj_weight is not None\n",
    "        ), \"use_separate_proj_weight is True but v_proj_weight is None\"\n",
    "        if in_proj_bias is None:\n",
    "            b_q = b_k = b_v = None\n",
    "        else:\n",
    "            b_q, b_k, b_v = in_proj_bias.chunk(3)\n",
    "        q, k, v = _in_projection(\n",
    "            query,\n",
    "            key,\n",
    "            value,\n",
    "            q_proj_weight,\n",
    "            k_proj_weight,\n",
    "            v_proj_weight,\n",
    "            b_q,\n",
    "            b_k,\n",
    "            b_v,\n",
    "        )\n",
    "\n",
    "    # prep attention mask\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        # ensure attn_mask's dim is 3\n",
    "        if attn_mask.dim() == 2:\n",
    "            correct_2d_size = (tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_2d_size:\n",
    "                raise RuntimeError(\n",
    "                    f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\"\n",
    "                )\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        elif attn_mask.dim() == 3:\n",
    "            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_3d_size:\n",
    "                raise RuntimeError(\n",
    "                    f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\"\n",
    "                )\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                f\"attn_mask's dimension {attn_mask.dim()} is not supported\"\n",
    "            )\n",
    "\n",
    "    # add bias along batch dimension (currently second)\n",
    "    if bias_k is not None and bias_v is not None:\n",
    "        assert static_k is None, \"bias cannot be added to static key.\"\n",
    "        assert static_v is None, \"bias cannot be added to static value.\"\n",
    "        k = torch.cat([k, bias_k.repeat(1, bsz, 1)])\n",
    "        v = torch.cat([v, bias_v.repeat(1, bsz, 1)])\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = pad(attn_mask, (0, 1))\n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = pad(key_padding_mask, (0, 1))\n",
    "    else:\n",
    "        assert bias_k is None\n",
    "        assert bias_v is None\n",
    "\n",
    "    #\n",
    "    # reshape q, k, v for multihead attention and make them batch first\n",
    "    #\n",
    "    q = q.view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    if static_k is None:\n",
    "        k = k.view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    else:\n",
    "        # TODO finish disentangling control flow so we don't do in-projections when statics are passed\n",
    "        assert (\n",
    "            static_k.size(0) == bsz * num_heads\n",
    "        ), f\"expecting static_k.size(0) of {bsz * num_heads}, but got {static_k.size(0)}\"\n",
    "        assert (\n",
    "            static_k.size(2) == head_dim\n",
    "        ), f\"expecting static_k.size(2) of {head_dim}, but got {static_k.size(2)}\"\n",
    "        k = static_k\n",
    "    if static_v is None:\n",
    "        v = v.view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    else:\n",
    "        # TODO finish disentangling control flow so we don't do in-projections when statics are passed\n",
    "        assert (\n",
    "            static_v.size(0) == bsz * num_heads\n",
    "        ), f\"expecting static_v.size(0) of {bsz * num_heads}, but got {static_v.size(0)}\"\n",
    "        assert (\n",
    "            static_v.size(2) == head_dim\n",
    "        ), f\"expecting static_v.size(2) of {head_dim}, but got {static_v.size(2)}\"\n",
    "        v = static_v\n",
    "\n",
    "    # add zero attention along batch dimension (now first)\n",
    "    if add_zero_attn:\n",
    "        zero_attn_shape = (bsz * num_heads, 1, head_dim)\n",
    "        k = torch.cat(\n",
    "            [k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1\n",
    "        )\n",
    "        v = torch.cat(\n",
    "            [v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1\n",
    "        )\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = pad(attn_mask, (0, 1))\n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = pad(key_padding_mask, (0, 1))\n",
    "\n",
    "    # update source sequence length after adjustments\n",
    "    src_len = k.size(1)\n",
    "\n",
    "    # merge key padding and attention masks\n",
    "    if key_padding_mask is not None:\n",
    "        assert key_padding_mask.shape == (\n",
    "            bsz,\n",
    "            src_len,\n",
    "        ), f\"expecting key_padding_mask shape of {(bsz, src_len)}, but got {key_padding_mask.shape}\"\n",
    "        key_padding_mask = (\n",
    "            key_padding_mask.view(bsz, 1, 1, src_len)\n",
    "            .expand(-1, num_heads, -1, -1)\n",
    "            .reshape(bsz * num_heads, 1, src_len)\n",
    "        )\n",
    "        if attn_mask is None:\n",
    "            attn_mask = key_padding_mask\n",
    "        else:\n",
    "            attn_mask = attn_mask + key_padding_mask\n",
    "\n",
    "    # adjust dropout probability\n",
    "    if not training:\n",
    "        dropout_p = 0.0\n",
    "\n",
    "    #\n",
    "    # (deep breath) calculate attention and out projection\n",
    "    #\n",
    "\n",
    "    if need_weights:\n",
    "        B, Nt, E = q.shape\n",
    "        q_scaled = q * math.sqrt(1.0 / float(E))\n",
    "\n",
    "        assert not (\n",
    "            is_causal and attn_mask is None\n",
    "        ), \"FIXME: is_causal not implemented for need_weights\"\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            pre_softmax_interaction = attn_mask.expand(-1,128,-1)\n",
    "            pre_softmax_attention = torch.bmm(q_scaled, k.transpose(-2, -1))\n",
    "            pre_softmax_attention.detach()\n",
    "            pre_softmax_interaction.detach()\n",
    "            attn_output_weights = torch.baddbmm(\n",
    "                input=attn_mask, batch1=q_scaled, batch2=k.transpose(-2, -1)\n",
    "            )\n",
    "        else:\n",
    "            attn_output_weights = pre_softmax_attention = torch.bmm(q_scaled, k.transpose(-2, -1))\n",
    "        \n",
    "        attn_output_weights = softmax(attn_output_weights, dim=-1)\n",
    "        if dropout_p > 0.0:\n",
    "            attn_output_weights = dropout(attn_output_weights, p=dropout_p)\n",
    "\n",
    "        attn_output = torch.bmm(attn_output_weights, v)\n",
    "\n",
    "        attn_output = (\n",
    "            attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)\n",
    "        )\n",
    "        attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
    "        attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))\n",
    "\n",
    "        # optionally average attention weights over heads\n",
    "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
    "        if average_attn_weights:\n",
    "            attn_output_weights = attn_output_weights.mean(dim=1)\n",
    "\n",
    "        if not is_batched:\n",
    "            # squeeze the output if input was unbatched\n",
    "            attn_output = attn_output.squeeze(1)\n",
    "            attn_output_weights = attn_output_weights.squeeze(0)\n",
    "        if return_pre_softmax:\n",
    "            return attn_output, attn_output_weights, pre_softmax_attention, pre_softmax_interaction\n",
    "        else:\n",
    "            return attn_output, attn_output_weights\n",
    "    else:\n",
    "        # attn_mask can be either (L,S) or (N*num_heads, L, S)\n",
    "        # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)\n",
    "        # in order to match the input for SDPA of (N, num_heads, L, S)\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n",
    "                attn_mask = attn_mask.unsqueeze(0)\n",
    "            else:\n",
    "                attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n",
    "\n",
    "        q = q.view(bsz, num_heads, tgt_len, head_dim)\n",
    "        k = k.view(bsz, num_heads, src_len, head_dim)\n",
    "        v = v.view(bsz, num_heads, src_len, head_dim)\n",
    "\n",
    "        attn_output = scaled_dot_product_attention(\n",
    "            q, k, v, attn_mask, dropout_p, is_causal\n",
    "        )\n",
    "        attn_output = (\n",
    "            attn_output.permute(2, 0, 1, 3).contiguous().view(bsz * tgt_len, embed_dim)\n",
    "        )\n",
    "\n",
    "        attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
    "        attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))\n",
    "        if not is_batched:\n",
    "            # squeeze the output if input was unbatched\n",
    "            attn_output = attn_output.squeeze(1)\n",
    "        return attn_output, None\n",
    "def _mha_shape_check(\n",
    "    query: Tensor,\n",
    "    key: Tensor,\n",
    "    value: Tensor,\n",
    "    key_padding_mask: Optional[Tensor],\n",
    "    attn_mask: Optional[Tensor],\n",
    "    num_heads: int,\n",
    "):\n",
    "    # Verifies the expected shape for `query, `key`, `value`, `key_padding_mask` and `attn_mask`\n",
    "    # and returns if the input is batched or not.\n",
    "    # Raises an error if `query` is not 2-D (unbatched) or 3-D (batched) tensor.\n",
    "\n",
    "    # Shape check.\n",
    "    if query.dim() == 3:\n",
    "        # Batched Inputs\n",
    "        is_batched = True\n",
    "        assert key.dim() == 3 and value.dim() == 3, (\n",
    "            \"For batched (3-D) `query`, expected `key` and `value` to be 3-D\"\n",
    "            f\" but found {key.dim()}-D and {value.dim()}-D tensors respectively\"\n",
    "        )\n",
    "        if key_padding_mask is not None:\n",
    "            assert key_padding_mask.dim() == 2, (\n",
    "                \"For batched (3-D) `query`, expected `key_padding_mask` to be `None` or 2-D\"\n",
    "                f\" but found {key_padding_mask.dim()}-D tensor instead\"\n",
    "            )\n",
    "        if attn_mask is not None:\n",
    "            assert attn_mask.dim() in (2, 3), (\n",
    "                \"For batched (3-D) `query`, expected `attn_mask` to be `None`, 2-D or 3-D\"\n",
    "                f\" but found {attn_mask.dim()}-D tensor instead\"\n",
    "            )\n",
    "    elif query.dim() == 2:\n",
    "        # Unbatched Inputs\n",
    "        is_batched = False\n",
    "        assert key.dim() == 2 and value.dim() == 2, (\n",
    "            \"For unbatched (2-D) `query`, expected `key` and `value` to be 2-D\"\n",
    "            f\" but found {key.dim()}-D and {value.dim()}-D tensors respectively\"\n",
    "        )\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            assert key_padding_mask.dim() == 1, (\n",
    "                \"For unbatched (2-D) `query`, expected `key_padding_mask` to be `None` or 1-D\"\n",
    "                f\" but found {key_padding_mask.dim()}-D tensor instead\"\n",
    "            )\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            assert attn_mask.dim() in (2, 3), (\n",
    "                \"For unbatched (2-D) `query`, expected `attn_mask` to be `None`, 2-D or 3-D\"\n",
    "                f\" but found {attn_mask.dim()}-D tensor instead\"\n",
    "            )\n",
    "            if attn_mask.dim() == 3:\n",
    "                expected_shape = (num_heads, query.shape[0], key.shape[0])\n",
    "                assert (\n",
    "                    attn_mask.shape == expected_shape\n",
    "                ), f\"Expected `attn_mask` shape to be {expected_shape} but got {attn_mask.shape}\"\n",
    "    else:\n",
    "        raise AssertionError(\n",
    "            f\"query should be unbatched 2D or batched 3D tensor but received {query.dim()}-D query tensor\"\n",
    "        )\n",
    "\n",
    "    return is_batched\n",
    "def _in_projection_packed(\n",
    "    q: Tensor,\n",
    "    k: Tensor,\n",
    "    v: Tensor,\n",
    "    w: Tensor,\n",
    "    b: Optional[Tensor] = None,\n",
    ") -> List[Tensor]:\n",
    "    r\"\"\"Perform the in-projection step of the attention operation, using packed weights.\n",
    "\n",
    "    Output is a triple containing projection tensors for query, key and value.\n",
    "\n",
    "    Args:\n",
    "        q, k, v: query, key and value tensors to be projected. For self-attention,\n",
    "            these are typically the same tensor; for encoder-decoder attention,\n",
    "            k and v are typically the same tensor. (We take advantage of these\n",
    "            identities for performance if they are present.) Regardless, q, k and v\n",
    "            must share a common embedding dimension; otherwise their shapes may vary.\n",
    "        w: projection weights for q, k and v, packed into a single tensor. Weights\n",
    "            are packed along dimension 0, in q, k, v order.\n",
    "        b: optional projection biases for q, k and v, packed into a single tensor\n",
    "            in q, k, v order.\n",
    "\n",
    "    Shape:\n",
    "        Inputs:\n",
    "        - q: :math:`(..., E)` where E is the embedding dimension\n",
    "        - k: :math:`(..., E)` where E is the embedding dimension\n",
    "        - v: :math:`(..., E)` where E is the embedding dimension\n",
    "        - w: :math:`(E * 3, E)` where E is the embedding dimension\n",
    "        - b: :math:`E * 3` where E is the embedding dimension\n",
    "\n",
    "        Output:\n",
    "        - in output list :math:`[q', k', v']`, each output tensor will have the\n",
    "            same shape as the corresponding input tensor.\n",
    "    \"\"\"\n",
    "    E = q.size(-1)\n",
    "    if k is v:\n",
    "        if q is k:\n",
    "            # self-attention\n",
    "            proj = linear(q, w, b)\n",
    "            # reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\n",
    "            proj = (\n",
    "                proj.unflatten(-1, (3, E))\n",
    "                .unsqueeze(0)\n",
    "                .transpose(0, -2)\n",
    "                .squeeze(-2)\n",
    "                .contiguous()\n",
    "            )\n",
    "            return proj[0], proj[1], proj[2]\n",
    "        else:\n",
    "            # encoder-decoder attention\n",
    "            w_q, w_kv = w.split([E, E * 2])\n",
    "            if b is None:\n",
    "                b_q = b_kv = None\n",
    "            else:\n",
    "                b_q, b_kv = b.split([E, E * 2])\n",
    "            q_proj = linear(q, w_q, b_q)\n",
    "            kv_proj = linear(k, w_kv, b_kv)\n",
    "            # reshape to 2, E and not E, 2 is deliberate for better memory coalescing and keeping same order as chunk()\n",
    "            kv_proj = (\n",
    "                kv_proj.unflatten(-1, (2, E))\n",
    "                .unsqueeze(0)\n",
    "                .transpose(0, -2)\n",
    "                .squeeze(-2)\n",
    "                .contiguous()\n",
    "            )\n",
    "            return (q_proj, kv_proj[0], kv_proj[1])\n",
    "    else:\n",
    "        w_q, w_k, w_v = w.chunk(3)\n",
    "        if b is None:\n",
    "            b_q = b_k = b_v = None\n",
    "        else:\n",
    "            b_q, b_k, b_v = b.chunk(3)\n",
    "        return linear(q, w_q, b_q), linear(k, w_k, b_k), linear(v, w_v, b_v)\n",
    "def softmax(\n",
    "    input: Tensor,\n",
    "    dim: Optional[int] = None,\n",
    "    _stacklevel: int = 3,\n",
    "    dtype: Optional[torch.dtype] = None,\n",
    ") -> Tensor:\n",
    "    r\"\"\"Apply a softmax function.\n",
    "\n",
    "    Softmax is defined as:\n",
    "\n",
    "    :math:`\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}`\n",
    "\n",
    "    It is applied to all slices along dim, and will re-scale them so that the elements\n",
    "    lie in the range `[0, 1]` and sum to 1.\n",
    "\n",
    "    See :class:`~torch.nn.Softmax` for more details.\n",
    "\n",
    "    Args:\n",
    "        input (Tensor): input\n",
    "        dim (int): A dimension along which softmax will be computed.\n",
    "        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
    "          If specified, the input tensor is casted to :attr:`dtype` before the operation\n",
    "          is performed. This is useful for preventing data type overflows. Default: None.\n",
    "\n",
    "    .. note::\n",
    "        This function doesn't work directly with NLLLoss,\n",
    "        which expects the Log to be computed between the Softmax and itself.\n",
    "        Use log_softmax instead (it's faster and has better numerical properties).\n",
    "\n",
    "    \"\"\"\n",
    "    if has_torch_function_unary(input):\n",
    "        return handle_torch_function(\n",
    "            softmax, (input,), input, dim=dim, _stacklevel=_stacklevel, dtype=dtype\n",
    "        )\n",
    "    if dim is None:\n",
    "        dim = _get_softmax_dim(\"softmax\", input.dim(), _stacklevel)\n",
    "    if dtype is None:\n",
    "        ret = input.softmax(dim)\n",
    "    else:\n",
    "        ret = input.softmax(dim, dtype=dtype)\n",
    "    return ret\n",
    "\n",
    "\n",
    "\n",
    "def build_features_and_labels(tree, transform_features=True):\n",
    "    # load arrays from the tree\n",
    "    a = tree.arrays(filter_name=['part_*', 'jet_pt', 'jet_energy', 'label_*'])\n",
    "\n",
    "    # compute new features\n",
    "    a['part_mask'] = ak.ones_like(a['part_energy'])\n",
    "    a['part_pt'] = np.hypot(a['part_px'], a['part_py'])\n",
    "    a['part_pt_log'] = np.log(a['part_pt'])\n",
    "    a['part_e_log'] = np.log(a['part_energy'])\n",
    "    a['part_logptrel'] = np.log(a['part_pt']/a['jet_pt'])\n",
    "    a['part_logerel'] = np.log(a['part_energy']/a['jet_energy'])\n",
    "    a['part_deltaR'] = np.hypot(a['part_deta'], a['part_dphi'])\n",
    "    a['part_d0'] = np.tanh(a['part_d0val'])\n",
    "    a['part_dz'] = np.tanh(a['part_dzval'])\n",
    "\n",
    "    # apply standardization\n",
    "    if transform_features:\n",
    "        a['part_pt_log'] = (a['part_pt_log'] - 1.7) * 0.7\n",
    "        a['part_e_log'] = (a['part_e_log'] - 2.0) * 0.7\n",
    "        a['part_logptrel'] = (a['part_logptrel'] - (-4.7)) * 0.7\n",
    "        a['part_logerel'] = (a['part_logerel'] - (-4.7)) * 0.7\n",
    "        a['part_deltaR'] = (a['part_deltaR'] - 0.2) * 4.0\n",
    "        a['part_d0err'] = _clip(a['part_d0err'], 0, 1)\n",
    "        a['part_dzerr'] = _clip(a['part_dzerr'], 0, 1)\n",
    "\n",
    "    feature_list = {\n",
    "        'pf_points': ['part_deta', 'part_dphi'], # not used in ParT\n",
    "        'pf_features': [\n",
    "            'part_pt_log',\n",
    "            'part_e_log',\n",
    "            'part_logptrel',\n",
    "            'part_logerel',\n",
    "            'part_deltaR',\n",
    "            'part_charge',\n",
    "            'part_isChargedHadron',\n",
    "            'part_isNeutralHadron',\n",
    "            'part_isPhoton',\n",
    "            'part_isElectron',\n",
    "            'part_isMuon',\n",
    "            'part_d0',\n",
    "            'part_d0err',\n",
    "            'part_dz',\n",
    "            'part_dzerr',\n",
    "            'part_deta',\n",
    "            'part_dphi',\n",
    "        ],\n",
    "        'pf_vectors': [\n",
    "            'part_px',\n",
    "            'part_py',\n",
    "            'part_pz',\n",
    "            'part_energy',\n",
    "        ],\n",
    "        'pf_mask': ['part_mask']\n",
    "    }\n",
    "\n",
    "    out = {}\n",
    "    for k, names in feature_list.items():\n",
    "        out[k] = np.stack([_pad(a[n], maxlen=128).to_numpy() for n in names], axis=1)\n",
    "\n",
    "    label_list = ['label_QCD', 'label_Hbb', 'label_Hcc', 'label_Hgg', 'label_H4q', 'label_Hqql', 'label_Zqq', 'label_Wqq', 'label_Tbqq', 'label_Tbl']\n",
    "    out['label'] = np.stack([a[n].to_numpy().astype('int') for n in label_list], axis=1)\n",
    "\n",
    "    return out\n",
    "\n",
    "def _clip(a, a_min, a_max):\n",
    "    try:\n",
    "        return np.clip(a, a_min, a_max)\n",
    "    except ValueError:\n",
    "        return ak.unflatten(np.clip(ak.flatten(a), a_min, a_max), ak.num(a))\n",
    "\n",
    "def _pad(a, maxlen, value=0, dtype='float32'):\n",
    "    if isinstance(a, np.ndarray) and a.ndim >= 2 and a.shape[1] == maxlen:\n",
    "        return a\n",
    "    elif isinstance(a, ak.Array):\n",
    "        if a.ndim == 1:\n",
    "            a = ak.unflatten(a, 1)\n",
    "        a = ak.fill_none(ak.pad_none(a, maxlen, clip=True), value)\n",
    "        return ak.values_astype(a, dtype)\n",
    "    else:\n",
    "        x = (np.ones((len(a), maxlen)) * value).astype(dtype)\n",
    "        for idx, s in enumerate(a):\n",
    "            if not len(s):\n",
    "                continue\n",
    "            trunc = s[:maxlen].astype(dtype)\n",
    "            x[idx, :len(trunc)] = trunc\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "''' Particle Transformer (ParT)\n",
    "\n",
    "Paper: \"Particle Transformer for Jet Tagging\" - https://arxiv.org/abs/2202.03772\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def delta_phi(a, b):\n",
    "    return (a - b + math.pi) % (2 * math.pi) - math.pi\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def delta_r2(eta1, phi1, eta2, phi2):\n",
    "    return (eta1 - eta2)**2 + delta_phi(phi1, phi2)**2\n",
    "\n",
    "\n",
    "def to_pt2(x, eps=1e-8):\n",
    "    pt2 = x[:, :2].square().sum(dim=1, keepdim=True)\n",
    "    if eps is not None:\n",
    "        pt2 = pt2.clamp(min=eps)\n",
    "    return pt2\n",
    "\n",
    "\n",
    "def to_m2(x, eps=1e-8):\n",
    "    m2 = x[:, 3:4].square() - x[:, :3].square().sum(dim=1, keepdim=True)\n",
    "    if eps is not None:\n",
    "        m2 = m2.clamp(min=eps)\n",
    "    return m2\n",
    "\n",
    "\n",
    "def atan2(y, x):\n",
    "    sx = torch.sign(x)\n",
    "    sy = torch.sign(y)\n",
    "    pi_part = (sy + sx * (sy ** 2 - 1)) * (sx - 1) * (-math.pi / 2)\n",
    "    atan_part = torch.arctan(y / (x + (1 - sx ** 2))) * sx ** 2\n",
    "    return atan_part + pi_part\n",
    "\n",
    "\n",
    "def to_ptrapphim(x, return_mass=True, eps=1e-8, for_onnx=False):\n",
    "    # x: (N, 4, ...), dim1 : (px, py, pz, E)\n",
    "    px, py, pz, energy = x.split((1, 1, 1, 1), dim=1)\n",
    "    pt = torch.sqrt(to_pt2(x, eps=eps))\n",
    "    # rapidity = 0.5 * torch.log((energy + pz) / (energy - pz))\n",
    "    rapidity = 0.5 * torch.log(1 + (2 * pz) / (energy - pz).clamp(min=1e-20))\n",
    "    phi = (atan2 if for_onnx else torch.atan2)(py, px)\n",
    "    if not return_mass:\n",
    "        return torch.cat((pt, rapidity, phi), dim=1)\n",
    "    else:\n",
    "        m = torch.sqrt(to_m2(x, eps=eps))\n",
    "        return torch.cat((pt, rapidity, phi, m), dim=1)\n",
    "\n",
    "\n",
    "def boost(x, boostp4, eps=1e-8):\n",
    "    # boost x to the rest frame of boostp4\n",
    "    # x: (N, 4, ...), dim1 : (px, py, pz, E)\n",
    "    p3 = -boostp4[:, :3] / boostp4[:, 3:].clamp(min=eps)\n",
    "    b2 = p3.square().sum(dim=1, keepdim=True)\n",
    "    gamma = (1 - b2).clamp(min=eps)**(-0.5)\n",
    "    gamma2 = (gamma - 1) / b2\n",
    "    gamma2.masked_fill_(b2 == 0, 0)\n",
    "    bp = (x[:, :3] * p3).sum(dim=1, keepdim=True)\n",
    "    v = x[:, :3] + gamma2 * bp * p3 + x[:, 3:] * gamma * p3\n",
    "    return v\n",
    "\n",
    "\n",
    "def p3_norm(p, eps=1e-8):\n",
    "    return p[:, :3] / p[:, :3].norm(dim=1, keepdim=True).clamp(min=eps)\n",
    "\n",
    "\n",
    "def pairwise_lv_fts(xi, xj, num_outputs=4, eps=1e-8, for_onnx=False):\n",
    "    pti, rapi, phii = to_ptrapphim(xi, False, eps=None, for_onnx=for_onnx).split((1, 1, 1), dim=1)\n",
    "    ptj, rapj, phij = to_ptrapphim(xj, False, eps=None, for_onnx=for_onnx).split((1, 1, 1), dim=1)\n",
    "\n",
    "    delta = delta_r2(rapi, phii, rapj, phij).sqrt()\n",
    "    lndelta = torch.log(delta.clamp(min=eps))\n",
    "    if num_outputs == 1:\n",
    "        return lndelta\n",
    "\n",
    "    if num_outputs > 1:\n",
    "        ptmin = ((pti <= ptj) * pti + (pti > ptj) * ptj) if for_onnx else torch.minimum(pti, ptj)\n",
    "        lnkt = torch.log((ptmin * delta).clamp(min=eps))\n",
    "        lnz = torch.log((ptmin / (pti + ptj).clamp(min=eps)).clamp(min=eps))\n",
    "        outputs = [lnkt, lnz, lndelta]\n",
    "\n",
    "    if num_outputs > 3:\n",
    "        xij = xi + xj\n",
    "        lnm2 = torch.log(to_m2(xij, eps=eps))\n",
    "        outputs.append(lnm2)\n",
    "\n",
    "    if num_outputs > 4:\n",
    "        lnds2 = torch.log(torch.clamp(-to_m2(xi - xj, eps=None), min=eps))\n",
    "        outputs.append(lnds2)\n",
    "\n",
    "    # the following features are not symmetric for (i, j)\n",
    "    if num_outputs > 5:\n",
    "        xj_boost = boost(xj, xij)\n",
    "        costheta = (p3_norm(xj_boost, eps=eps) * p3_norm(xij, eps=eps)).sum(dim=1, keepdim=True)\n",
    "        outputs.append(costheta)\n",
    "\n",
    "    if num_outputs > 6:\n",
    "        deltarap = rapi - rapj\n",
    "        deltaphi = delta_phi(phii, phij)\n",
    "        outputs += [deltarap, deltaphi]\n",
    "\n",
    "    assert (len(outputs) == num_outputs)\n",
    "    return torch.cat(outputs, dim=1)\n",
    "\n",
    "\n",
    "def build_sparse_tensor(uu, idx, seq_len):\n",
    "    # inputs: uu (N, C, num_pairs), idx (N, 2, num_pairs)\n",
    "    # return: (N, C, seq_len, seq_len)\n",
    "    batch_size, num_fts, num_pairs = uu.size()\n",
    "    idx = torch.min(idx, torch.ones_like(idx) * seq_len)\n",
    "    i = torch.cat((\n",
    "        torch.arange(0, batch_size, device=uu.device).repeat_interleave(num_fts * num_pairs).unsqueeze(0),\n",
    "        torch.arange(0, num_fts, device=uu.device).repeat_interleave(num_pairs).repeat(batch_size).unsqueeze(0),\n",
    "        idx[:, :1, :].expand_as(uu).flatten().unsqueeze(0),\n",
    "        idx[:, 1:, :].expand_as(uu).flatten().unsqueeze(0),\n",
    "    ), dim=0)\n",
    "    return torch.sparse_coo_tensor(\n",
    "        i, uu.flatten(),\n",
    "        size=(batch_size, num_fts, seq_len + 1, seq_len + 1),\n",
    "        device=uu.device).to_dense()[:, :, :seq_len, :seq_len]\n",
    "\n",
    "\n",
    "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
    "    # From https://github.com/rwightman/pytorch-image-models/blob/18ec173f95aa220af753358bf860b16b6691edb2/timm/layers/weight_init.py#L8\n",
    "    r\"\"\"Fills the input Tensor with values drawn from a truncated\n",
    "    normal distribution. The values are effectively drawn from the\n",
    "    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n",
    "    with values outside :math:`[a, b]` redrawn until they are within\n",
    "    the bounds. The method used for generating the random values works\n",
    "    best when :math:`a \\leq \\text{mean} \\leq b`.\n",
    "    Args:\n",
    "        tensor: an n-dimensional `torch.Tensor`\n",
    "        mean: the mean of the normal distribution\n",
    "        std: the standard deviation of the normal distribution\n",
    "        a: the minimum cutoff value\n",
    "        b: the maximum cutoff value\n",
    "    Examples:\n",
    "        >>> w = torch.empty(3, 5)\n",
    "        >>> nn.init.trunc_normal_(w)\n",
    "    \"\"\"\n",
    "    def norm_cdf(x):\n",
    "        # Computes standard normal cumulative distribution function\n",
    "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
    "\n",
    "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
    "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
    "                      \"The distribution of values may be incorrect.\",\n",
    "                      stacklevel=2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Values are generated by using a truncated uniform distribution and\n",
    "        # then using the inverse CDF for the normal distribution.\n",
    "        # Get upper and lower cdf values\n",
    "        l = norm_cdf((a - mean) / std)\n",
    "        u = norm_cdf((b - mean) / std)\n",
    "\n",
    "        # Uniformly fill tensor with values from [l, u], then translate to\n",
    "        # [2l-1, 2u-1].\n",
    "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
    "\n",
    "        # Use inverse cdf transform for normal distribution to get truncated\n",
    "        # standard normal\n",
    "        tensor.erfinv_()\n",
    "\n",
    "        # Transform to proper mean, std\n",
    "        tensor.mul_(std * math.sqrt(2.))\n",
    "        tensor.add_(mean)\n",
    "\n",
    "        # Clamp to ensure it's in the proper range\n",
    "        tensor.clamp_(min=a, max=b)\n",
    "        return tensor\n",
    "\n",
    "\n",
    "class SequenceTrimmer(nn.Module):\n",
    "\n",
    "    def __init__(self, enabled=False, target=(0.9, 1.02), **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.enabled = enabled\n",
    "        self.target = target\n",
    "        self._counter = 0\n",
    "\n",
    "    def forward(self, x, v=None, mask=None, uu=None, **kwargs):\n",
    "        # x: (N, C, P)\n",
    "        # v: (N, 4, P) [px,py,pz,energy]\n",
    "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
    "        # uu: (N, C', P, P)\n",
    "        if mask is None:\n",
    "            mask = torch.ones_like(x[:, :1])\n",
    "        mask = mask.bool()\n",
    "\n",
    "        if self.enabled:\n",
    "            if self._counter < 5:\n",
    "                self._counter += 1\n",
    "            else:\n",
    "                if self.training:\n",
    "                    q = min(1, random.uniform(*self.target))\n",
    "                    maxlen = torch.quantile(mask.type_as(x).sum(dim=-1), q).long()\n",
    "                    rand = torch.rand_like(mask.type_as(x))\n",
    "                    rand.masked_fill_(~mask, -1)\n",
    "                    perm = rand.argsort(dim=-1, descending=True)  # (N, 1, P)\n",
    "                    mask = torch.gather(mask, -1, perm)\n",
    "                    x = torch.gather(x, -1, perm.expand_as(x))\n",
    "                    if v is not None:\n",
    "                        v = torch.gather(v, -1, perm.expand_as(v))\n",
    "                    if uu is not None:\n",
    "                        uu = torch.gather(uu, -2, perm.unsqueeze(-1).expand_as(uu))\n",
    "                        uu = torch.gather(uu, -1, perm.unsqueeze(-2).expand_as(uu))\n",
    "                else:\n",
    "                    maxlen = mask.sum(dim=-1).max()\n",
    "                maxlen = max(maxlen, 1)\n",
    "                if maxlen < mask.size(-1):\n",
    "                    mask = mask[:, :, :maxlen]\n",
    "                    x = x[:, :, :maxlen]\n",
    "                    if v is not None:\n",
    "                        v = v[:, :, :maxlen]\n",
    "                    if uu is not None:\n",
    "                        uu = uu[:, :, :maxlen, :maxlen]\n",
    "\n",
    "        return x, v, mask, uu\n",
    "\n",
    "\n",
    "class Embed(nn.Module):\n",
    "    def __init__(self, input_dim, dims, normalize_input=True, activation='gelu'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_bn = nn.BatchNorm1d(input_dim) if normalize_input else None\n",
    "        module_list = []\n",
    "        for dim in dims:\n",
    "            module_list.extend([\n",
    "                nn.LayerNorm(input_dim),\n",
    "                nn.Linear(input_dim, dim),\n",
    "                nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
    "            ])\n",
    "            input_dim = dim\n",
    "        self.embed = nn.Sequential(*module_list)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        if self.input_bn is not None:\n",
    "            # x: (batch, embed_dim, seq_len)\n",
    "            x = self.input_bn(x)\n",
    "            x = x.permute(2, 0, 1).contiguous()\n",
    "        # x: (seq_len, batch, embed_dim)\n",
    "        return self.embed(x)\n",
    "\n",
    "\n",
    "class PairEmbed(nn.Module):\n",
    "    def __init__(\n",
    "            self, pairwise_lv_dim, pairwise_input_dim, dims,\n",
    "            remove_self_pair=False, use_pre_activation_pair=True, mode='sum',\n",
    "            normalize_input=True, activation='gelu', eps=1e-8,\n",
    "            for_onnx=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pairwise_lv_dim = pairwise_lv_dim\n",
    "        self.pairwise_input_dim = pairwise_input_dim\n",
    "        self.is_symmetric = (pairwise_lv_dim <= 5) and (pairwise_input_dim == 0)\n",
    "        self.remove_self_pair = remove_self_pair\n",
    "        self.mode = mode\n",
    "        self.for_onnx = for_onnx\n",
    "        self.pairwise_lv_fts = partial(pairwise_lv_fts, num_outputs=pairwise_lv_dim, eps=eps, for_onnx=for_onnx)\n",
    "        self.out_dim = dims[-1]\n",
    "\n",
    "        if self.mode == 'concat':\n",
    "            input_dim = pairwise_lv_dim + pairwise_input_dim\n",
    "            module_list = [nn.BatchNorm1d(input_dim)] if normalize_input else []\n",
    "            for dim in dims:\n",
    "                module_list.extend([\n",
    "                    nn.Conv1d(input_dim, dim, 1),\n",
    "                    nn.BatchNorm1d(dim),\n",
    "                    nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
    "                ])\n",
    "                input_dim = dim\n",
    "            if use_pre_activation_pair:\n",
    "                module_list = module_list[:-1]\n",
    "            self.embed = nn.Sequential(*module_list)\n",
    "        elif self.mode == 'sum':\n",
    "            if pairwise_lv_dim > 0:\n",
    "                input_dim = pairwise_lv_dim\n",
    "                module_list = [nn.BatchNorm1d(input_dim)] if normalize_input else []\n",
    "                for dim in dims:\n",
    "                    module_list.extend([\n",
    "                        nn.Conv1d(input_dim, dim, 1),\n",
    "                        nn.BatchNorm1d(dim),\n",
    "                        nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
    "                    ])\n",
    "                    input_dim = dim\n",
    "                if use_pre_activation_pair:\n",
    "                    module_list = module_list[:-1]\n",
    "                self.embed = nn.Sequential(*module_list)\n",
    "\n",
    "            if pairwise_input_dim > 0:\n",
    "                input_dim = pairwise_input_dim\n",
    "                module_list = [nn.BatchNorm1d(input_dim)] if normalize_input else []\n",
    "                for dim in dims:\n",
    "                    module_list.extend([\n",
    "                        nn.Conv1d(input_dim, dim, 1),\n",
    "                        nn.BatchNorm1d(dim),\n",
    "                        nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
    "                    ])\n",
    "                    input_dim = dim\n",
    "                if use_pre_activation_pair:\n",
    "                    module_list = module_list[:-1]\n",
    "                self.fts_embed = nn.Sequential(*module_list)\n",
    "        else:\n",
    "            raise RuntimeError('`mode` can only be `sum` or `concat`')\n",
    "\n",
    "    def forward(self, x, uu=None, **kwargs):\n",
    "        # x: (batch, v_dim, seq_len)\n",
    "        # uu: (batch, v_dim, seq_len, seq_len)\n",
    "        assert (x is not None or uu is not None)\n",
    "        with torch.no_grad():\n",
    "            if x is not None:\n",
    "                batch_size, _, seq_len = x.size()\n",
    "            else:\n",
    "                batch_size, _, seq_len, _ = uu.size()\n",
    "            if self.is_symmetric and not self.for_onnx:\n",
    "                i, j = torch.tril_indices(seq_len, seq_len, offset=-1 if self.remove_self_pair else 0,\n",
    "                                          device=(x if x is not None else uu).device)\n",
    "                if x is not None:\n",
    "                    x = x.unsqueeze(-1).repeat(1, 1, 1, seq_len)\n",
    "                    xi = x[:, :, i, j]  # (batch, dim, seq_len*(seq_len+1)/2)\n",
    "                    xj = x[:, :, j, i]\n",
    "                    x = self.pairwise_lv_fts(xi, xj)\n",
    "                if uu is not None:\n",
    "                    # (batch, dim, seq_len*(seq_len+1)/2)\n",
    "                    uu = uu[:, :, i, j]\n",
    "            else:\n",
    "                if x is not None:\n",
    "                    x = self.pairwise_lv_fts(x.unsqueeze(-1), x.unsqueeze(-2))\n",
    "                    if self.remove_self_pair:\n",
    "                        i = torch.arange(0, seq_len, device=x.device)\n",
    "                        x[:, :, i, i] = 0\n",
    "                    x = x.view(-1, self.pairwise_lv_dim, seq_len * seq_len)\n",
    "                if uu is not None:\n",
    "                    uu = uu.view(-1, self.pairwise_input_dim, seq_len * seq_len)\n",
    "            if self.mode == 'concat':\n",
    "                if x is None:\n",
    "                    pair_fts = uu\n",
    "                elif uu is None:\n",
    "                    pair_fts = x\n",
    "                else:\n",
    "                    pair_fts = torch.cat((x, uu), dim=1)\n",
    "\n",
    "        if self.mode == 'concat':\n",
    "            elements = self.embed(pair_fts)  # (batch, embed_dim, num_elements)\n",
    "        elif self.mode == 'sum':\n",
    "            if x is None:\n",
    "                elements = self.fts_embed(uu)\n",
    "            elif uu is None:\n",
    "                elements = self.embed(x)\n",
    "            else:\n",
    "                elements = self.embed(x) + self.fts_embed(uu)\n",
    "\n",
    "        if self.is_symmetric and not self.for_onnx:\n",
    "            y = torch.zeros(batch_size, self.out_dim, seq_len, seq_len, dtype=elements.dtype, device=elements.device)\n",
    "            y[:, :, i, j] = elements\n",
    "            y[:, :, j, i] = elements\n",
    "        else:\n",
    "            y = elements.view(-1, self.out_dim, seq_len, seq_len)\n",
    "        return y\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, embed_dim=128, num_heads=8, ffn_ratio=4,\n",
    "                 dropout=0.1, attn_dropout=0.1, activation_dropout=0.1,\n",
    "                 add_bias_kv=False, activation='gelu',\n",
    "                 scale_fc=True, scale_attn=True, scale_heads=True, scale_resids=True, return_pre_softmax=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.ffn_dim = embed_dim * ffn_ratio\n",
    "        self.return_pre_softmax = return_pre_softmax\n",
    "\n",
    "        self.pre_attn_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.attn = MultiheadAttention(\n",
    "                embed_dim, \n",
    "                num_heads, \n",
    "                return_pre_softmax=self.return_pre_softmax,\n",
    "                add_bias_kv=add_bias_kv\n",
    "                )\n",
    "\n",
    "        self.post_attn_norm = nn.LayerNorm(embed_dim) if scale_attn else None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.pre_fc_norm = nn.LayerNorm(embed_dim)\n",
    "        self.fc1 = nn.Linear(embed_dim, self.ffn_dim)\n",
    "        self.act = nn.GELU() if activation == 'gelu' else nn.ReLU()\n",
    "        self.act_dropout = nn.Dropout(activation_dropout)\n",
    "        self.post_fc_norm = nn.LayerNorm(self.ffn_dim) if scale_fc else None\n",
    "        self.fc2 = nn.Linear(self.ffn_dim, embed_dim)\n",
    "\n",
    "        self.c_attn = nn.Parameter(torch.ones(num_heads), requires_grad=True) if scale_heads else None\n",
    "        self.w_resid = nn.Parameter(torch.ones(embed_dim), requires_grad=True) if scale_resids else None\n",
    "\n",
    "    def forward(self, x, x_cls=None, padding_mask=None, attn_mask=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
    "            x_cls (Tensor, optional): class token input to the layer of shape `(1, batch, embed_dim)`\n",
    "            padding_mask (ByteTensor, optional): binary\n",
    "                ByteTensor of shape `(batch, seq_len)` where padding\n",
    "                elements are indicated by ``1``.\n",
    "\n",
    "        Returns:\n",
    "            encoded output of shape `(seq_len, batch, embed_dim)`\n",
    "        \"\"\"\n",
    "\n",
    "        if x_cls is not None:\n",
    "            with torch.no_grad():\n",
    "                # prepend one element for x_cls: -> (batch, 1+seq_len)\n",
    "                padding_mask = torch.cat((torch.zeros_like(padding_mask[:, :1]), padding_mask), dim=1)\n",
    "            # class attention: https://arxiv.org/pdf/2103.17239.pdf\n",
    "            residual = x_cls\n",
    "            u = torch.cat((x_cls, x), dim=0)  # (seq_len+1, batch, embed_dim)\n",
    "            u = self.pre_attn_norm(u)\n",
    "\n",
    "            if self.return_pre_softmax:\n",
    "                dict_output = self.attn(x_cls, u, u, key_padding_mask=padding_mask, \n",
    "                                                                                     return_pre_softmax=self.return_pre_softmax\n",
    "                                                                            )  # (1, batch, embed_dim)\n",
    "                x = dict_output['output']  # (1, batch, embed_dim)\n",
    "                pre_softmax_attention = dict_output['pre_softmax_attention']\n",
    "                pre_softmax_interaction = dict_output['pre_softmax_interaction']\n",
    "\n",
    "                pre_softmax_attention.detach()\n",
    "                pre_softmax_interaction.detach()\n",
    "\n",
    "            else:\n",
    "                x, _ = self.attn(x_cls, u, u, key_padding_mask=padding_mask, \n",
    "                                    return_pre_softmax=self.return_pre_softmax\n",
    "                                    )  # (1, batch, embed_dim)\n",
    "\n",
    "        else:            \n",
    "            residual = x\n",
    "            x = self.pre_attn_norm(x)\n",
    "            \n",
    "            if self.return_pre_softmax:\n",
    "                dict_output = self.attn(x, x, x, key_padding_mask=padding_mask, attn_mask=attn_mask,\n",
    "                                         return_pre_softmax=self.return_pre_softmax\n",
    "                                         )\n",
    "                x = dict_output['output']\n",
    "                pre_softmax_attention = dict_output['pre_softmax_attention']\n",
    "                pre_softmax_interaction = dict_output['pre_softmax_interaction']\n",
    "\n",
    "                pre_softmax_attention.detach()\n",
    "                pre_softmax_interaction.detach()\n",
    "\n",
    "            else:\n",
    "                x, _ = self.attn(x, x, x, key_padding_mask=padding_mask,\n",
    "                                attn_mask=attn_mask, return_pre_softmax=self.return_pre_softmax\n",
    "                                )  # (seq_len, batch, embed_dim)\n",
    "\n",
    "        if self.c_attn is not None:\n",
    "            tgt_len = x.size(0)\n",
    "            x = x.view(tgt_len, -1, self.num_heads, self.head_dim)\n",
    "            x = torch.einsum('tbhd,h->tbdh', x, self.c_attn)\n",
    "            x = x.reshape(tgt_len, -1, self.embed_dim)\n",
    "        if self.post_attn_norm is not None:\n",
    "            x = self.post_attn_norm(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x += residual.reshape(x.shape)\n",
    "\n",
    "        residual = x\n",
    "        x = self.pre_fc_norm(x)\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.act_dropout(x)\n",
    "        if self.post_fc_norm is not None:\n",
    "            x = self.post_fc_norm(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        if self.w_resid is not None:\n",
    "            residual = torch.mul(self.w_resid, residual)\n",
    "        x += residual\n",
    "\n",
    "        if self.return_pre_softmax:\n",
    "            return {'output': x, 'pre_softmax_attention': pre_softmax_attention, 'pre_softmax_interaction': pre_softmax_interaction}\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class ParticleTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 num_classes=None,\n",
    "                 # network configurations\n",
    "                 pair_input_dim=4,\n",
    "                 pair_extra_dim=0,\n",
    "                 remove_self_pair=False,\n",
    "                 use_pre_activation_pair=True,\n",
    "                 embed_dims=[128, 512, 128],\n",
    "                 pair_embed_dims=[64, 64, 64],\n",
    "                 num_heads=8,\n",
    "                 num_layers=8,\n",
    "                 num_cls_layers=2,\n",
    "                 block_params=None,\n",
    "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
    "                 fc_params=[],\n",
    "                 activation='gelu',\n",
    "                 # misc\n",
    "                 trim=True,\n",
    "                 for_inference=False,\n",
    "                 use_amp=False,\n",
    "                 return_pre_softmax=False,\n",
    "                 **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
    "        self.for_inference = for_inference\n",
    "        self.use_amp = use_amp\n",
    "        self.return_pre_softmax = return_pre_softmax\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # init the collected pre_softmax matrices for later torch.cat()\n",
    "\n",
    "        #if self.return_pre_softmax:\n",
    "        #    self.pre_softmax_attention = torch.empty(0, dtype=torch.float32)\n",
    "        #    self.pre_softmax_interaction = torch.empty(0, dtype=torch.float32)\n",
    "        #    self.cls_pre_softmax_attention = torch.empty(0, dtype=torch.float32)\n",
    "        #    self.cls_pre_softmax_interaction = torch.empty(0, dtype=torch.float32)\n",
    "\n",
    "        embed_dim = embed_dims[-1] if len(embed_dims) > 0 else input_dim\n",
    "        self.default_cfg = default_cfg = dict(embed_dim=embed_dim, num_heads=num_heads, ffn_ratio=4,\n",
    "                           dropout=0.1, attn_dropout=0.1, activation_dropout=0.1,\n",
    "                           add_bias_kv=False, activation=activation,\n",
    "                           scale_fc=True, scale_attn=True, scale_heads=True, scale_resids=True, return_pre_softmax=self.return_pre_softmax)\n",
    "                           \n",
    "\n",
    "        cfg_block = copy.deepcopy(default_cfg)\n",
    "        if block_params is not None:\n",
    "            cfg_block.update(block_params)\n",
    "        _logger.info('cfg_block: %s' % str(cfg_block))\n",
    "\n",
    "        cfg_cls_block = copy.deepcopy(default_cfg)\n",
    "        if cls_block_params is not None:\n",
    "            cfg_cls_block.update(cls_block_params)\n",
    "        _logger.info('cfg_cls_block: %s' % str(cfg_cls_block))\n",
    "\n",
    "        self.pair_extra_dim = pair_extra_dim\n",
    "        self.embed = Embed(input_dim, embed_dims, activation=activation) if len(embed_dims) > 0 else nn.Identity()\n",
    "        self.pair_embed = PairEmbed(\n",
    "            pair_input_dim, pair_extra_dim, pair_embed_dims + [cfg_block['num_heads']],\n",
    "            remove_self_pair=remove_self_pair, use_pre_activation_pair=use_pre_activation_pair,\n",
    "            for_onnx=for_inference) if pair_embed_dims is not None and pair_input_dim + pair_extra_dim > 0 else None\n",
    "        # we want to add descriptive layer names, so use ModuleDict instead of List\n",
    "        #self.blocks = nn.ModuleDict({f'Block_{i}': Block(**cfg_block) for i in range(num_layers)})\n",
    "        #self.cls_blocks = nn.ModuleDict({f'cls_Block_{i}': Block(**cfg_cls_block) for i in range(num_cls_layers)})\n",
    "\n",
    "        self.blocks = nn.ModuleList([Block(**cfg_block) for _ in range(num_layers)])\n",
    "        self.cls_blocks = nn.ModuleList([Block(**cfg_cls_block) for _ in range(num_cls_layers)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        if fc_params is not None:\n",
    "            fcs = []\n",
    "            in_dim = embed_dim\n",
    "            for out_dim, drop_rate in fc_params:\n",
    "                fcs.append(nn.Sequential(nn.Linear(in_dim, out_dim), nn.ReLU(), nn.Dropout(drop_rate)))\n",
    "                in_dim = out_dim\n",
    "            fcs.append(nn.Linear(in_dim, num_classes))\n",
    "            self.fc = nn.Sequential(*fcs)\n",
    "        else:\n",
    "            self.fc = None\n",
    "\n",
    "        # init\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim), requires_grad=True)\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'cls_token', }\n",
    "\n",
    "    def forward(self, x, v=None, mask=None, uu=None, uu_idx=None, **kwargs):\n",
    "        # x: (N, C, P)\n",
    "        # v: (N, 4, P) [px,py,pz,energy]\n",
    "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
    "        # for pytorch: uu (N, C', num_pairs), uu_idx (N, 2, num_pairs)\n",
    "        # for onnx: uu (N, C', P, P), uu_idx=None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if not self.for_inference:\n",
    "                if uu_idx is not None:\n",
    "                    uu = build_sparse_tensor(uu, uu_idx, x.size(-1))\n",
    "            x, v, mask, uu = self.trimmer(x, v, mask, uu)\n",
    "            padding_mask = ~mask.squeeze(1)  # (N, P)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
    "            # input embedding\n",
    "            x = self.embed(x).masked_fill(~mask.permute(2, 0, 1), 0)  # (P, N, C)\n",
    "            attn_mask = None\n",
    "            if (v is not None or uu is not None) and self.pair_embed is not None:\n",
    "                attn_mask = self.pair_embed(v, uu).view(-1, v.size(-1), v.size(-1))  # (N*num_heads, P, P)\n",
    "\n",
    "            # transform\n",
    "            for block in self.blocks:\n",
    "                if self.return_pre_softmax:\n",
    "                    #x, pre_softmax_attention_vals, pre_softmax_interaction_vals = block(x, x_cls=None, padding_mask=padding_mask, \n",
    "                    #                                                          attn_mask=attn_mask, return_pre_softmax=self.return_pre_softmax)['output']\n",
    "                    #pre_softmax_attention_vals = pre_softmax_attention_vals.unsqueeze(0)  # (N, num_heads, P, P)\n",
    "                    #pre_softmax_interaction_vals = pre_softmax_interaction_vals.unsqueeze(0)  # (N, num_heads, P, P)\n",
    "\n",
    "                    #self.pre_softmax_attention = torch.cat((self.pre_softmax_attention, pre_softmax_attention_vals), dim=0)\n",
    "                    #self.pre_softmax_interaction = torch.cat((self.pre_softmax_interaction, pre_softmax_interaction_vals), dim=0)\n",
    "                    x = block(x, x_cls=None, padding_mask=padding_mask, attn_mask=attn_mask, return_pre_softmax=self.return_pre_softmax)['output']\n",
    "                else:\n",
    "                    x = block(x, x_cls=None, padding_mask=padding_mask, attn_mask=attn_mask, return_pre_softmax=self.return_pre_softmax)\n",
    "\n",
    "            # extract class token\n",
    "            cls_tokens = self.cls_token.expand(1, x.size(1), -1)  # (1, N, C)\n",
    "            for block in self.cls_blocks:\n",
    "                if self.return_pre_softmax:\n",
    "                    cls_tokens = block(x, x_cls=cls_tokens, padding_mask=padding_mask, return_pre_softmax=self.return_pre_softmax)['output']\n",
    "                else:\n",
    "                    cls_tokens = block(x, x_cls=cls_tokens, padding_mask=padding_mask)\n",
    "\n",
    "            x_cls = self.norm(cls_tokens).squeeze(0)\n",
    "\n",
    "            # fc\n",
    "            if self.fc is None:\n",
    "                return x_cls\n",
    "            output = self.fc(x_cls)\n",
    "            if self.for_inference:\n",
    "                output = torch.softmax(output, dim=1)\n",
    "            # print('output:\\n', output)\n",
    "            if self.return_pre_softmax:\n",
    "                return output#, self.pre_softmax_attention, self.pre_softmax_interaction\n",
    "            else:\n",
    "                return output\n",
    "\n",
    "\n",
    "class ParticleTransformerTagger(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 pf_input_dim,\n",
    "                 sv_input_dim,\n",
    "                 num_classes=None,\n",
    "                 # network configurations\n",
    "                 pair_input_dim=4,\n",
    "                 pair_extra_dim=0,\n",
    "                 remove_self_pair=False,\n",
    "                 use_pre_activation_pair=True,\n",
    "                 embed_dims=[128, 512, 128],\n",
    "                 pair_embed_dims=[64, 64, 64],\n",
    "                 num_heads=8,\n",
    "                 num_layers=8,\n",
    "                 num_cls_layers=2,\n",
    "                 block_params=None,\n",
    "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
    "                 fc_params=[],\n",
    "                 activation='gelu',\n",
    "                 # misc\n",
    "                 trim=True,\n",
    "                 for_inference=False,\n",
    "                 use_amp=False,\n",
    "                 **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.use_amp = use_amp\n",
    "\n",
    "        self.pf_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
    "        self.sv_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
    "\n",
    "        self.pf_embed = Embed(pf_input_dim, embed_dims, activation=activation)\n",
    "        self.sv_embed = Embed(sv_input_dim, embed_dims, activation=activation)\n",
    "\n",
    "        self.part = ParticleTransformer(input_dim=embed_dims[-1],\n",
    "                                        num_classes=num_classes,\n",
    "                                        # network configurations\n",
    "                                        pair_input_dim=pair_input_dim,\n",
    "                                        pair_extra_dim=pair_extra_dim,\n",
    "                                        remove_self_pair=remove_self_pair,\n",
    "                                        use_pre_activation_pair=use_pre_activation_pair,\n",
    "                                        embed_dims=[],\n",
    "                                        pair_embed_dims=pair_embed_dims,\n",
    "                                        num_heads=num_heads,\n",
    "                                        num_layers=num_layers,\n",
    "                                        num_cls_layers=num_cls_layers,\n",
    "                                        block_params=block_params,\n",
    "                                        cls_block_params=cls_block_params,\n",
    "                                        fc_params=fc_params,\n",
    "                                        activation=activation,\n",
    "                                        # misc\n",
    "                                        trim=False,\n",
    "                                        for_inference=for_inference,\n",
    "                                        use_amp=use_amp)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'part.cls_token', }\n",
    "\n",
    "    def forward(self, pf_x, pf_v=None, pf_mask=None, sv_x=None, sv_v=None, sv_mask=None, **kwargs):\n",
    "        # x: (N, C, P)\n",
    "        # v: (N, 4, P) [px,py,pz,energy]\n",
    "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pf_x, pf_v, pf_mask, _ = self.pf_trimmer(pf_x, pf_v, pf_mask)\n",
    "            sv_x, sv_v, sv_mask, _ = self.sv_trimmer(sv_x, sv_v, sv_mask)\n",
    "            v = torch.cat([pf_v, sv_v], dim=2)\n",
    "            mask = torch.cat([pf_mask, sv_mask], dim=2)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
    "            pf_x = self.pf_embed(pf_x)  # after embed: (seq_len, batch, embed_dim)\n",
    "            sv_x = self.sv_embed(sv_x)\n",
    "            x = torch.cat([pf_x, sv_x], dim=0)\n",
    "\n",
    "            return self.part(x, v, mask)\n",
    "\n",
    "\n",
    "class ParticleTransformerTaggerWithExtraPairFeatures(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 pf_input_dim,\n",
    "                 sv_input_dim,\n",
    "                 num_classes=None,\n",
    "                 # network configurations\n",
    "                 pair_input_dim=4,\n",
    "                 pair_extra_dim=0,\n",
    "                 remove_self_pair=False,\n",
    "                 use_pre_activation_pair=True,\n",
    "                 embed_dims=[128, 512, 128],\n",
    "                 pair_embed_dims=[64, 64, 64],\n",
    "                 num_heads=8,\n",
    "                 num_layers=8,\n",
    "                 num_cls_layers=2,\n",
    "                 block_params=None,\n",
    "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
    "                 fc_params=[],\n",
    "                 activation='gelu',\n",
    "                 # misc\n",
    "                 trim=True,\n",
    "                 for_inference=False,\n",
    "                 use_amp=False,\n",
    "                 **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.use_amp = use_amp\n",
    "        self.for_inference = for_inference\n",
    "\n",
    "        self.pf_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
    "        self.sv_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
    "\n",
    "        self.pf_embed = Embed(pf_input_dim, embed_dims, activation=activation)\n",
    "        self.sv_embed = Embed(sv_input_dim, embed_dims, activation=activation)\n",
    "\n",
    "        self.part = ParticleTransformer(input_dim=embed_dims[-1],\n",
    "                                        num_classes=num_classes,\n",
    "                                        # network configurations\n",
    "                                        pair_input_dim=pair_input_dim,\n",
    "                                        pair_extra_dim=pair_extra_dim,\n",
    "                                        remove_self_pair=remove_self_pair,\n",
    "                                        use_pre_activation_pair=use_pre_activation_pair,\n",
    "                                        embed_dims=[],\n",
    "                                        pair_embed_dims=pair_embed_dims,\n",
    "                                        num_heads=num_heads,\n",
    "                                        num_layers=num_layers,\n",
    "                                        num_cls_layers=num_cls_layers,\n",
    "                                        block_params=block_params,\n",
    "                                        cls_block_params=cls_block_params,\n",
    "                                        fc_params=fc_params,\n",
    "                                        activation=activation,\n",
    "                                        # misc\n",
    "                                        trim=False,\n",
    "                                        for_inference=for_inference,\n",
    "                                        use_amp=use_amp)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'part.cls_token', }\n",
    "\n",
    "    def forward(self, pf_x, pf_v=None, pf_mask=None, sv_x=None, sv_v=None, sv_mask=None, pf_uu=None, pf_uu_idx=None, **kwargs):\n",
    "        # x: (N, C, P)\n",
    "        # v: (N, 4, P) [px,py,pz,energy]\n",
    "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if not self.for_inference:\n",
    "                if pf_uu_idx is not None:\n",
    "                    pf_uu = build_sparse_tensor(pf_uu, pf_uu_idx, pf_x.size(-1))\n",
    "\n",
    "            pf_x, pf_v, pf_mask, pf_uu = self.pf_trimmer(pf_x, pf_v, pf_mask, pf_uu)\n",
    "            sv_x, sv_v, sv_mask, _ = self.sv_trimmer(sv_x, sv_v, sv_mask)\n",
    "            v = torch.cat([pf_v, sv_v], dim=2)\n",
    "            mask = torch.cat([pf_mask, sv_mask], dim=2)\n",
    "            uu = torch.zeros(v.size(0), pf_uu.size(1), v.size(2), v.size(2), dtype=v.dtype, device=v.device)\n",
    "            uu[:, :, :pf_x.size(2), :pf_x.size(2)] = pf_uu\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
    "            pf_x = self.pf_embed(pf_x)  # after embed: (seq_len, batch, embed_dim)\n",
    "            sv_x = self.sv_embed(sv_x)\n",
    "            x = torch.cat([pf_x, sv_x], dim=0)\n",
    "\n",
    "            return self.part(x, v, mask, uu)\n",
    "\n",
    "class ParticleTransformerWrapper(torch.nn.Module):\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.mod = ParticleTransformer(**kwargs)\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'mod.cls_token', }\n",
    "\n",
    "    def forward(self, points, features, lorentz_vectors, mask):\n",
    "        #if 'return_pre_softmax' in self.kwargs.keys() and self.kwargs['return_pre_softmax'] == True :\n",
    "        #    output, pre_softmax_attention, pre_softmax_interaction = self.mod(features, v=lorentz_vectors, mask=mask)\n",
    "        #    return output, pre_softmax_attention, pre_softmax_interaction\n",
    "        #else:\n",
    "        output = self.mod(features, v=lorentz_vectors, mask=mask)\n",
    "        return output\n",
    "\n",
    "# Define Forward Hook to grab pre-softmax w/o altering outputs of forward() for ParticleTransformer\n",
    "\n",
    "class Pre_Softmax_Hook:\n",
    "    def __init__(self, model, layer_name='test'):\n",
    "        self.model = model\n",
    "        self.kwargs = self.model.kwargs\n",
    "\n",
    "        if isinstance(self.model, ParticleTransformerWrapper):\n",
    "            self.model = self.model.mod\n",
    "\n",
    "        for module in self.model.blocks:\n",
    "            #if layer_name in name:\n",
    "                # register forward hook functions\n",
    "            handle_attn = module.register_forward_hook(lambda *args, **kwargs: Pre_Softmax_Hook.get_pre_softmax_attention(self, *args, **kwargs))\n",
    "            handle_inter = module.register_forward_hook(lambda *args, **kwargs: Pre_Softmax_Hook.get_pre_softmax_interaction(self, *args, **kwargs))\n",
    "\n",
    "            print(f\"Registered hook onto module\")\n",
    "        #else:\n",
    "        #    print(f'A Layer does not match')\n",
    "\n",
    "        # for now, we will not register hooks on class blocks\n",
    "        # would not be too terribly difficult to implement, but I am tired as shit\n",
    "        # sorry for the language\n",
    "\n",
    "        #for module in self.model.cls_blocks:\n",
    "            #if layer_name in name:\n",
    "                # register forward hook function - no interaction in class blocks\n",
    "            #cls_handle_attn = module.register_forward_hook(lambda *args, **kwargs: Pre_Softmax_Hook.get_pre_softmax_attention(self, *args, **kwargs))\n",
    "            \n",
    "            #print(f\"Registered hook onto module\")\n",
    "        #else:\n",
    "        #    print(f'Layer {module} does not match {layer_name}')\n",
    "\n",
    "        self.handle_attn = handle_attn\n",
    "        self.handle_inter = handle_inter\n",
    "        #self.cls_handle_attn = cls_handle_attn\n",
    "\n",
    "        self.embed_dim = self.model.default_cfg['embed_dim']\n",
    "        self.num_heads = self.model.default_cfg['num_heads']\n",
    "        self.seq_len = self.embed_dim\n",
    "\n",
    "        self.pre_softmax_attentions = torch.empty((0, self.num_heads, self.seq_len, self.seq_len), dtype=torch.float32)\n",
    "        self.pre_softmax_interactions = torch.empty((0, self.num_heads, self.seq_len, self.seq_len), dtype=torch.float32)\n",
    "\n",
    "    # hooks will grab from the outputs of Block\n",
    "    # meaning we don't need to modify forward methods for anything else\n",
    "    # 3 hours later -- lol this was wrong\n",
    "\n",
    "    def get_pre_softmax_attention(self, module, input, output):\n",
    "        print('Getting pre_softmax attention...')\n",
    "\n",
    "        # handle batching - we will divide 1st dimension by the number such that it will comport with num_heads\n",
    "\n",
    "        output_hooked = output['pre_softmax_attention'].unsqueeze(dim=0)\n",
    "        output_split = torch.split(output_hooked, self.num_heads, dim=1)\n",
    "\n",
    "        print('Split the output into heads.\\nNew Tensor Shapes:')\n",
    "        for tensor in output_split:\n",
    "            print(f'{tensor.shape}')\n",
    "\n",
    "        for tensor in output_split:\n",
    "            self.pre_softmax_attentions = torch.cat((self.pre_softmax_attentions, tensor), dim=0)\n",
    "\n",
    "        # test to see if it will cat, if function is called\n",
    "\n",
    "        #self.pre_softmax_attentions = torch.cat((self.pre_softmax_attentions, torch.randn((1,self.model.num_layers, self.num_heads, self.seq_len, self.seq_len), dtype=torch.float32)), dim=0)\n",
    "\n",
    "    def get_pre_softmax_interaction(self, module, input, output):\n",
    "        print('Getting pre-softmaxed interaction...')\n",
    "\n",
    "        output_hooked = output['pre_softmax_interaction'].unsqueeze(dim=0)\n",
    "        output_split = torch.split(output_hooked, self.num_heads, dim=1)\n",
    "\n",
    "        print(f'Got shape:{output_hooked.shape}')\n",
    "\n",
    "        print('Split the output into heads.\\nNew Tensor Shapes:')\n",
    "        for tensor in output_split:\n",
    "            print(f'{tensor.shape}')\n",
    "\n",
    "        for tensor in output_split:\n",
    "            self.pre_softmax_interactions = torch.cat((self.pre_softmax_interactions, tensor), dim=0)\n",
    "\n",
    "    #def __call__(self, input_tensor: Tensor):\n",
    "    #    output = self.model(input_tensor, **self.kwargs)\n",
    "    #    return output\n",
    "\n",
    "    def clear(self):\n",
    "        self.pre_softmax_attentions = torch.empty((0, self.num_heads, self.seq_len, self.seq_len), dtype=torch.float32)\n",
    "        self.pre_softmax_interactions = torch.empty((0, self.num_heads, self.seq_len, self.seq_len), dtype=torch.float32)\n",
    "        self.handle_attn.remove()\n",
    "        self.handle_inter.remove()\n",
    "        #self.cls_handle_attn.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "05022d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "{'embed_dim': 128, 'num_heads': 8, 'ffn_ratio': 4, 'dropout': 0.1, 'attn_dropout': 0.1, 'activation_dropout': 0.1, 'add_bias_kv': False, 'activation': 'gelu', 'scale_fc': True, 'scale_attn': True, 'scale_heads': True, 'scale_resids': True, 'return_pre_softmax': True}\n",
      "Layer Names in test_ParT:\n",
      "Block(\n",
      "  (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (act): GELU(approximate='none')\n",
      "  (act_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n",
      "Block(\n",
      "  (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (act): GELU(approximate='none')\n",
      "  (act_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n",
      "Block(\n",
      "  (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (act): GELU(approximate='none')\n",
      "  (act_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n",
      "Block(\n",
      "  (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (act): GELU(approximate='none')\n",
      "  (act_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n",
      "Block(\n",
      "  (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (act): GELU(approximate='none')\n",
      "  (act_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n",
      "Block(\n",
      "  (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (act): GELU(approximate='none')\n",
      "  (act_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n",
      "Block(\n",
      "  (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (act): GELU(approximate='none')\n",
      "  (act_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n",
      "Block(\n",
      "  (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (act): GELU(approximate='none')\n",
      "  (act_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n",
      "Block(\n",
      "  (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (act): GELU(approximate='none')\n",
      "  (act_dropout): Dropout(p=0, inplace=False)\n",
      "  (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n",
      "Block(\n",
      "  (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (act): GELU(approximate='none')\n",
      "  (act_dropout): Dropout(p=0, inplace=False)\n",
      "  (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# testing with TL config and data\n",
    "\n",
    "test_config = dict(\n",
    "            input_dim=7,  # part_pt_log, part_e_log, part_logptrel, part_logerel, part_deltaR, part_deta, part_dphi\n",
    "            num_classes=2,  # Top vs QCD\n",
    "            pair_input_dim=4,\n",
    "            use_pre_activation_pair=False,\n",
    "            embed_dims=[128, 512, 128],\n",
    "            pair_embed_dims=[64, 64, 64],\n",
    "            num_heads=8,\n",
    "            num_layers=8,\n",
    "            num_cls_layers=2,\n",
    "            block_params=None,\n",
    "            cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
    "            fc_params=[],\n",
    "            activation='gelu',\n",
    "            trim=True,\n",
    "            for_inference=False,\n",
    "            return_pre_softmax=True, # this model will collect attention and interaction matrices before softmax is applied\n",
    "        )\n",
    "\n",
    "test_ParT = ParticleTransformerWrapper(**test_config)\n",
    "\n",
    "print(test_ParT.mod.return_pre_softmax)\n",
    "print(test_ParT.mod.default_cfg)\n",
    "\n",
    "print(f'Layer Names in test_ParT:')\n",
    "for name in test_ParT.mod.blocks:\n",
    "    print(name)\n",
    "for name in test_ParT.mod.cls_blocks:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "949683ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that the model by itself can product outputs\n",
    "# bring in actual data to verify this\n",
    "# code from QG_and_TL_AttentionGraphs.ipynb\n",
    "\n",
    "def build_features_and_labels_tl(tree, transform_features=True):\n",
    "    \"\"\"Build features for TopLandscape dataset based on top_kin.yaml\"\"\"\n",
    "    # load arrays from the tree\n",
    "    a = tree.arrays(filter_name=['part_*', 'jet_pt', 'jet_energy', 'label'])\n",
    "\n",
    "    # compute new features (same as QG)\n",
    "    a['part_mask'] = ak.ones_like(a['part_energy'])\n",
    "    a['part_pt'] = np.hypot(a['part_px'], a['part_py'])\n",
    "    a['part_pt_log'] = np.log(a['part_pt'])\n",
    "    a['part_e_log'] = np.log(a['part_energy'])\n",
    "    a['part_logptrel'] = np.log(a['part_pt']/a['jet_pt'])\n",
    "    a['part_logerel'] = np.log(a['part_energy']/a['jet_energy'])\n",
    "    a['part_deltaR'] = np.hypot(a['part_deta'], a['part_dphi'])\n",
    "\n",
    "    # apply standardization based on top_kin.yaml (same as QG)\n",
    "    if transform_features:\n",
    "        a['part_pt_log'] = (a['part_pt_log'] - 1.7) * 0.7\n",
    "        a['part_e_log'] = (a['part_e_log'] - 2.0) * 0.7\n",
    "        a['part_logptrel'] = (a['part_logptrel'] - (-4.7)) * 0.7\n",
    "        a['part_logerel'] = (a['part_logerel'] - (-4.7)) * 0.7\n",
    "        a['part_deltaR'] = (a['part_deltaR'] - 0.2) * 4.0\n",
    "\n",
    "    # Feature list for TopLandscape (same kinematic features as QG)\n",
    "    feature_list = {\n",
    "        'pf_points': ['part_deta', 'part_dphi'],\n",
    "        'pf_features': [\n",
    "            'part_pt_log',\n",
    "            'part_e_log',\n",
    "            'part_logptrel', \n",
    "            'part_logerel',\n",
    "            'part_deltaR',\n",
    "            'part_deta',\n",
    "            'part_dphi',\n",
    "        ],\n",
    "        'pf_vectors': [\n",
    "            'part_px',\n",
    "            'part_py',\n",
    "            'part_pz',\n",
    "            'part_energy',\n",
    "        ],\n",
    "        'pf_mask': ['part_mask']\n",
    "    }\n",
    "\n",
    "    def _pad(a, maxlen=128, value=0, dtype='float32'):\n",
    "        if isinstance(a, np.ndarray) and a.ndim >= 2 and a.shape[1] == maxlen:\n",
    "            return a\n",
    "        elif isinstance(a, ak.Array):\n",
    "            if a.ndim == 1:\n",
    "                a = ak.unflatten(a, 1)\n",
    "            a = ak.fill_none(ak.pad_none(a, maxlen, clip=True), value)\n",
    "            return ak.values_astype(a, dtype)\n",
    "        else:\n",
    "            x = (np.ones((len(a), maxlen)) * value).astype(dtype)\n",
    "            for idx, s in enumerate(a):\n",
    "                if not len(s):\n",
    "                    continue\n",
    "                trunc = s[:maxlen].astype(dtype)\n",
    "                x[idx, :len(trunc)] = trunc\n",
    "            return x\n",
    "\n",
    "    out = {}\n",
    "    for k, names in feature_list.items():\n",
    "        out[k] = np.stack([_pad(a[n], maxlen=128).to_numpy() for n in names], axis=1)\n",
    "\n",
    "    # Labels for TopLandscape (binary classification) \n",
    "    out['label'] = a['label'].to_numpy().astype('int')\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a172333c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading actual TopLandscape data from ../tl_test_file_0.root\n",
      "This part is working - TL\n",
      "TL sample data shapes:\n",
      "  pf_points: (6, 2, 128)\n",
      "  pf_features: (6, 7, 128)\n",
      "  pf_vectors: (6, 4, 128)\n",
      "  pf_mask: (6, 1, 128)\n",
      "  labels: (6,)\n",
      "\n",
      "Feature dimensions:\n",
      "  TopLandscape (kin): 7 features\n"
     ]
    }
   ],
   "source": [
    "# loading TL data\n",
    "\n",
    "def load_data(dataset_type='qg', batch_size=300):\n",
    "    \"\"\"\n",
    "    Load data from \n",
    "    \n",
    "    Args:\n",
    "        dataset_type: 'qg', QuarkGluon, or 'tl', TopLandscape\n",
    "        batch_size: Number of jets to load\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        if dataset_type == 'qg':\n",
    "            # Try to load QuarkGluon data \n",
    "            data_path = \"../qg_test_file_0.root\"\n",
    "            #if os.path.exists(data_path):\n",
    "            #    print(f\"Loading QuarkGluon data from {data_path}\")\n",
    "            #    with uproot.open(data_path)['tree'] as tree:\n",
    "            #        print('This part is working - QG')\n",
    "            #        data = build_features_and_labels_qg(tree)\n",
    "            #        # Truncate to batch_size\n",
    "            #        if data['pf_points'].shape[0] > batch_size:\n",
    "            #            print(f\"Truncating from {data['pf_points'].shape[0]} jets to {batch_size} jets\")\n",
    "            #            data = {\n",
    "            #                'pf_points': data['pf_points'][:batch_size],\n",
    "            #                'pf_features': data['pf_features'][:batch_size], \n",
    "            #                'pf_vectors': data['pf_vectors'][:batch_size],\n",
    "            #                'pf_mask': data['pf_mask'][:batch_size],\n",
    "            #                'labels': data['label'][:batch_size]\n",
    "            #            }\n",
    "            #        return data\n",
    "\n",
    "        elif dataset_type == 'tl':\n",
    "            # Try to load TopLandscape data\n",
    "            data_path = \"../tl_test_file_0.root\"\n",
    "            if os.path.exists(data_path):\n",
    "                print(f\"Loading TopLandscape data from {data_path}\")\n",
    "                with uproot.open(data_path)['tree'] as tree:\n",
    "                    print('This part is working - TL')\n",
    "                    data = build_features_and_labels_tl(tree)\n",
    "                    # Truncate to batch_size\n",
    "                    #if data['pf_points'].shape[0] > batch_size:\n",
    "                    #    print(f\"Truncating from {data['pf_points'].shape[0]} jets to {batch_size} jets\")\n",
    "                    data = {\n",
    "                            'pf_points': data['pf_points'][:batch_size],\n",
    "                            'pf_features': data['pf_features'][:batch_size],\n",
    "                            'pf_vectors': data['pf_vectors'][:batch_size],\n",
    "                            'pf_mask': data['pf_mask'][:batch_size],\n",
    "                            'labels': data['label'][:batch_size]\n",
    "                        }\n",
    "                    return data\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Could not load actual data: {e}\")\n",
    "        raise e\n",
    "\n",
    "tl_data = load_data('tl', batch_size=6)\n",
    "\n",
    "print(f\"TL sample data shapes:\")\n",
    "for k, v in tl_data.items():\n",
    "    print(f\"  {k}: {v.shape}\")\n",
    "\n",
    "print(f\"\\nFeature dimensions:\")\n",
    "#print(f\"  QuarkGluon (kinpid): {qg_data['pf_features'].shape[1]} features\")\n",
    "print(f\"  TopLandscape (kin): {tl_data['pf_features'].shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd32187",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim_legge/miniconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel_launcher.py:353: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting pre_softmax attention...\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre-softmaxed interaction...\n",
      "Got shape:torch.Size([1, 48, 128, 128])\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre_softmax attention...\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre-softmaxed interaction...\n",
      "Got shape:torch.Size([1, 48, 128, 128])\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre_softmax attention...\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre-softmaxed interaction...\n",
      "Got shape:torch.Size([1, 48, 128, 128])\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre_softmax attention...\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre-softmaxed interaction...\n",
      "Got shape:torch.Size([1, 48, 128, 128])\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre_softmax attention...\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre-softmaxed interaction...\n",
      "Got shape:torch.Size([1, 48, 128, 128])\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre_softmax attention...\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre-softmaxed interaction...\n",
      "Got shape:torch.Size([1, 48, 128, 128])\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre_softmax attention...\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre-softmaxed interaction...\n",
      "Got shape:torch.Size([1, 48, 128, 128])\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre_softmax attention...\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre-softmaxed interaction...\n",
      "Got shape:torch.Size([1, 48, 128, 128])\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# forward pass of model by itself to check that it can run\n",
    "\n",
    "tltrained_modelpath = '/home/tim_legge/save_tl_model/on-tl-run3_best_epoch_state.pt'\n",
    "\n",
    "model = test_ParT\n",
    "tl_state_dict = torch.load(tltrained_modelpath, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(tl_state_dict)\n",
    "tl_pf_features = tl_data['pf_features'][:]\n",
    "tl_pf_vectors = tl_data['pf_vectors'][:]\n",
    "tl_pf_mask = tl_data['pf_mask'][:]\n",
    "tl_pf_points = tl_data['pf_points'][:]\n",
    "tl_labels = tl_data['labels'][:]\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    tl_y_pred= model(torch.from_numpy(tl_pf_points),torch.from_numpy(tl_pf_features),torch.from_numpy(tl_pf_vectors),torch.from_numpy(tl_pf_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ed8c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run clear in order to sample results again\n",
    "# test_hooks.clear()\n",
    "\n",
    "test_hooks = Pre_Softmax_Hook(model=test_ParT, layer_name='Block')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc5b0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting pre_softmax attention...\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre-softmaxed interaction...\n",
      "Got shape:torch.Size([1, 48, 128, 128])\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre_softmax attention...\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre-softmaxed interaction...\n",
      "Got shape:torch.Size([1, 48, 128, 128])\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre_softmax attention...\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre-softmaxed interaction...\n",
      "Got shape:torch.Size([1, 48, 128, 128])\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre_softmax attention...\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre-softmaxed interaction...\n",
      "Got shape:torch.Size([1, 48, 128, 128])\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre_softmax attention...\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre-softmaxed interaction...\n",
      "Got shape:torch.Size([1, 48, 128, 128])\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre_softmax attention...\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre-softmaxed interaction...\n",
      "Got shape:torch.Size([1, 48, 128, 128])\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre_softmax attention...\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre-softmaxed interaction...\n",
      "Got shape:torch.Size([1, 48, 128, 128])\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre_softmax attention...\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "Getting pre-softmaxed interaction...\n",
      "Got shape:torch.Size([1, 48, 128, 128])\n",
      "Split the output into heads.\n",
      "New Tensor Shapes:\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n",
      "torch.Size([1, 8, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# forward pass with hooks enabled to collect pre-softmax attention and interaction matrices\n",
    "    \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    tl_y_pred = model(torch.from_numpy(tl_pf_points), torch.from_numpy(tl_pf_features), torch.from_numpy(tl_pf_vectors), torch.from_numpy(tl_pf_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "dc996953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48, 8, 128, 128])\n",
      "torch.Size([48, 8, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# check properties of test_hooks attention and interaction matrices\n",
    "\n",
    "print(test_hooks.pre_softmax_attentions.shape)  # should be (num_layers, num_heads, seq_len, seq_len)\n",
    "print(test_hooks.pre_softmax_interactions.shape)  # should be (num_layers, num_heads, seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6c4b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1778, 0.9216, 0.9527, 0.9801, 1.1148],\n",
      "        [0.6650, 1.2714, 1.1943, 1.1475, 1.4950],\n",
      "        [0.2347, 0.9348, 0.8578, 0.8427, 1.0863],\n",
      "        [0.0630, 0.8184, 0.7172, 0.7023, 0.9496],\n",
      "        [0.7404, 1.1706, 1.1707, 1.1545, 1.3597]])\n",
      "tensor([[ 4.0852e-04,  5.3245e+00,  3.2974e+00,  3.1406e+00,  2.8534e+00],\n",
      "        [ 5.3245e+00,  4.0852e-04, -1.1387e-01, -1.5723e-01, -7.1758e-02],\n",
      "        [ 3.2974e+00, -1.1387e-01,  4.0852e-04, -9.0576e-02, -1.6118e-01],\n",
      "        [ 3.1406e+00, -1.5723e-01, -9.0576e-02, -1.5330e-01, -1.6572e-01],\n",
      "        [ 2.8534e+00, -7.1758e-02, -1.6118e-01, -1.6572e-01,  4.0852e-04]])\n"
     ]
    }
   ],
   "source": [
    "# print some values from both for another check\n",
    "\n",
    "print(test_hooks.pre_softmax_attentions[0, 0, :5, :5])  # first layer, first head, first 5x5 block\n",
    "print(test_hooks.pre_softmax_interactions[0, 0, :5, :5])  # first layer, first head, first 5x5 block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b6cf63",
   "metadata": {},
   "source": [
    "Implementation used here could be extended to pretty much any other variable that passes through Block.\n",
    "I'm reminded of vector norm analysis here (https://arxiv.org/pdf/2004.10102) from Kobayashi et al."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
